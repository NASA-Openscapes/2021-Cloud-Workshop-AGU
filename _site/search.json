[
  {
    "objectID": "tutorials/index.html",
    "href": "tutorials/index.html",
    "title": "Tutorials Overview",
    "section": "",
    "text": "These tutorials are a combination of narrative, links, code, and outputs. They have been developed for live demos during the Workshop, and are available for self-paced learning.\nTutorials are markdown (.md) and Jupyter (.ipynb) notebooks, and are available on GitHub:\nhttps://github.com/NASA-Openscapes/2021-Cloud-Workshop-AGU/tree/main/tutorials."
  },
  {
    "objectID": "tutorials/00_Setup.html#step-1.-login-to-the-hub",
    "href": "tutorials/00_Setup.html#step-1.-login-to-the-hub",
    "title": "00. Setup for tutorials",
    "section": "Step 1. Login to the Hub",
    "text": "Step 1. Login to the Hub\nPlease go to https://openscapes.2i2c.cloud/hub/. Log in with your GitHub Account, and select ‚ÄúSmall‚Äù.\nAlternatively, you can also click this badge to launch the Hub:\n\n\n\n\n\n\n\n\n\n\nNote: It takes a few minutes for the Hub to load. Please be patient!\n\nWhile the Hub loads, we‚Äôll:\n\nDiscuss cloud environments\nSee how my Desktop is setup\nFork the Hackathon repository at github.com\nDiscuss python and conda environments\n\nThen, when the Hub is loaded, we‚Äôll get oriented in the Hub and clone the forked repository into our cloud environment."
  },
  {
    "objectID": "tutorials/00_Setup.html#discussion-cloud-environment",
    "href": "tutorials/00_Setup.html#discussion-cloud-environment",
    "title": "00. Setup for tutorials",
    "section": "Discussion: Cloud environment",
    "text": "Discussion: Cloud environment\nA brief overview about the NASA Openscapes Cloud Environment (following lessons from the Clinic).\n\nCloud infrastructure\n\nCloud: AWS us-west-2\n\nData: AWS S3 (cloud) and NASA DAAC data centers (on-prem).\nCloud compute environment: 2i2c Jupyterhub deployment\n\nIDE: JupyterLab"
  },
  {
    "objectID": "tutorials/00_Setup.html#discussion-my-desktop-setup",
    "href": "tutorials/00_Setup.html#discussion-my-desktop-setup",
    "title": "00. Setup for tutorials",
    "section": "Discussion: My desktop setup",
    "text": "Discussion: My desktop setup\nI‚Äôll screenshare to show and/or talk through how I have oriented the following software we‚Äôre using:\n\n2i2c Jupyterhub (our main workspace)\nHackathon Repo <> Hackathon Book (my teaching notes, your reference material)\nZoom Chat\nSlack"
  },
  {
    "objectID": "tutorials/00_Setup.html#step-2.-fork-the-hackathon-github-repository",
    "href": "tutorials/00_Setup.html#step-2.-fork-the-hackathon-github-repository",
    "title": "00. Setup for tutorials",
    "section": "Step 2. Fork the Hackathon GitHub repository",
    "text": "Step 2. Fork the Hackathon GitHub repository\n‚ÄúHow do I get the tutorial repository into the Hub?‚Äù. There are 2 steps. The first is from GitHub.com to fork the tutorial repository so that there is a connected copy in your user account that you can edit and push changes that won‚Äôt affect the nasa-openscapes copy.\nGo to https://github.com/nasa-openscapes/2021-Cloud-Hackathon and fork the repository.\n\nNote: if you‚Äôve already done this in the Pre-Hackathon Clinic, you‚Äôll need to make sure you have the latest, following the daily setup instructions."
  },
  {
    "objectID": "tutorials/00_Setup.html#discussion-python-and-conda-environments",
    "href": "tutorials/00_Setup.html#discussion-python-and-conda-environments",
    "title": "00. Setup for tutorials",
    "section": "Discussion: Python and Conda environments",
    "text": "Discussion: Python and Conda environments\nWhy Python?\n\n\n\nPython Data Stack. Source: Jake VanderPlas, ‚ÄúThe State of the Stack,‚Äù SciPy Keynote (SciPy 2015).\n\n\nDefault Python Environment:\nWe‚Äôve set up the Python environment with conda.\n\n\n\n\n\n\nConda environment\n\n\n\n\n\nname: openscapes\nchannels:\n  - conda-forge\ndependencies:\n  - python=3.9\n  - pangeo-notebook\n  - awscli~=1.20\n  - boto3~=1.19\n  - gdal~=3.3\n  - rioxarray~=0.8\n  - xarray~=0.19\n  - h5netcdf~=0.11\n  - netcdf4~=1.5\n  - h5py~=2.10\n  - geoviews~=1.9\n  - matplotlib-base~=3.4\n  - hvplot~=0.7\n  - pyproj~=3.2\n  - bqplot~=0.12\n  - geopandas~=0.10\n  - zarr~=2.10\n  - cartopy~=0.20\n  - shapely==1.7.1\n  - pyresample~=1.22\n  - joblib~=1.1\n  - pystac-client~=0.3\n  - s3fs~=2021.7\n  - ipyleaflet~=0.14\n  - sidecar~=0.5\n  - jupyterlab-geojson~=3.1\n  - jupyterlab-git\n  - jupyter-resource-usage\n  - ipympl~=0.6\n  - conda-lock~=0.12\n  - pooch~=1.5\n  - pip\n  - pip:\n    - tqdm\n    - harmony-py\n    - earthdata\n    - zarr-eosdis-store\n\n\n\n\nBash terminal and installed software\nLibraries that are available from the terminal\n\ngdal 3.3 commands ( gdalinfo, gdaltransform‚Ä¶)\nhdf5 commands ( h5dump, h5ls..)\nnetcdf4 commands (ncdump, ncinfo ‚Ä¶)\njq (parsing json files or streams from curl)\ncurl (fetch resources from the web)\nawscli (AWS API client, to interact with AWS cloud services)\nvim (editor)\ntree ( directory tree)\nmore ‚Ä¶\n\n\n\nUpdating the environment\nScientific Python is a vast space and we only included libraries that are needed in our tutorials. Our default environment can be updated to include any Python library that‚Äôs available on pip or conda.\nThe project used to create our default environment is called corn (as it can include many Python kernels).\nIf we want to update a library or install a whole new environment we need to open an issue on this repository. We can help your teams do this during project hacktime.\n\n\ncorn üåΩ"
  },
  {
    "objectID": "tutorials/00_Setup.html#step-3.-jupyterhub-orientation",
    "href": "tutorials/00_Setup.html#step-3.-jupyterhub-orientation",
    "title": "00. Setup for tutorials",
    "section": "Step 3. JupyterHub orientation",
    "text": "Step 3. JupyterHub orientation\nNow that the Hub is loaded, let‚Äôs get oriented.\n\n\n\n\nFirst impressions\n\nLauncher & the big blue button\n‚Äúhome directory‚Äù"
  },
  {
    "objectID": "tutorials/00_Setup.html#step-4.-clone-the-hackathon-github-repository",
    "href": "tutorials/00_Setup.html#step-4.-clone-the-hackathon-github-repository",
    "title": "00. Setup for tutorials",
    "section": "Step 4. Clone the Hackathon GitHub repository",
    "text": "Step 4. Clone the Hackathon GitHub repository\nNow we‚Äôll clone the GitHub repository, using a git extension for the JupyterHub. Go to your github account, and navigate to the repository that you just created by forking from the Openscapes repository.\nClick to copy the url for cloning the repository.\n\nNow, go to JupyterHub and click on the git extension in the left panel and then click the blue button ‚ÄúClone a Repository‚Äù.\n\nThen, paste the repository link to the forked repository that you copied from your github account into the ‚ÄúClone a repo‚Äù pop up window. Then click the blue ‚ÄúCLONE‚Äù button. It will take a few moments to clone the repository into your Hub.\nYour link should look like https://github.com/YOUR-USERNAME/2021-Cloud-Hackathon. For example, the link is https://github.com/virdi/2021-Cloud-Hackathon. Note that it include your github username in the repo link.\n\nAlternatively, you can use the terminal (command line) as per github workflows: first-time setup.\nOnce the repository is cloned, you will see a new directory in the ‚ÄúFile Browser‚Äù panel on the left named ‚Äú2021-Cloud-Hackathon‚Äù. In this directory, you have all hackathon material including the tutorials and this book to follow along during other Tutorials. You are all set.\n\n\nREMEMBER: This is your copy (or fork) of the hackathon materials and jupyter notebooks. So feel free to make any changes to the content of this repository."
  },
  {
    "objectID": "tutorials/00_Setup.html#jupyter-notebooks",
    "href": "tutorials/00_Setup.html#jupyter-notebooks",
    "title": "00. Setup for tutorials",
    "section": "Jupyter notebooks",
    "text": "Jupyter notebooks\nLet‚Äôs get oriented to Jupyter notebooks, which we‚Äôll use in all the tutorials."
  },
  {
    "objectID": "tutorials/00_Setup.html#how-do-i-end-my-session",
    "href": "tutorials/00_Setup.html#how-do-i-end-my-session",
    "title": "00. Setup for tutorials",
    "section": "How do I end my session?",
    "text": "How do I end my session?\n(Also see How do I end my Openscapes session? Will I lose all of my work?)\nWhen you are finished working for the day it is important to explicitly log out of your Openscapes session. The reason for this is it will save us a bit of money! When you keep a session active it uses up AWS resources and keeps a series of virtual machines deployed.\nStopping the server happens automatically when you log out, so navigate to ‚ÄúFile -> Log Out‚Äù and just click ‚ÄúLog Out‚Äù!\n!!! NOTE ‚Äúlogging out‚Äù - Logging out will NOT cause any of your work to be lost or deleted. It simply shuts down some resources. It would be equivalent to turning off your desktop computer at the end of the day."
  },
  {
    "objectID": "tutorials/00_Setup.html#step-5.-tracking-changes-optional",
    "href": "tutorials/00_Setup.html#step-5.-tracking-changes-optional",
    "title": "00. Setup for tutorials",
    "section": "Step 5. Tracking changes (Optional)",
    "text": "Step 5. Tracking changes (Optional)\nNow that you have forked and cloned the repository in your Hub, you can make changes (edit, add, and/or delete content) and track these files using git. In this step, we will provide an overview of how to use git using the graphical interface (the JupyterLab git extension).\n\nStep 5.1. Configure Git (git config)\nConfigure git with your name and email address as shown here.\ngit config --global user.name \"Makhan Virdi\"\ngit config --global user.email \"Makhan.Virdi@gmail.com\"\nOpen a new terminal: File >> New >> Terminal\n\nConfigure git to store your github credentials to avoid having to enter your github username and token each time you push changes to your repository(in Step 5.5, we will describe how to use github token instead of a password)\ngit config --global credential.helper store\n\n\nStep 5.2. Create a new file\nLet‚Äôs create a new file: In the left panel on your Hub, click on the ‚Äúdirectory‚Äù icon and then double click on ‚Äú2021-Cloud-Hackathon‚Äù directory. Then, create a new file using the text editor in your 2i2c JupyterHub (File >> New >> Text File). Add some text to this file, for example: A test file. Save this file and rename it to test.txt.\n\n\n\nStep 5.3. Track the changes to the new file (git add)\nClick the git icon in the left panel. You can see that the newly added file is in the ‚ÄúUntracked‚Äù section. You can click the + icon next to the file name to let git track this file for changes.\n\n\n\nStep 5.4. Commit the changes to the new file (git commit)\nNow, you will see that the file is Staged, which means that git is ready to take a snapshot of this file (and the repository) with the changes that you made. This snapshot is called a commit. To commit the changes, add a note (called a commit message) by typing in the text box that say ‚ÄúSummary‚Äù.\nNow, click the blue ‚ÄúCOMMIT‚Äù button to commit this change.\n\nNote: A short message indicating the type of change to this file is a good practice. Optionally, a longer description may be added to the ‚ÄúDescription‚Äù field.\n\n\n\n\nStep 5.5. Transmit committed changes to your github (git push) {#step-5.5.-transmit-committed-changes-to-your-github-(git-push}\nAt this stage, you have committed the changes to your git repository on your Hub. However, these changes are still on your Hub and needs to be transmitted to your repository on github (so that both the local copy on the JupyterHub and the remote copy on github are in sync).\nAs seen in the picture below, the git extension indicates (with an orange dot on the cloud icon) that it is ready to push your changes to the remote (remote = your repository on github.com). To push to github, click the cloud button with an up arrow (circled in red in the picture).\n\nThe git extension in the Hub will prompt you to enter your github.com credentials. Enter you github.com username and a Personal Access Token (DO NOT use your password). To create a Personal Access Token, visit https://github.com/settings/tokens/new and create a new token with the permission as per the image below and specify its validity for 90 days.\n\n\nIMPORTANT: You will see this token only once, so be sure to copy this. If you do not copy your token at this stage, you will need to generate a new token.\n\nOnce you generate the token, copy it and paste in the Hub window that prompted you to enter the ‚ÄúPersonal Access Token‚Äù.\n\nGit will show a message at the bottom right telling that the changes were ‚ÄúSuccessfully pushed‚Äù. Also, you will see that the ‚Äúcloud icon with an up arrow‚Äù no longer has an orange dot, indicating that there are no more committed changes to push to the remote (github.com).\n\nNote: You have configured git extension to store your credentials. You will not be prompted for your login/token again!\n\n\nThat‚Äôs all. You can use the same workflow (add > commit > push) for any other new or modified files!\n\n\nNote: If you are comfortable with the command line, you can use the Terminal (In Hub, New > Terminal) and follow the steps outlined in the Clinic section."
  },
  {
    "objectID": "logistics/schedule.html#hackathon-day-1-november-15",
    "href": "logistics/schedule.html#hackathon-day-1-november-15",
    "title": "Schedule",
    "section": "Hackathon Day 1: November 15",
    "text": "Hackathon Day 1: November 15\n_schedule-day1.md\n\nWelcome Day 1\nPlease see the CloudHackathon_Notes Google Doc:\n_welcome-day1.md\n\n\nClosing Day 1\n_closing-day1-on.md"
  },
  {
    "objectID": "logistics/schedule.html#hackathon-day-2-november-16",
    "href": "logistics/schedule.html#hackathon-day-2-november-16",
    "title": "Schedule",
    "section": "Hackathon Day 2: November 16",
    "text": "Hackathon Day 2: November 16\n_schedule-day2.md\n\nWelcome Day 2\n_welcome-day2-on.md\n\n\nClosing Day 2\n_closing-day1-on.md"
  },
  {
    "objectID": "logistics/schedule.html#hackathon-day-3-november-17",
    "href": "logistics/schedule.html#hackathon-day-3-november-17",
    "title": "Schedule",
    "section": "Hackathon Day 3: November 17",
    "text": "Hackathon Day 3: November 17\n_schedule-day3.md\n\nWelcome Day 3\n_welcome-day2-on.md\n\n\nClosing Day 3\n_closing-day1-on.md"
  },
  {
    "objectID": "logistics/schedule.html#hackathon-day-4-november-18",
    "href": "logistics/schedule.html#hackathon-day-4-november-18",
    "title": "Schedule",
    "section": "Hackathon Day 4: November 18",
    "text": "Hackathon Day 4: November 18\n_schedule-day4.md\n\nWelcome Day 4\n_welcome-day2-on.md\n\n\nClosing Day 4\n_closing-day1-on.md"
  },
  {
    "objectID": "logistics/schedule.html#hackathon-day-5-november-19",
    "href": "logistics/schedule.html#hackathon-day-5-november-19",
    "title": "Schedule",
    "section": "Hackathon Day 5: November 19",
    "text": "Hackathon Day 5: November 19\n_schedule-day5.md\n\nWelcome Day 5\n\nJupyterHub: Log in.\n\nLog into 2i2c at https://openscapes.2i2c.cloud/hub/. This takes a few minutes so please start this as soon we reconvene each day\n\nGroup Photo!\n\n\n\nWhat‚Äôs next\nSlides sharing about upcoming opportunities with the NASA Openscapes project.\n\n\nClosing Day 5\n_closing-day5.md"
  },
  {
    "objectID": "logistics/schedule.html#pre-hackathon-clinic-november-9",
    "href": "logistics/schedule.html#pre-hackathon-clinic-november-9",
    "title": "Schedule",
    "section": "Pre-Hackathon Clinic: November 9",
    "text": "Pre-Hackathon Clinic: November 9\nThis Clinic is optional and we will share a recording that participants can review ahead of time.\n_schedule-clinic.md"
  },
  {
    "objectID": "logistics/prerequisites.html#prerequisites",
    "href": "logistics/prerequisites.html#prerequisites",
    "title": "Prerequisites & help",
    "section": "Prerequisites",
    "text": "Prerequisites\nIf you would like to follow along hands-on during the Workshop, please do the following (20 minutes). All software or accounts are free.\n\nGitHub username\n\nCreate a GitHub account (if you don‚Äôt already have one) at https://github.com. Follow optional advice on choosing your username\nPlease provide your GitHub username here; this will allow us to add you to the cloud hackathon workspace.\nRemember your username and password; you will need to be logged in during the workshop!\n\nEarthdata Login account\n\nCreate an Earthdata Login account (if you don‚Äôt already have one) at https://urs.earthdata.nasa.gov\nRemember your username and password; you will need to be logged in during the workshop!\n\nZoom\n\nBe prepared to call into Zoom using the link provided in the Slack 2021-nasacloudworkshop-agu Channel.\n\nGet comfortable\n\nConsider your computer set-up in advance, including an external monitor if possible. You can follow along in Jupyter Hub on your own computer while also watching an instructor demo over Zoom (or equivalent), and will also want quick-access to Slack to ask for help and follow links."
  },
  {
    "objectID": "logistics/prerequisites.html#getting-help",
    "href": "logistics/prerequisites.html#getting-help",
    "title": "Prerequisites & help",
    "section": "Getting help",
    "text": "Getting help\nWe will use Zoom Chat as our main channel for help. Please use this to post questions and request a breakout room."
  },
  {
    "objectID": "logistics/github-workflows.html#first-time-setup",
    "href": "logistics/github-workflows.html#first-time-setup",
    "title": "GitHub workflows",
    "section": "First-Time Setup",
    "text": "First-Time Setup\n\nFork the hackathon repo\nGo to https://github.com/nasa-openscapes/2021-Cloud-Hackathon and fork the repository. This will enable you to you can edit your own copy and live-code with us\n\nNote: The term fork means that you are going to copy the project into your own user space in Github\n\n\n\n\nFork a copy\n\n\n\n\nClone your forked repo into JupyterHub\nOpen your terminal\ngit clone https://github.com/YOUR-USERNAME/2021-Cloud-Hackathon\ndon‚Äôt do all the credentials/token ‚Äî they don‚Äôt need push access to follow along with the tutorials. We can help them via Slack/breakouts when/if they need to push with tokens"
  },
  {
    "objectID": "logistics/github-workflows.html#daily-setup",
    "href": "logistics/github-workflows.html#daily-setup",
    "title": "GitHub workflows",
    "section": "Daily Setup",
    "text": "Daily Setup\nThe daily setup has 2 steps: get the latest into your forked copy of the repo, then get the latest of your fork into your JupyterHub.\nIf you have any conflicts with the following steps, you will likely need to commit your work, or clear your work if you don‚Äôt want to keep anything you‚Äôve done. See below for daily setup troubleshooting as well as Git update, revert, etc.\n\nGitHub: Update your fork\nFrom github.com Fetch and merge: Update your forked repo from main by clicking ‚ÄúFetch upstream‚Äù beneath the big green code button, and then the green ‚ÄúFetch and merge‚Äù button. You may have to refresh the page to see any recent activity.\n\n\n\nJupyterHub: Get your fork‚Äôs updates\nGo to https://openscapes.2i2c.cloud/hub/\nGo to the GitHub extension and click the ‚Äúpull button‚Äù\nIf you see\n\nYou‚Äôll need to decide if you want to keep or delete the changes you made yesterday. This will depend on the work you did and how important it was. If you‚Äôd like to delete it, please follow delete your local changes below, and then come back above and retype git status and git pull.\nIf you‚Äôd like to keep your changes, you‚Äôll need to commit them. You can press ‚ÄúCancel‚Äù and look at the files in the ‚ÄúChanged‚Äù category and you can hover over the file to inspect them (open, diff, discard, add) and if you‚Äôd like to add them, they will be staged and then you can commit them with a message.\nYou could also do the above in the terminal, making sure you are in the 2021-Cloud-Hackathon directory (double check with pwd and move with cd)\ngit status\ngit pull\n\n\nDaily setup troubleshooting\nNot a git repository - in your terminal if you see the following, you likely need to cd change directory into your GitHub folder.\nfatal: not a git repository (or any parent up to mount point /home)\nStopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set)."
  },
  {
    "objectID": "logistics/github-workflows.html#git-update-revert-etc",
    "href": "logistics/github-workflows.html#git-update-revert-etc",
    "title": "GitHub workflows",
    "section": "Git: update, revert, etc",
    "text": "Git: update, revert, etc\nThese are some useful commands to revert/delete your local changes and update your fork with the most recent information from the main branch.\n\nDelete your local changes\nThere are several ways to delete your local changes if you were playing around and want to reset. Here are a few:\n\nUndo changes you‚Äôve maybe saved or committed, but not pushed\nThis is less time and internet intensive (no new clone/download).\nIf you‚Äôve got changes saved, but not yet staged, committed, or pushed, you‚Äôll delete unstaged changes in the working directory with clean:\nYou‚Äôll need to make sure you‚Äôre in the github repository (use pwd to check your present working directory and cd to change directory)\ngit clean -df\ngit checkout -- .\n\n\nBurn it all down\nYou‚Äôll delete the whole repo that you have locally, and then reclone.\nYou‚Äôll need to make sure you‚Äôre in the github repository (use pwd to check your present working directory and cd to change directory)\nrm -rf YOUR-REPO\nHere is a whole blog on how to go back in time (walk back changes), with conceptual diagrams, command line code, and screenshots from RStudio. https://ohi-science.org/news/github-going-back-in-time\n\n\n\nUpdate local branch with remote main branch\nIf while you‚Äôre working you would like to update your local your-branch with the most recent updates on the main branch on GitHub.com, there are several ways to do this. Here‚Äôs one.\ngit checkout your-branch\ngit fetch\ngit merge origin/main\n\n\nUpdate from main"
  },
  {
    "objectID": "logistics/github-workflows.html#project-hacktime-setup",
    "href": "logistics/github-workflows.html#project-hacktime-setup",
    "title": "GitHub workflows",
    "section": "Project Hacktime Setup",
    "text": "Project Hacktime Setup\nHere are some suggestions for collaborating with your project groups (and beyond!) using GitHub.\nThis means a combination of creating a place to collaborate (a github repository) and a shared workflow to contribute.\n\nCreate a repository on GitHub.com\nJust one person does this.\nYou can do this in one of your user/organization accounts, or ask someone from the Cloud Hackathon Team to create one for you in the NASA-Openscapes organization.\nHere are instructions for creating a repo on GitHub.com ‚Äî remember to make it public so that other hackathon folks can see and help!\n\n\nDiscuss edit access vs branches\nThe person who created the repo will manage the permission.\nThe simplest way to collaborate on GitHub is if everyone has permission to edit the repository directly through the main branch. Talk to your team ‚Äî folks that have experience using branches can do so but others can push changes directly to the main branch.\nHere are instructions for updating github repo permissions.\n\n\nClone repo into 2i2c\nEverybody does this.\nTo to the JupyterHub, go to your Terminal, then:\ngit clone https://github.com/USERNAME/REPOSITORY-NAME.\n\n\nCheck in as you push changes\nWhether you‚Äôre using branches or not, check in with each other as you push updates to avoid merge conflicts and have the latest progress.\nHere are instructions for a workflow with branches (optional).\n\n\nUploading files from your local computer to 2i2c\nDo this using the ‚ÄúUpload Files‚Äù button in JupyterHub in 2i2c, the UP arrow two over from the big blue + button."
  },
  {
    "objectID": "logistics/github-workflows.html#github-qa",
    "href": "logistics/github-workflows.html#github-qa",
    "title": "GitHub workflows",
    "section": "GitHub Q&A",
    "text": "GitHub Q&A\n\nWhen should I fork+clone instead of just clone?\nFrom Mike Gangl:\nA fork becomes ‚Äòindependent‚Äô of the repository you‚Äôre forking. So you control if/when you pull in changes. a clone on the other hand, will be linked to the original repository- so if you do ‚Äúpull‚Äù you‚Äôll get the changes from the parent repository as well.\nIf you plan on contributing to a project, a fork is usually the best way to do that, if you plan on simply consuming the project (e.g.¬†run a tutorial) then cloning is fine. A fork can always be appropriate.\nOr, if you plan on updating and making changes that you‚Äôd want to preserve, a fork allows you to do that in your own repository, whereas cloning would need you to have permissions to write (push) to the repository."
  },
  {
    "objectID": "logistics/index.html#for-hackathon-participants",
    "href": "logistics/index.html#for-hackathon-participants",
    "title": "Logistics overview",
    "section": "For Hackathon Participants",
    "text": "For Hackathon Participants\nBefore the hackathon, please complete the prerequisites and be prepared for the daily setup and be familiar with the ways of getting help."
  },
  {
    "objectID": "logistics/index.html#for-hackathon-helpers---a-mentors-guide",
    "href": "logistics/index.html#for-hackathon-helpers---a-mentors-guide",
    "title": "Logistics overview",
    "section": "For Hackathon Helpers - A Mentor‚Äôs Guide",
    "text": "For Hackathon Helpers - A Mentor‚Äôs Guide\nMentor = trainer = helper = any DAAC or Openscapes staff\n\nSlack\nChannels in Openscapes workspace\nUpdate your Slack name temporarily to include ‚Äúhelper‚Äù, e.g.¬†Catalina Oaida (Helper)\n\nMentors‚Äô/Helper channel - #nasa-daac-mentors1\n\nOur back channel to coordinate amongst ourselves\n\nGeneral channel - #2021-nasacloudhack-general\n\nAnnouncements, general questions and communications\n\nPitchfest channel - #2021-nasacloudhack-projects\n\nParticipants discuss project ideas\nUse slack direct message (DM) for team/project work\n\nHelp channel - #2021-nasacloudhack-help\n\nTroubleshooting, share screenshots, etc\nWhen you see a question you will reply to, please add the ‚Äúeyes‚Äù emoji below so other helpers know that you are looking into it. Then please reply in-thread in Slack to help. Can tag additional helpers if you need further support.\n\n\n\n\nGithub\nHackathon repo: https://github.com/NASA-Openscapes/2021-Cloud-Hackathon; any updates to the main branch will be updated in this book (via GitHub Action).\n\ntutorials folder\ntutorials-templates folder for live coding - same as tutorials but the code is removed and only markdown remains\nParticipants will create their own repos, we can link to their projects in the cloud hackathon repo to capture the hackathon projects artifacts\nWorking with the CH repo\n\nParticipants are instructed to fork the CH repo to their own github, then clone the CH repo. This way they can push any updates or changes to the CH repo within their own version of the repo without impacting the original CH repo\nThis should be covered in Tutorial 0.\n\nWhat happens to my work on 2i2c after the 3 months?\n\nCode: push to your own Github repo\nData and analysis outputs - working on S3 solution\n\nFrom Clinic chapter: ‚ÄúThis section is a step-by-step guide to set up git on your 2i2c instance and configure git to use your github.com account for managing your repositories hosted on github.com.‚Äù We are also looking into using Git Extension.\n\n\n\nZoom teleconference\n\nHost: Erin and Julie\nBreak-out rooms\n\nWe will set up Breakout rooms ahead of time, and have helpers ready to meet with participants. If there are many people in breakout rooms/lots of same issues, we‚Äôll pause the tutorial and address things together\nIf a participant is stuck and needs 1:1 help, ping one of the Zoom hosts in the mentor/general channel to place you in a breakout room with participant X\n\nChat - Encourage participants to use the Slack general or help channel for questions, not the Zoom chat\n\nErin will mention this in the Welcome (logistics) session beginning of Day 1\n\n\n\n\nTutorials workflow\n\nWelcome mentor share link to CH Book during Welcome each day\nWelcome mentor to instruct participant to spin up 2i2c JupyterHub if they haven‚Äôt done so that morning already\nTutorial presenters can also role-model having the CH Book open as a tab when they teach if they want to refer to it.\nTutorial presenter will be using template notebooks based on each tutorial\nTutorial presenter will live code, and participants will follow along in their instance\nTutorial presenter screen zoom level should be 130%\n\n\n\nHelpers support during ‚Äúlive coding‚Äù Tutorials Demos\n\nTutorial mentor live codes, participants follow along\nAll other helpers monitor Slack help (and general) channel for questions, people being stuck\n\nWhen you see a question you will reply to, please add the ‚Äúeyes‚Äù emoji below so other helpers know that you are looking into it. Then please reply in-thread in Slack to help. Can tag additional helpers if you need further support.\nIf needed, ask host to place you and participant in breakout room temporarily for easier help/support\n\nReminder: Update your Slack name temporarily to include ‚Äúhelper‚Äù, e.g.¬†‚ÄúCatalina Oaida (Helper)‚Äù\n\n\n\nHelper support during Team Hack Time\n\nZoom host - Place participants in breakout rooms based on their team\n\nCan collaborate in zoom breakout rooms and in their slack team DM (direct messaging)\n\nParticipants should post any team project questions in the help slack channel; screenshot are helpful\n\nParticipants can create a DM thread with their teammates for team work discussions\n\nMentors monitor the help slack channel for questions\n\nWhen you see a question you will reply to, please add the ‚Äúeyes‚Äù emoji below so other helpers know that you are looking into it. Then please reply in-thread in Slack to help. Can tag additional helpers if you need further support.\nand/or ask host to be added to the team zoom breakout room\n\nMentors use the mentor slack channel if you need additional help from a colleague\n\nThey can then also be added to zoom breakout room, as needed\n\nBeyond the scheduled time for Team Hack Time, the zoom meeting will close out, but participants are welcome to continue to discuss via their DM in slack\n\nNo on-call mentor support beyond the scheduled team hack time session\nAdditional help from mentors available 8-9am Tue-Fri during optional office hours\n\n\n\n\n2i2c JupyterHub\nLog-in before we start each day: https://openscapes.2i2c.cloud/hub/"
  },
  {
    "objectID": "logistics/jupyter-notebooks-hubs.html",
    "href": "logistics/jupyter-notebooks-hubs.html",
    "title": "2021 Cloud Hackathon",
    "section": "",
    "text": "Jupyter notebook question: is there a way to copy multiple cells at once and paste in a new notebook? (rather than having to go into each cell individually to copy that snippet of code) - while pressing shift, with the mouse or the arrow keys select the cells you want then you can press ‚ÄòC‚Äô or right click and copy the cells and then go to a different notebook and paste them. - to make this work you need to focus on the notebook not in a cell (press ESC if so)"
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "2021 Cloud Workshop at AGU",
    "section": "Welcome",
    "text": "Welcome\n\nWelcome to the 2021 Cloud Workshop at AGU: Enabling Analysis in the Cloud Using NASA Earth Science Data, co-hosted by the NASA EOSDIS Land Processes Distributed Active Archive Center (LP.DAAC), National Snow and Ice Data Center DAAC (NSIDC DAAC), Physical Oceanography Distributed Active Archive Center (PO.DAAC), with support provided by ASDC DAAC, GES DISC, IMPACT, and NASA Openscapes.\nThe Cloud Workshop will take place virtually on December 12, 2021, from 8am-12pm CST (UTC-6) in AGU session SCIWS31. Registration is required."
  },
  {
    "objectID": "index.html#about",
    "href": "index.html#about",
    "title": "2021 Cloud Workshop at AGU",
    "section": "About",
    "text": "About\nThe 2021 Cloud Workshop at AGU: Enabling Analysis in the Cloud Using NASA Earth Science Data is a virtual half-day collaborative open science learning experience aimed at exploring, learning, and promoting effective cloud-based science and applications workflows using NASA Earthdata Cloud data, tools, and services (among others), in support of Earth science data processing and analysis in the era of big data."
  },
  {
    "objectID": "logistics/schedule.html#workshop-schedule",
    "href": "logistics/schedule.html#workshop-schedule",
    "title": "Schedule",
    "section": "Workshop Schedule",
    "text": "Workshop Schedule\n\n\n\nTime, CST (UTC-6)\nEvent\nLeads/Instructors\n\n\n\n\n8:00 am\nWelcome\nCatalina & Cyndi (?)\n\n\n8:15 am\nTutorial 0: Quick orientation 2i2c\nLuis (?)\n\n\n8:30 am\nTutorial 1: EarthData Search\n(?)\n\n\n9:15 am\nTutorial 2: EarthData Authentication\n(?)\n\n\n9:30 am\nBreak\n\n\n\n9:45 am\nTutorial 3: Direct S3 Access\nAaron (?)\n\n\n10:30 am\nTutorial 4: Cloud-non-cloud\nAMY (?)\n\n\n11:15 am\nQ&A\nAll\n\n\n11:45 am\nClosing (survey and next opportunities)\nJulie/Erin/Catalina (?)"
  },
  {
    "objectID": "logistics/prerequisites.html",
    "href": "logistics/prerequisites.html",
    "title": "2021 Cloud Hackathon",
    "section": "",
    "text": "Zoom Breakout Rooms\n\nDuring Tutorials Session\nIf you‚Äôd like to talk to someone and live-screenshare about your issue, please write in Zoom Chat that you need help and we will move you into a breakout room with a helper.\n\n\nDuring Team Hacktime\nDuring the team project time, you will be placed in a Zoom breakout room with your respective teammates to collaborate more easily. If you have questions as you work, post your question(s) in the Slack 2021-nasacloudhack-help Channel and a helper will respond in that thread. If needed, a helper can also join your team‚Äôs Zoom breakour room for easy screensharing, troubleshooting or to further discuss a question."
  },
  {
    "objectID": "logistics/index.html#for-workshop-participants",
    "href": "logistics/index.html#for-workshop-participants",
    "title": "Logistics overview",
    "section": "For Workshop Participants",
    "text": "For Workshop Participants\nBefore the workshop, please complete the prerequisites."
  },
  {
    "objectID": "tutorials/04_On-Prem_Cloud.html#accessing-and-harmonizing-data-located-within-and-outside-of-the-nasa-earthdata-cloud",
    "href": "tutorials/04_On-Prem_Cloud.html#accessing-and-harmonizing-data-located-within-and-outside-of-the-nasa-earthdata-cloud",
    "title": "2021 Cloud Workshop at AGU",
    "section": "Accessing and harmonizing data located within and outside of the NASA Earthdata Cloud",
    "text": "Accessing and harmonizing data located within and outside of the NASA Earthdata Cloud"
  },
  {
    "objectID": "tutorials/04_On-Prem_Cloud.html#timing",
    "href": "tutorials/04_On-Prem_Cloud.html#timing",
    "title": "2021 Cloud Workshop at AGU",
    "section": "Timing",
    "text": "Timing\n\nExercise: 45 min"
  },
  {
    "objectID": "tutorials/04_On-Prem_Cloud.html#summary",
    "href": "tutorials/04_On-Prem_Cloud.html#summary",
    "title": "2021 Cloud Workshop at AGU",
    "section": "Summary",
    "text": "Summary\nThis tutorial will combine several workflow steps and components from the previous days, demonstrating the process of using the geolocation of data available outside of the Earthdata Cloud to then access coincident variables of cloud-accessible data. This may be a common use case as NASA Earthdata continues to migrate to the cloud, producing a ‚Äúhybrid‚Äù data archive across Amazon Web Services (AWS) and original on-premise data storage systems. Additionally, you may also want to combine field measurements with remote sensing data available on the Earthdata Cloud.\nThis specific example explores the pairing of the ICESat-2 ATL07 Sea Ice Height data product, currently (as of November 2021) available publicly via direct download at the NSIDC DAAC, along with Sea Surface Temperature (SST) from the GHRSST MODIS L2 dataset (MODIS_A-JPL-L2P-v2019.0) available from PO.DAAC on the Earthdata Cloud.\nThe use case we‚Äôre looking at today centers over an area north of Greenland for a single day in June, where a melt pond was observed using the NASA OpenAltimetry application. Melt ponds are an important feature of Arctic sea ice dynamics, leading to an decrease in sea ice albedo and other changes in heat balance. Many NASA Earthdata datasets produce variables including sea ice albedo, sea surface temperature, air temperature, and sea ice height, which can be used to better understand these dynamics.\n\nObjectives\n\nPractice skills searching for data in CMR, determining granule coverage across two datasets over an area of interest.\nDownload data from an on-premise storage system to our cloud environment.\nRead in 1-dimensional trajectory data (ICESat-2 ATL07) into xarray and perform attribute conversions.\nSelect and read in sea surface temperature (SST) data (MODIS_A-JPL-L2P-v2019.0) from the Earthdata Cloud into xarray.\nExtract, resample, and plot coincident SST data based on ICESat-2 geolocation."
  },
  {
    "objectID": "tutorials/04_On-Prem_Cloud.html#import-packages",
    "href": "tutorials/04_On-Prem_Cloud.html#import-packages",
    "title": "2021 Cloud Workshop at AGU",
    "section": "Import packages",
    "text": "Import packages\nimport os\nfrom pathlib import Path\nfrom pprint import pprint\n\n# Access EDS\nimport requests\n\n# Access AWS S3\nimport s3fs\n\n# Read and work with datasets\nimport xarray as xr\nimport numpy as np\nimport h5py\n\n# For plotting\nimport matplotlib.pyplot as plt\nimport cartopy.crs as ccrs\nfrom shapely.geometry import box\n\n# For resampling\nimport pyresample"
  },
  {
    "objectID": "tutorials/04_On-Prem_Cloud.html#specify-data-time-range-and-area-of-interest",
    "href": "tutorials/04_On-Prem_Cloud.html#specify-data-time-range-and-area-of-interest",
    "title": "2021 Cloud Workshop at AGU",
    "section": "Specify data, time range, and area of interest",
    "text": "Specify data, time range, and area of interest\nWe are going to focus on getting data for an area north of Greenland for a single day in June.\nThese bounding_box and temporal variables will be used for data search, subset, and access below:\n# Bounding Box spatial parameter in decimal degree 'W,S,E,N' format.\nbounding_box = '-62.8,81.7,-56.4,83'\n\n# Each date in yyyy-MM-ddTHH:mm:ssZ format; date range in start,end format\ntemporal = '2019-06-22T00:00:00Z,2019-06-22T23:59:59Z'\nSince we‚Äôve already demonstrated how to locate a dataset‚Äôs collection_id and use the cloud_hosted parameter to determine whether a dataset resides in the Earthdata Cloud, we are going to skip forward and declare these variables:\nmodis_concept_id = 'C1940473819-POCLOUD'\nicesat2_concept_id = 'C2003771980-NSIDC_ECS'"
  },
  {
    "objectID": "tutorials/04_On-Prem_Cloud.html#search-and-download-icesat-2-atl07-files",
    "href": "tutorials/04_On-Prem_Cloud.html#search-and-download-icesat-2-atl07-files",
    "title": "2021 Cloud Workshop at AGU",
    "section": "Search and download ICESat-2 ATL07 files",
    "text": "Search and download ICESat-2 ATL07 files\nPerform a granule search over our time and area of interest. How many granules are returned?\ngranule_url = 'https://cmr.earthdata.nasa.gov/search/granules'\n\nresponse = requests.get(granule_url,\n                       params={\n                           'concept_id': icesat2_concept_id,\n                           'temporal': temporal,\n                           'bounding_box': bounding_box,\n                           'page_size': 200,\n                       },\n                       headers={\n                           'Accept': 'application/json'\n                       }\n                      )\nprint(response.headers['CMR-Hits'])\n\n2\n\n\nPrint the file names, size, and links:\n\ngranules = response.json()['feed']['entry']\nfor granule in granules:\n    print(f'{granule[\"producer_granule_id\"]} {granule[\"granule_size\"]} {granule[\"links\"][0][\"href\"]}')\n\nATL07-01_20190622055317_12980301_004_01.h5 237.0905504227 https://n5eil01u.ecs.nsidc.org/DP9/ATLAS/ATL07.004/2019.06.22/ATL07-01_20190622055317_12980301_004_01.h5\nATL07-01_20190622200154_13070301_004_01.h5 230.9151573181 https://n5eil01u.ecs.nsidc.org/DP9/ATLAS/ATL07.004/2019.06.22/ATL07-01_20190622200154_13070301_004_01.h5\n\n\n\nDownload ATL07 files\nAlthough several services are supported for ICESat-2 data, we are demonstrating direct access through the ‚Äúon-prem‚Äù file system at NSIDC for simplicity.\nSome of these services include: - icepyx - From the icepyx documentation: ‚Äúicepyx is both a software library and a community composed of ICESat-2 data users, developers, and the scientific community. We are working together to develop a shared library of resources - including existing resources, new code, tutorials, and use-cases/examples - that simplify the process of querying, obtaining, analyzing, and manipulating ICESat-2 datasets to enable scientific discovery.‚Äù - NSIDC DAAC Data Access and Service API - The API provided by the NSIDC DAAC allows you to access data programmatically using specific temporal and spatial filters. The same subsetting, reformatting, and reprojection services available on select data sets through NASA Earthdata Search can also be applied using this API. - IceFlow - The IceFlow python library simplifies accessing and combining data from several of NASA‚Äôs cryospheric altimetry missions, including ICESat/GLAS, Operation IceBridge, and ICESat-2. In particular, IceFlow harmonizes the various file formats and georeferencing parameters across several of the missions‚Äô data sets, allowing you to analyze data across the multi-decadal time series.\nWe‚Äôve found 2 granules. We‚Äôll download the first one and write it to a file with the same name as the producer_granule_id.\nWe need the url for the granule as well. This is href links we printed out above.\nicesat_id = granules[0]['producer_granule_id']\nicesat_url = granules[0]['links'][0]['href']\nTo retrieve the granule data, we use the requests.get() method, which will utilize the .netrc file on the backend to authenticate the request against Earthdata Login.\nr = requests.get(icesat_url)\nThe response returned by requests has the same structure as all the other responses: a header and contents. The header information has information about the response, including the size of the data we downloaded in bytes.\n\nfor k, v in r.headers.items():\n    print(f'{k}: {v}')\n\nDate: Thu, 18 Nov 2021 04:02:03 GMT\nServer: Apache\nVary: User-Agent\nContent-Disposition: attachment\nContent-Length: 248607461\nKeep-Alive: timeout=15, max=100\nConnection: Keep-Alive\n\n\nThe contents needs to be saved to a file. To keep the directory clean, we will create a downloads directory to store the file. We can use a shell command to do this or use the makedirs method from the os package.\nos.makedirs(\"downloads\", exist_ok=True)\nYou should see a downloads directory in the file browser.\nTo write the data to a file, we use open to open a file. We need to specify that the file is open for writing by using the write-mode w. We also need to specify that we want to write bytes by setting the binary-mode b. This is important because the response contents are bytes. The default mode for open is text-mode. So make sure you use b.\nWe‚Äôll use the with statement context-manager to open the file, write the contents of the response, and then close the file. Once the data in r.content is written sucessfully to the file, or if there is an error, the file is closed by the context-manager.\nWe also need to prepend the downloads path to the filename. We do this using Path from the pathlib package in the standard library.\noutfile = Path('downloads', icesat_id)\nif not outfile.exists():\n    with open(outfile, 'wb') as f:\n        f.write(r.content)\nATL07-01_20190622055317_12980301_004_01.h5 is an HDF5 file. xarray can open this but you need to tell it which group to read the data from. In this case we read the sea ice segment height data for ground-track 1 left-beam. You can explore the variable hierarchy in Earthdata Search, by selecting the Customize option under Download Data.\nThis code block performs the following operations: - Extracts the height_segment_height variable from the heights group, along with the dimension variables contained in the higher level sea_ice_segments group, - Convert attributes from bytestrings to strings, - Drops the HDF attribute DIMENSION_LIST, - Sets _FillValue to NaN\n\nvariable_names = [\n    '/gt1l/sea_ice_segments/latitude',\n    '/gt1l/sea_ice_segments/longitude',\n    '/gt1l/sea_ice_segments/delta_time',\n    '/gt1l/sea_ice_segments/heights/height_segment_height'\n    ]\nwith h5py.File(outfile, 'r') as h5:\n    data_vars = {}\n    for varname in variable_names:\n        var = h5[varname]\n        name = varname.split('/')[-1]\n        # Convert attributes\n        attrs = {}\n        for k, v in var.attrs.items():\n            if k != 'DIMENSION_LIST':\n                if isinstance(v, bytes):\n                    attrs[k] = v.decode('utf-8')\n                else:\n                    attrs[k] = v\n        data = var[:]\n        if '_FillValue' in attrs:\n            data = np.where(data < attrs['_FillValue'], data, np.nan)\n        data_vars[name] = (['segment'], data, attrs)\n    is2_ds = xr.Dataset(data_vars)\n    \nis2_ds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:                (segment: 235584)\nDimensions without coordinates: segment\nData variables:\n    latitude               (segment) float64 82.38 82.38 82.38 ... 72.61 72.61\n    longitude              (segment) float64 -55.11 -55.11 ... 145.1 145.1\n    delta_time             (segment) float64 4.642e+07 4.642e+07 ... 4.642e+07\n    height_segment_height  (segment) float32 nan nan nan ... -0.4335 -0.4463xarray.DatasetDimensions:segment: 235584Coordinates: (0)Data variables: (4)latitude(segment)float6482.38 82.38 82.38 ... 72.61 72.61contentType :referenceInformationcoordinates :delta_time longitudedescription :Latitude, WGS84, North=+, Lat of segment centerlong_name :Latitudesource :ATBD, section 4.4standard_name :latitudeunits :degrees_northvalid_max :90.0valid_min :-90.0array([82.38431982, 82.38431982, 82.38431982, ..., 72.60984638,\n       72.60977493, 72.60970985])longitude(segment)float64-55.11 -55.11 ... 145.1 145.1contentType :referenceInformationcoordinates :delta_time latitudedescription :Longitude, WGS84, East=+,Lon of segment centerlong_name :Longitudesource :ATBD, section 4.4standard_name :longitudeunits :degrees_eastvalid_max :180.0valid_min :-180.0array([-55.10896068, -55.10896068, -55.10896068, ..., 145.05396164,\n       145.05392851, 145.05389832])delta_time(segment)float644.642e+07 4.642e+07 ... 4.642e+07CLASS :DIMENSION_SCALENAME :gt1l/sea_ice_segments/delta_timeREFERENCE_LIST :[(<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)]contentType :physicalMeasurementcoordinates :latitude longitudedescription :Number of GPS seconds since the ATLAS SDP epoch. The ATLAS Standard Data Products (SDP) epoch offset is defined within /ancillary_data/atlas_sdp_gps_epoch as the number of GPS seconds between the GPS epoch (1980-01-06T00:00:00.000000Z UTC) and the ATLAS SDP epoch. By adding the offset contained within atlas_sdp_gps_epoch to delta time parameters, the time in gps_seconds relative to the GPS epoch can be computed.long_name :Elapsed GPS secondssource :telemetrystandard_name :timeunits :seconds since 2018-01-01array([46419293.64266939, 46419293.64266939, 46419293.64266939, ...,\n       46419681.87646231, 46419681.87759533, 46419681.87862704])height_segment_height(segment)float32nan nan nan ... -0.4335 -0.4463_FillValue :3.4028235e+38contentType :referenceInformationcoordinates :../delta_time ../latitude ../longitudedescription :Mean height from along-track segment fit detremined by the sea ice algorithm. The sea ice height is relative to the tide-free MSS.long_name :height of segment surfacesource :ATBD, section 4.2.2.4units :metersarray([        nan,         nan,         nan, ..., -0.46550068,\n       -0.43347716, -0.4462675 ], dtype=float32)Attributes: (0)\n\n\n\nis2_ds.height_segment_height.plot() ;"
  },
  {
    "objectID": "tutorials/04_On-Prem_Cloud.html#determine-the-ghrsst-modis-l2-granules-returned-from-our-time-and-area-of-interest",
    "href": "tutorials/04_On-Prem_Cloud.html#determine-the-ghrsst-modis-l2-granules-returned-from-our-time-and-area-of-interest",
    "title": "2021 Cloud Workshop at AGU",
    "section": "Determine the GHRSST MODIS L2 granules returned from our time and area of interest",
    "text": "Determine the GHRSST MODIS L2 granules returned from our time and area of interest\n\nresponse = requests.get(granule_url, \n                        params={\n                            'concept_id': modis_concept_id,\n                            'temporal': temporal,\n                            'bounding_box': bounding_box,\n                            'page_size': 200,\n                            },\n                        headers={\n                            'Accept': 'application/json'\n                            }\n                       )\nprint(response.headers['CMR-Hits'])\n\n14\n\n\n\ngranules = response.json()['feed']['entry']\nfor granule in granules:\n    print(f'{granule[\"title\"]} {granule[\"granule_size\"]} {granule[\"links\"][0][\"href\"]}')\n\n20190622000501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 20.71552562713623 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622000501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622014501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 9.34600830078125E-5 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622014501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622032501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 21.307741165161133 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622032501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622050001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 9.34600830078125E-5 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622050001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622050501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 20.065649032592773 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622050501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622064001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 9.34600830078125E-5 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622064001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622064501-JPL-L2P_GHRSST-SSTskin-MODIS_A-N-v02.0-fv01.0 18.602201461791992 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622064501-JPL-L2P_GHRSST-SSTskin-MODIS_A-N-v02.0-fv01.0.nc\n20190622064501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 18.665077209472656 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622064501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622082001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 19.782299995422363 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622082001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622100001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 19.13440227508545 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622100001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622113501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 20.3239164352417 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622113501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622114001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 9.34600830078125E-5 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622114001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622163001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 19.257243156433105 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622163001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622181001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 19.93498420715332 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622181001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc"
  },
  {
    "objectID": "tutorials/04_On-Prem_Cloud.html#load-data-into-xarray-via-s3-direct-access",
    "href": "tutorials/04_On-Prem_Cloud.html#load-data-into-xarray-via-s3-direct-access",
    "title": "2021 Cloud Workshop at AGU",
    "section": "Load data into xarray via S3 direct access",
    "text": "Load data into xarray via S3 direct access\nOur CMR granule search returned 14 files for our time and area of interest. However, not all granules will be suitable for analysis.\nI‚Äôve identified the image with granule id G1956158784-POCLOUD as a good candidate, this is the 9th granule. In this image, our area of interest is close to nadir. This means that the instantaneous field of view over the area of interest cover a smaller area than at the edge of the image.\nWe are looking for the link for direct download access via s3. This is a url but with a prefix s3://. This happens to be the first href link in the metadata.\nFor a single granule we can cut and paste the s3 link. If we have several granules, the s3 links can be extracted with some simple code.\n\ngranule = granules[9]\n\nfor link in granule['links']:\n    if link['href'].startswith('s3://'):\n        s3_link = link['href']\n        \ns3_link\n\n's3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622100001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc'\n\n\n\nGet S3 credentials\nAs with the previous S3 download tutorials we need credentials to access data from s3: access keys and tokens.\ns3_credentials = requests.get('https://archive.podaac.earthdata.nasa.gov/s3credentials').json()\nEssentially, what we are doing in this step is to ‚Äúmount‚Äù the s3 bucket as a file system. This allows us to treat the S3 bucket in a similar way to a local file system.\ns3_fs = s3fs.S3FileSystem(\n    key=s3_credentials[\"accessKeyId\"],\n    secret=s3_credentials[\"secretAccessKey\"],\n    token=s3_credentials[\"sessionToken\"],\n)\n\n\nOpen a s3 file\nNow we have the S3FileSystem set up, we can access the granule. xarray cannot open a S3File directly, so we use the open method for the S3FileSystem to open the granule using the endpoint url we extracted from the metadata. We also have to set the mode='rb'. This opens the granule in read-only mode and in byte-mode. Byte-mode is important. By default, open opens a file as text - in this case it would just be a string of characters - and xarray doesn‚Äôt know what to do with that.\nWe then pass the S3File object f to xarray.open_dataset. For this dataset, we also have to set decode_cf=False. This switch tells xarray not to use information contained in variable attributes to generate human readable coordinate variables. Normally, this should work for netcdf files but for this particular cloud-hosted dataset, variable attribute data is not in the form expected by xarray. We‚Äôll fix this.\nf = s3_fs.open(s3_link, mode='rb')\nmodis_ds = xr.open_dataset(f, decode_cf=False)\nIf you click on the Show/Hide Attributes icon (the first document-like icon to the right of coordinate variable metadata) you can see that attributes are one-element arrays containing bytestrings.\n\nmodis_ds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:                  (nj: 2030, ni: 1354, time: 1)\nCoordinates:\n  * time                     (time) int32 1214042401\nDimensions without coordinates: nj, ni\nData variables:\n    lat                      (nj, ni) float32 ...\n    lon                      (nj, ni) float32 ...\n    sea_surface_temperature  (time, nj, ni) int16 ...\n    sst_dtime                (time, nj, ni) int16 ...\n    quality_level            (time, nj, ni) int8 ...\n    sses_bias                (time, nj, ni) int8 ...\n    sses_standard_deviation  (time, nj, ni) int8 ...\n    l2p_flags                (time, nj, ni) int16 ...\n    chlorophyll_a            (time, nj, ni) float32 ...\n    K_490                    (time, nj, ni) int16 ...\n    wind_speed               (time, nj, ni) int8 ...\n    dt_analysis              (time, nj, ni) int8 ...\nAttributes: (12/49)\n    Conventions:                [b'CF-1.7, ACDD-1.3']\n    title:                      [b'MODIS Aqua L2P SST']\n    summary:                    [b'Sea surface temperature retrievals produce...\n    references:                 [b'GHRSST Data Processing Specification v2r5']\n    institution:                [b'NASA/JPL/OBPG/RSMAS']\n    history:                    [b'MODIS L2P created at JPL PO.DAAC']\n    ...                         ...\n    publisher_email:            [b'ghrsst-po@nceo.ac.uk']\n    processing_level:           [b'L2P']\n    cdm_data_type:              [b'swath']\n    startDirection:             [b'Ascending']\n    endDirection:               [b'Descending']\n    day_night_flag:             [b'Day']xarray.DatasetDimensions:nj: 2030ni: 1354time: 1Coordinates: (1)time(time)int321214042401long_name :[b'reference time of sst file']standard_name :[b'time']units :[b'seconds since 1981-01-01 00:00:00']comment :[b'time of first sensor observation']coverage_content_type :[b'coordinate']array([1214042401], dtype=int32)Data variables: (12)lat(nj, ni)float32...long_name :[b'latitude']standard_name :[b'latitude']units :[b'degrees_north']_FillValue :[-999.]valid_min :[-90.]valid_max :[90.]comment :[b'geographical coordinates, WGS84 projection']coverage_content_type :[b'coordinate'][2748620 values with dtype=float32]lon(nj, ni)float32...long_name :[b'longitude']standard_name :[b'longitude']units :[b'degrees_east']_FillValue :[-999.]valid_min :[-180.]valid_max :[180.]comment :[b'geographical coordinates, WGS84 projection']coverage_content_type :[b'coordinate'][2748620 values with dtype=float32]sea_surface_temperature(time, nj, ni)int16...long_name :[b'sea surface temperature']standard_name :[b'sea_surface_skin_temperature']units :[b'kelvin']_FillValue :[-32767]valid_min :[-1000]valid_max :[10000]comment :[b'sea surface temperature from thermal IR (11 um) channels']scale_factor :[0.005]add_offset :[273.15]source :[b'NASA and University of Miami']coordinates :[b'lon lat']coverage_content_type :[b'physicalMeasurement'][2748620 values with dtype=int16]sst_dtime(time, nj, ni)int16...long_name :[b'time difference from reference time']units :[b'seconds']_FillValue :[-32768]valid_min :[-32767]valid_max :[32767]comment :[b'time plus sst_dtime gives seconds after 00:00:00 UTC January 1, 1981']coordinates :[b'lon lat']coverage_content_type :[b'referenceInformation'][2748620 values with dtype=int16]quality_level(time, nj, ni)int8...long_name :[b'quality level of SST pixel']_FillValue :[-128]valid_min :[0]valid_max :[5]comment :[b'thermal IR SST proximity confidence value; signed byte array: WARNING Some applications are unable to properly handle signed byte values. If values are encountered > 127, please subtract 256 from this reported value']coordinates :[b'lon lat']flag_values :[0 1 2 3 4 5]flag_meanings :[b'no_data bad_data worst_quality low_quality acceptable_quality best_quality']coverage_content_type :[b'qualityInformation'][2748620 values with dtype=int8]sses_bias(time, nj, ni)int8...long_name :[b'SSES bias error based on proximity confidence flags']units :[b'kelvin']_FillValue :[-128]valid_min :[-127]valid_max :[127]comment :[b'thermal IR SST bias error; signed byte array: WARNING Some applications are unable to properly handle signed byte values. If values are encountered > 127, please subtract 256 from this reported value']scale_factor :[0.15748031]add_offset :[0.]coordinates :[b'lon lat']coverage_content_type :[b'auxiliaryInformation'][2748620 values with dtype=int8]sses_standard_deviation(time, nj, ni)int8...long_name :[b'SSES standard deviation error based on proximity confidence flags']units :[b'kelvin']_FillValue :[-128]valid_min :[-127]valid_max :[127]comment :[b'thermal IR SST standard deviation error; signed byte array: WARNING Some applications are unable to properly handle signed byte values. If values are encountered > 127, please subtract 256 from this reported value']scale_factor :[0.07874016]add_offset :[10.]coordinates :[b'lon lat']coverage_content_type :[b'auxiliaryInformation'][2748620 values with dtype=int8]l2p_flags(time, nj, ni)int16...long_name :[b'L2P flags']valid_min :[0]valid_max :[16]comment :[b'These flags can be used to further filter data variables']coordinates :[b'lon lat']flag_meanings :[b'microwave land ice lake river']flag_masks :[ 1  2  4  8 16]coverage_content_type :[b'qualityInformation'][2748620 values with dtype=int16]chlorophyll_a(time, nj, ni)float32...long_name :[b'Chlorophyll Concentration, OC3 Algorithm']units :[b'mg m^-3']_FillValue :[-32767.]valid_min :[0.001]valid_max :[100.]comment :[b'non L2P core field']coordinates :[b'lon lat']coverage_content_type :[b'auxiliaryInformation'][2748620 values with dtype=float32]K_490(time, nj, ni)int16...long_name :[b'Diffuse attenuation coefficient at 490 nm (OBPG)']units :[b'm^-1']_FillValue :[-32767]valid_min :[50]valid_max :[30000]comment :[b'non L2P core field']scale_factor :[0.0002]add_offset :[0.]coordinates :[b'lon lat']coverage_content_type :[b'auxiliaryInformation'][2748620 values with dtype=int16]wind_speed(time, nj, ni)int8...long_name :[b'10m wind speed']standard_name :[b'wind_speed']units :[b'm s-1']_FillValue :[-128]valid_min :[-127]valid_max :[127]comment :[b'Wind at 10 meters above the sea surface']scale_factor :[0.2]add_offset :[25.]source :[b'TBD.  Placeholder.  Currently empty']coordinates :[b'lon lat']grid_mapping :[b'TBD']time_offset :[2.]height :[b'10 m']coverage_content_type :[b'auxiliaryInformation'][2748620 values with dtype=int8]dt_analysis(time, nj, ni)int8...long_name :[b'deviation from SST reference climatology']units :[b'kelvin']_FillValue :[-128]valid_min :[-127]valid_max :[127]comment :[b'TBD']scale_factor :[0.1]add_offset :[0.]source :[b'TBD. Placeholder.  Currently empty']coordinates :[b'lon lat']coverage_content_type :[b'auxiliaryInformation'][2748620 values with dtype=int8]Attributes: (49)Conventions :[b'CF-1.7, ACDD-1.3']title :[b'MODIS Aqua L2P SST']summary :[b'Sea surface temperature retrievals produced at the NASA OBPG for the MODIS Aqua sensor.  These have been reformatted to GHRSST GDS specifications by the JPL PO.DAAC']references :[b'GHRSST Data Processing Specification v2r5']institution :[b'NASA/JPL/OBPG/RSMAS']history :[b'MODIS L2P created at JPL PO.DAAC']comment :[b'L2P Core without DT analysis or other ancillary fields; Day, Start Node:Ascending, End Node:Descending; WARNING Some applications are unable to properly handle signed byte values. If values are encountered > 127, please subtract 256 from this reported value; Refined']license :[b'GHRSST and PO.DAAC protocol allow data use as free and open.']id :[b'MODIS_A-JPL-L2P-v2019.0']naming_authority :[b'org.ghrsst']product_version :[b'2019.0']uuid :[b'f6e1f61d-c4a4-4c17-8354-0c15e12d688b']gds_version_id :[b'2.0']netcdf_version_id :[b'4.1']date_created :[b'20200221T085224Z']file_quality_level :[3]spatial_resolution :[b'1km']start_time :[b'20190622T100001Z']time_coverage_start :[b'20190622T100001Z']stop_time :[b'20190622T100459Z']time_coverage_end :[b'20190622T100459Z']northernmost_latitude :[89.9862]southernmost_latitude :[66.2723]easternmost_longitude :[-45.9467]westernmost_longitude :[152.489]source :[b'MODIS sea surface temperature observations for the OBPG']platform :[b'Aqua']sensor :[b'MODIS']metadata_link :[b'http://podaac.jpl.nasa.gov/ws/metadata/dataset/?format=iso&shortName=MODIS_A-JPL-L2P-v2019.0']keywords :[b'Oceans > Ocean Temperature > Sea Surface Temperature']keywords_vocabulary :[b'NASA Global Change Master Directory (GCMD) Science Keywords']standard_name_vocabulary :[b'NetCDF Climate and Forecast (CF) Metadata Convention']geospatial_lat_units :[b'degrees_north']geospatial_lat_resolution :[0.01]geospatial_lon_units :[b'degrees_east']geospatial_lon_resolution :[0.01]acknowledgment :[b'The MODIS L2P sea surface temperature data are sponsored by NASA']creator_name :[b'Ed Armstrong, JPL PO.DAAC']creator_email :[b'edward.m.armstrong@jpl.nasa.gov']creator_url :[b'http://podaac.jpl.nasa.gov']project :[b'Group for High Resolution Sea Surface Temperature']publisher_name :[b'The GHRSST Project Office']publisher_url :[b'http://www.ghrsst.org']publisher_email :[b'ghrsst-po@nceo.ac.uk']processing_level :[b'L2P']cdm_data_type :[b'swath']startDirection :[b'Ascending']endDirection :[b'Descending']day_night_flag :[b'Day']\n\n\nTo fix this, we need to extract array elements as scalars, and convert those scalars from bytestrings to strings. We use the decode method to do this. The bytestrings are encoded as utf-8, which is a unicode character format. This is the default encoding for decode but we‚Äôve included it as an argument to be explicit.\nNot all attributes are bytestrings. Some are floats. Take a look at _FillValue, and valid_min and valid_max. To avoid an error, we use the isinstance function to check if the value of an attributes is type bytes - a bytestring. If it is, then we decode it. If not, we just extract the scalar and do nothing else.\nWe also fix the global attributes.\ndef fix_attributes(da):\n    '''Decodes bytestring attributes to strings'''\n    for attr, value in da.attrs.items():\n        if isinstance(value[0], bytes):\n            da.attrs[attr] = value[0].decode('utf-8')\n        else:\n            da.attrs[attr] = value[0]\n    return\n\n# Fix variable attributes\nfor var in modis_ds.variables:\n    da = modis_ds[var]\n    fix_attributes(da)\n            \n# Fix global attributes\nfix_attributes(modis_ds)\nWith this done, we can use the xarray function decode_cf to convert the attributes.\nmodis_ds = xr.decode_cf(modis_ds)\n\nmodis_ds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:                  (nj: 2030, ni: 1354, time: 1)\nCoordinates:\n    lat                      (nj, ni) float32 ...\n    lon                      (nj, ni) float32 ...\n  * time                     (time) datetime64[ns] 2019-06-22T10:00:01\nDimensions without coordinates: nj, ni\nData variables:\n    sea_surface_temperature  (time, nj, ni) float32 ...\n    sst_dtime                (time, nj, ni) timedelta64[ns] ...\n    quality_level            (time, nj, ni) float32 ...\n    sses_bias                (time, nj, ni) float32 ...\n    sses_standard_deviation  (time, nj, ni) float32 ...\n    l2p_flags                (time, nj, ni) int16 ...\n    chlorophyll_a            (time, nj, ni) float32 ...\n    K_490                    (time, nj, ni) float32 ...\n    wind_speed               (time, nj, ni) float32 ...\n    dt_analysis              (time, nj, ni) float32 ...\nAttributes: (12/49)\n    Conventions:                CF-1.7, ACDD-1.3\n    title:                      MODIS Aqua L2P SST\n    summary:                    Sea surface temperature retrievals produced a...\n    references:                 GHRSST Data Processing Specification v2r5\n    institution:                NASA/JPL/OBPG/RSMAS\n    history:                    MODIS L2P created at JPL PO.DAAC\n    ...                         ...\n    publisher_email:            ghrsst-po@nceo.ac.uk\n    processing_level:           L2P\n    cdm_data_type:              swath\n    startDirection:             Ascending\n    endDirection:               Descending\n    day_night_flag:             Dayxarray.DatasetDimensions:nj: 2030ni: 1354time: 1Coordinates: (3)lat(nj, ni)float32...long_name :latitudestandard_name :latitudeunits :degrees_northvalid_min :-90.0valid_max :90.0comment :geographical coordinates, WGS84 projectioncoverage_content_type :coordinate[2748620 values with dtype=float32]lon(nj, ni)float32...long_name :longitudestandard_name :longitudeunits :degrees_eastvalid_min :-180.0valid_max :180.0comment :geographical coordinates, WGS84 projectioncoverage_content_type :coordinate[2748620 values with dtype=float32]time(time)datetime64[ns]2019-06-22T10:00:01long_name :reference time of sst filestandard_name :timecomment :time of first sensor observationcoverage_content_type :coordinatearray(['2019-06-22T10:00:01.000000000'], dtype='datetime64[ns]')Data variables: (10)sea_surface_temperature(time, nj, ni)float32...long_name :sea surface temperaturestandard_name :sea_surface_skin_temperatureunits :kelvinvalid_min :-1000valid_max :10000comment :sea surface temperature from thermal IR (11 um) channelssource :NASA and University of Miamicoverage_content_type :physicalMeasurement[2748620 values with dtype=float32]sst_dtime(time, nj, ni)timedelta64[ns]...long_name :time difference from reference timevalid_min :-32767valid_max :32767comment :time plus sst_dtime gives seconds after 00:00:00 UTC January 1, 1981coverage_content_type :referenceInformation[2748620 values with dtype=timedelta64[ns]]quality_level(time, nj, ni)float32...long_name :quality level of SST pixelvalid_min :0valid_max :5comment :thermal IR SST proximity confidence value; signed byte array: WARNING Some applications are unable to properly handle signed byte values. If values are encountered > 127, please subtract 256 from this reported valueflag_values :0flag_meanings :no_data bad_data worst_quality low_quality acceptable_quality best_qualitycoverage_content_type :qualityInformation[2748620 values with dtype=float32]sses_bias(time, nj, ni)float32...long_name :SSES bias error based on proximity confidence flagsunits :kelvinvalid_min :-127valid_max :127comment :thermal IR SST bias error; signed byte array: WARNING Some applications are unable to properly handle signed byte values. If values are encountered > 127, please subtract 256 from this reported valuecoverage_content_type :auxiliaryInformation[2748620 values with dtype=float32]sses_standard_deviation(time, nj, ni)float32...long_name :SSES standard deviation error based on proximity confidence flagsunits :kelvinvalid_min :-127valid_max :127comment :thermal IR SST standard deviation error; signed byte array: WARNING Some applications are unable to properly handle signed byte values. If values are encountered > 127, please subtract 256 from this reported valuecoverage_content_type :auxiliaryInformation[2748620 values with dtype=float32]l2p_flags(time, nj, ni)int16...long_name :L2P flagsvalid_min :0valid_max :16comment :These flags can be used to further filter data variablesflag_meanings :microwave land ice lake riverflag_masks :1coverage_content_type :qualityInformation[2748620 values with dtype=int16]chlorophyll_a(time, nj, ni)float32...long_name :Chlorophyll Concentration, OC3 Algorithmunits :mg m^-3valid_min :0.001valid_max :100.0comment :non L2P core fieldcoverage_content_type :auxiliaryInformation[2748620 values with dtype=float32]K_490(time, nj, ni)float32...long_name :Diffuse attenuation coefficient at 490 nm (OBPG)units :m^-1valid_min :50valid_max :30000comment :non L2P core fieldcoverage_content_type :auxiliaryInformation[2748620 values with dtype=float32]wind_speed(time, nj, ni)float32...long_name :10m wind speedstandard_name :wind_speedunits :m s-1valid_min :-127valid_max :127comment :Wind at 10 meters above the sea surfacesource :TBD.  Placeholder.  Currently emptygrid_mapping :TBDtime_offset :2.0height :10 mcoverage_content_type :auxiliaryInformation[2748620 values with dtype=float32]dt_analysis(time, nj, ni)float32...long_name :deviation from SST reference climatologyunits :kelvinvalid_min :-127valid_max :127comment :TBDsource :TBD. Placeholder.  Currently emptycoverage_content_type :auxiliaryInformation[2748620 values with dtype=float32]Attributes: (49)Conventions :CF-1.7, ACDD-1.3title :MODIS Aqua L2P SSTsummary :Sea surface temperature retrievals produced at the NASA OBPG for the MODIS Aqua sensor.  These have been reformatted to GHRSST GDS specifications by the JPL PO.DAACreferences :GHRSST Data Processing Specification v2r5institution :NASA/JPL/OBPG/RSMAShistory :MODIS L2P created at JPL PO.DAACcomment :L2P Core without DT analysis or other ancillary fields; Day, Start Node:Ascending, End Node:Descending; WARNING Some applications are unable to properly handle signed byte values. If values are encountered > 127, please subtract 256 from this reported value; Refinedlicense :GHRSST and PO.DAAC protocol allow data use as free and open.id :MODIS_A-JPL-L2P-v2019.0naming_authority :org.ghrsstproduct_version :2019.0uuid :f6e1f61d-c4a4-4c17-8354-0c15e12d688bgds_version_id :2.0netcdf_version_id :4.1date_created :20200221T085224Zfile_quality_level :3spatial_resolution :1kmstart_time :20190622T100001Ztime_coverage_start :20190622T100001Zstop_time :20190622T100459Ztime_coverage_end :20190622T100459Znorthernmost_latitude :89.9862southernmost_latitude :66.2723easternmost_longitude :-45.9467westernmost_longitude :152.489source :MODIS sea surface temperature observations for the OBPGplatform :Aquasensor :MODISmetadata_link :http://podaac.jpl.nasa.gov/ws/metadata/dataset/?format=iso&shortName=MODIS_A-JPL-L2P-v2019.0keywords :Oceans > Ocean Temperature > Sea Surface Temperaturekeywords_vocabulary :NASA Global Change Master Directory (GCMD) Science Keywordsstandard_name_vocabulary :NetCDF Climate and Forecast (CF) Metadata Conventiongeospatial_lat_units :degrees_northgeospatial_lat_resolution :0.01geospatial_lon_units :degrees_eastgeospatial_lon_resolution :0.01acknowledgment :The MODIS L2P sea surface temperature data are sponsored by NASAcreator_name :Ed Armstrong, JPL PO.DAACcreator_email :edward.m.armstrong@jpl.nasa.govcreator_url :http://podaac.jpl.nasa.govproject :Group for High Resolution Sea Surface Temperaturepublisher_name :The GHRSST Project Officepublisher_url :http://www.ghrsst.orgpublisher_email :ghrsst-po@nceo.ac.ukprocessing_level :L2Pcdm_data_type :swathstartDirection :AscendingendDirection :Descendingday_night_flag :Day\n\n\nLet‚Äôs make a quick plot to take a look at the sea_surface_temperature variable.\n\nmodis_ds.sea_surface_temperature.plot() ;\n\n\n\n\n\n\nPlot MODIS and ICESat-2 data on a map\n\nmap_proj = ccrs.NorthPolarStereo()\n\nfig = plt.figure(figsize=(10,5))\nax = fig.add_subplot(projection=map_proj)\nax.coastlines()\n\n# Plot MODIS sst, save object as sst_img, so we can add colorbar\nsst_img = ax.pcolormesh(modis_ds.lon, modis_ds.lat, modis_ds.sea_surface_temperature[0,:,:], \n                        vmin=240, vmax=270,  # Set max and min values for plotting\n                        cmap='viridis', shading='auto',   # shading='auto' to avoid warning\n                        transform=ccrs.PlateCarree())  # coords are lat,lon but map if NPS \n\n# Plot IS2 surface height \nis2_img = ax.scatter(is2_ds.longitude, is2_ds.latitude,\n                     c=is2_ds.height_segment_height, \n                     vmax=1.5,  # Set max height to plot\n                     cmap='Reds', alpha=0.6, s=2,\n                     transform=ccrs.PlateCarree())\n\n# Add colorbars\nfig.colorbar(sst_img, label='MODIS SST (K)')\nfig.colorbar(is2_img, label='ATL07 Height (m)')\n\n\n<matplotlib.colorbar.Colorbar at 0x7fd2eda24580>\n\n\n\n\n\n\n\nExtract SST coincident with ICESat-2 track\nThe MODIS SST is swath data, not a regularly-spaced grid of sea surface temperatures. ICESat-2 sea surface heights are irregularly spaced segments along one ground-track traced by the ATLAS instrument on-board ICESat-2. Fortunately, pyresample allows us to resample swath data.\npyresample has many resampling methods. We‚Äôre going to use the nearest neighbour resampling method, which is implemented using a k-dimensional tree algorithm or K-d tree. K-d trees are data structures that improve search efficiency for large data sets.\nThe first step is to define the geometry of the ICESat-2 and MODIS data. To do this we use the latitudes and longitudes of the datasets.\nis2_geometry = pyresample.SwathDefinition(lons=is2_ds.longitude,\n                                          lats=is2_ds.latitude)\nmodis_geometry = pyresample.SwathDefinition(lons=modis_ds.lon, lats=modis_ds.lat)\nWe then implement the resampling method, passing the two geometries we have defined, the data array we want to resample - in this case sea surface temperature, and a search radius. The resampling method expects a numpy.Array rather than an xarray.DataArray, so we use values to get the data as a numpy.Array.\nWe set the search radius to 1000 m. The MODIS data is nominally 1km spacing.\nsearch_radius=1000.\nfill_value = np.nan\nis2_sst = pyresample.kd_tree.resample_nearest(\n    modis_geometry,\n    modis_ds.sea_surface_temperature.values,\n    is2_geometry,\n    search_radius,\n    fill_value=fill_value\n)\n\nis2_sst\n\narray([263.375, 263.375, 263.375, ...,     nan,     nan,     nan],\n      dtype=float32)\n\n\n\nis2_ds['sea_surface_temperature'] = xr.DataArray(is2_sst, dims='segment')\nis2_ds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:                  (segment: 235584)\nDimensions without coordinates: segment\nData variables:\n    latitude                 (segment) float64 82.38 82.38 82.38 ... 72.61 72.61\n    longitude                (segment) float64 -55.11 -55.11 ... 145.1 145.1\n    delta_time               (segment) float64 4.642e+07 4.642e+07 ... 4.642e+07\n    height_segment_height    (segment) float32 nan nan nan ... -0.4335 -0.4463\n    sea_surface_temperature  (segment) float32 263.4 263.4 263.4 ... nan nan nanxarray.DatasetDimensions:segment: 235584Coordinates: (0)Data variables: (5)latitude(segment)float6482.38 82.38 82.38 ... 72.61 72.61contentType :referenceInformationcoordinates :delta_time longitudedescription :Latitude, WGS84, North=+, Lat of segment centerlong_name :Latitudesource :ATBD, section 4.4standard_name :latitudeunits :degrees_northvalid_max :90.0valid_min :-90.0array([82.38431982, 82.38431982, 82.38431982, ..., 72.60984638,\n       72.60977493, 72.60970985])longitude(segment)float64-55.11 -55.11 ... 145.1 145.1contentType :referenceInformationcoordinates :delta_time latitudedescription :Longitude, WGS84, East=+,Lon of segment centerlong_name :Longitudesource :ATBD, section 4.4standard_name :longitudeunits :degrees_eastvalid_max :180.0valid_min :-180.0array([-55.10896068, -55.10896068, -55.10896068, ..., 145.05396164,\n       145.05392851, 145.05389832])delta_time(segment)float644.642e+07 4.642e+07 ... 4.642e+07CLASS :DIMENSION_SCALENAME :gt1l/sea_ice_segments/delta_timeREFERENCE_LIST :[(<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)]contentType :physicalMeasurementcoordinates :latitude longitudedescription :Number of GPS seconds since the ATLAS SDP epoch. The ATLAS Standard Data Products (SDP) epoch offset is defined within /ancillary_data/atlas_sdp_gps_epoch as the number of GPS seconds between the GPS epoch (1980-01-06T00:00:00.000000Z UTC) and the ATLAS SDP epoch. By adding the offset contained within atlas_sdp_gps_epoch to delta time parameters, the time in gps_seconds relative to the GPS epoch can be computed.long_name :Elapsed GPS secondssource :telemetrystandard_name :timeunits :seconds since 2018-01-01array([46419293.64266939, 46419293.64266939, 46419293.64266939, ...,\n       46419681.87646231, 46419681.87759533, 46419681.87862704])height_segment_height(segment)float32nan nan nan ... -0.4335 -0.4463_FillValue :3.4028235e+38contentType :referenceInformationcoordinates :../delta_time ../latitude ../longitudedescription :Mean height from along-track segment fit detremined by the sea ice algorithm. The sea ice height is relative to the tide-free MSS.long_name :height of segment surfacesource :ATBD, section 4.2.2.4units :metersarray([        nan,         nan,         nan, ..., -0.46550068,\n       -0.43347716, -0.4462675 ], dtype=float32)sea_surface_temperature(segment)float32263.4 263.4 263.4 ... nan nan nanarray([263.375, 263.375, 263.375, ...,     nan,     nan,     nan],\n      dtype=float32)Attributes: (0)\n\n\n\n\nPlot SST and Height along track\nThis is a quick plot of the extracted data. We‚Äôre using matplotlib so we can use latitude as the x-value:\n\nis2_ds = is2_ds.set_coords(['latitude'])\n\nfig, ax1 = plt.subplots(figsize=(15, 7))\nax1.set_xlim(82.,88.)\nax1.plot(is2_ds.latitude, is2_ds.sea_surface_temperature, \n         color='orange', label='SST', zorder=3)\nax1.set_ylabel('SST (K)')\n\nax2 = ax1.twinx()\nax2.plot(is2_ds.latitude, is2_ds.height_segment_height, label='Height')\nax2.set_ylabel('Height (m)')\n\nfig.legend()\n\n<matplotlib.legend.Legend at 0x7fd2ef2ea880>"
  },
  {
    "objectID": "tutorials/01_Earthdata_Search.html",
    "href": "tutorials/01_Earthdata_Search.html",
    "title": "01. Earthdata Search",
    "section": "",
    "text": "This tutorial guides you through using Earthdata Search for NASA Earth observations search and discovery, and how to connect serach output (e.g.¬†download or access links) to a programmatic workflow in the cloud.\n\nStep 1. Go to Earthdata Search and Login\nGo to Earthdata Search https://search.earthdata.nasa.gov and use your Earthdata login credentials to log in. If you do not have an Earthdata account, please see the Workshop Prerequisites for guidance.\n\n\nStep 2. Search for dataset of interest\nUse the search box in the upper left to type key words. In this example we are interested in the ECCO dataset, hosted by the PO.DAAC. This dataset is available from the NASA Earthdata Cloud archive hosted in AWS cloud.\nClick on the ‚ÄúAvailable from AWS Cloud‚Äù filter option on the left. Here, 104 matching collections were found with the basic ECCO search.\n\n\n\nSearch for ECCO data available in AWS cloud\n\n\nLet‚Äôs refine our search further. Let‚Äôs search for ECCO monthly SSH in the search box (which will produce 37 matching collections), and for the time period for year 2015. The latter can be done using the ‚Äòcalendar‚Äô icon on the left under the search box.\nScroll down the list of returned matches until we see the dataset of interest, here ECCO Sea Surface Height - Monthly Mean 0.5 Degree (Version 4 Release 4).\nWe can click on the (i) icon for the dataset to read more details, including the dataset shortname (helpful for programmatic workflows) just below the dataset name; here ECCO_L4_SSH_05DEG_MONTHLY_V4R4.\n\n\n\nRefine search and temporal bounds\n\n\n\n\nStep 3. Explore the dataset details, including Cloud Access information\nScrolling down the info page for the dataset, we will see Cloud Access information, such as:\n\nwhether the dataset is available in the cloud,\nthe cloud region (all NASA Earthdata Cloud data is/will be in us-west-2 region) and\nthe S3 storage ‚Äòbucket‚Äô and ‚Äòobject prefix‚Äô where this data is located, and link to getting AWS credentials for data access. We will cover the latter in the Direct Access Tutorial.\n\n\n\n\nCloud access info in EDS\n\n\nPro Tip: Clicking on ‚ÄúFor Developers‚Äù to exapnd will provide programmatic endpoints such as those for the CMR API, and more. CMR API and CMR STAC API tutorials can be found on the 2021 Cloud Hackathon website.\nFor now, let‚Äôs say we are intersted in getting download or access links for specific data within this collection.\nAt the top of the dataset info section, click on Search Results, which will take us back to the list of datasets matching out search parameters. Clicking on the dataset (here again it‚Äôs the same ECCO Sea Surface Height - Monthly Mean 0.5 Degree (Version 4 Release 4)) we now see a list of files (granules) that are part of the dataset (collection).\n\n\nStep 4. Customize the download or data access\nClick on the + symbol to add a few files to our project. Here we added the first 3 listed for 2015. Then click on the green button towards the bottom ‚ÄúDownload‚Äù. This will take us to anther page with options to customize our download or access links.\n\n\n\nSelect granules and click download\n\n\n\nEntire file content\nLet‚Äôs stay we are interested in the entire file content, so we select the ‚ÄúDirect Download‚Äù option:\n\n\n\nCustomize your download or access\n\n\nClicking the green Download Data button again, will take us to the final page for instructions to download or links for data access in the cloud. You should see thre3 tabs: Download, AWS S3 Access, Download Script:\n  \n\n\nSubset or transform before download or access\nadd more here: Harmony, Opendap\n\n\n\nStep 5. Integrate file links into programmatic workflow, locally or in the AWS cloud.\nmore here ‚Ä¶. talk about direct access"
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "2021 Cloud Workshop at AGU",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nCloud Hackathon: Transitioning Earthdata Workflows to the Cloud is co-hosted by NASA‚Äôs PO.DAAC, NSIDC DAAC, LP.DAAC, with support from ASDC DAAC, GES DISC and the NASA Openscapes Project, and cloud computing infrastructure by 2i2c."
  },
  {
    "objectID": "tutorials/02_NASA_Earthdata_Authentication.html#summary",
    "href": "tutorials/02_NASA_Earthdata_Authentication.html#summary",
    "title": "2021 Cloud Workshop at AGU",
    "section": "Summary",
    "text": "Summary\nThis notebook creates a hidden .netrc file (_netrc for Window OS) with Earthdata login credentials in your home directory. This file is needed to access NASA Earthdata assets from a scripting environment like Python.\n\nEarthdata Login\nAn Earthdata Login account is required to access data, as well as discover restricted data, from the NASA Earthdata system. Thus, to access NASA data, you need Earthdata Login. Please visit https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account. This account is free to create and only takes a moment to set up.\n\n\nAuthentication via netrc File\nYou will need a netrc file containing your NASA Earthdata Login credentials in order to execute the notebooks. A netrc file can be created manually within text editor and saved to your home directory. An example of the required content is below.\nmachine urs.earthdata.nasa.gov\nlogin <USERNAME>\npassword <PASSWORD>\n<USERNAME> and <PASSWORD> would be replaced by your actual Earthdata Login username and password respectively."
  },
  {
    "objectID": "tutorials/02_NASA_Earthdata_Authentication.html#import-required-packages",
    "href": "tutorials/02_NASA_Earthdata_Authentication.html#import-required-packages",
    "title": "2021 Cloud Workshop at AGU",
    "section": "Import Required Packages",
    "text": "Import Required Packages\nfrom netrc import netrc\nfrom subprocess import Popen\nfrom platform import system\nfrom getpass import getpass\nimport os\nThe code below will:\n\ncheck what operating system (OS) is being used to determine which netrc file to check for/create (.netrc or _netrc)\ncheck if you have an netrc file, and if so, varify if those credentials are for the Earthdata endpoint\ncreate a netrc file if a netrc file is not present.\n\nurs = 'urs.earthdata.nasa.gov'    # Earthdata URL endpoint for authentication\nprompts = ['Enter NASA Earthdata Login Username: ',\n           'Enter NASA Earthdata Login Password: ']\n\n# Determine the OS (Windows machines usually use an '_netrc' file)\nnetrc_name = \"_netrc\" if system()==\"Windows\" else \".netrc\"\n\n# Determine if netrc file exists, and if so, if it includes NASA Earthdata Login Credentials\ntry:\n    netrcDir = os.path.expanduser(f\"~/{netrc_name}\")\n    netrc(netrcDir).authenticators(urs)[0]\n\n# Below, create a netrc file and prompt user for NASA Earthdata Login Username and Password\nexcept FileNotFoundError:\n    homeDir = os.path.expanduser(\"~\")\n    Popen('touch {0}{2} | echo machine {1} >> {0}{2}'.format(homeDir + os.sep, urs, netrc_name), shell=True)\n    Popen('echo login {} >> {}{}'.format(getpass(prompt=prompts[0]), homeDir + os.sep, netrc_name), shell=True)\n    Popen('echo \\'password {} \\'>> {}{}'.format(getpass(prompt=prompts[1]), homeDir + os.sep, netrc_name), shell=True)\n    # Set restrictive permissions\n    Popen('chmod 0600 {0}{1}'.format(homeDir + os.sep, netrc_name), shell=True)\n\n    # Determine OS and edit netrc file if it exists but is not set up for NASA Earthdata Login\nexcept TypeError:\n    homeDir = os.path.expanduser(\"~\")\n    Popen('echo machine {1} >> {0}{2}'.format(homeDir + os.sep, urs, netrc_name), shell=True)\n    Popen('echo login {} >> {}{}'.format(getpass(prompt=prompts[0]), homeDir + os.sep, netrc_name), shell=True)\n    Popen('echo \\'password {} \\'>> {}{}'.format(getpass(prompt=prompts[1]), homeDir + os.sep, netrc_name), shell=True)\n\nSee if the file was created\nIf the file was created, we‚Äôll see a .netrc file (_netrc for Window OS) in the list printed below. To view the contents from a Jupyter environment, click File on the top toolbar, select Open from Path‚Ä¶, type .netrc, and click Open. The .netrc file will open within the text editor.\n\n!!! Beware, your password will be visible if the .netrc file is opened in the text editor.\n\n!ls -al ~/"
  }
]