[
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "2021 Cloud Workshop at AGU",
    "section": "Welcome",
    "text": "Welcome to the 2021 Cloud Workshop at AGU: Enabling Analysis in the Cloud Using NASA Earth Science Data, co-hosted by the NASA EOSDIS Land Processes Distributed Active Archive Center (LP.DAAC), National Snow and Ice Data Center DAAC (NSIDC DAAC), Physical Oceanography Distributed Active Archive Center (PO.DAAC), with support provided by ASDC DAAC, GES DISC, IMPACT, and NASA Openscapes.\nThe Cloud Workshop will take place virtually on December 12, 2021, from 8am-12pm CST (UTC-6) in AGU session SCIWS31. Registration is required."
  },
  {
    "objectID": "index.html#about",
    "href": "index.html#about",
    "title": "2021 Cloud Workshop at AGU",
    "section": "About",
    "text": "The 2021 Cloud Workshop at AGU: Enabling Analysis in the Cloud Using NASA Earth Science Data is a virtual half-day collaborative open science learning experience aimed at exploring, learning, and promoting effective cloud-based science and applications workflows using NASA Earthdata Cloud data, tools, and services (among others), in support of Earth science data processing and analysis in the era of big data."
  },
  {
    "objectID": "logistics/schedule.html#workshop-schedule",
    "href": "logistics/schedule.html#workshop-schedule",
    "title": "Schedule",
    "section": "Workshop Schedule",
    "text": "Time, CST (UTC-6)\nEvent\nLeads/Instructors\n\n\n\n\n8:00 am\nWelcome\nCatalina & Cyndi (?)\n\n\n8:15 am\nTutorial 0: Quick orientation 2i2c\nLuis (?)\n\n\n8:30 am\nTutorial 1: EarthData Search\n(?)\n\n\n9:15 am\nTutorial 2: EarthData Authentication\n(?)\n\n\n9:30 am\nBreak\n\n\n\n9:45 am\nTutorial 3: Direct S3 Access\nAaron (?)\n\n\n10:30 am\nTutorial 4: Cloud-non-cloud\nAMY (?)\n\n\n11:15 am\nQ&A\nAll\n\n\n11:45 am\nClosing (survey and next opportunities)\nJulie/Erin/Catalina (?)"
  },
  {
    "objectID": "logistics/index.html#for-workshop-participants",
    "href": "logistics/index.html#for-workshop-participants",
    "title": "Logistics overview",
    "section": "For Workshop Participants",
    "text": "Before the workshop, please complete the prerequisites."
  },
  {
    "objectID": "logistics/prerequisites.html#prerequisites",
    "href": "logistics/prerequisites.html#prerequisites",
    "title": "Prerequisites & help",
    "section": "Prerequisites",
    "text": "If you would like to follow along hands-on during the Workshop, please do the following (20 minutes). All software or accounts are free.\n\nGitHub username\n\nCreate a GitHub account (if you don’t already have one) at https://github.com. Follow optional advice on choosing your username\nPlease provide your GitHub username here; this will allow us to add you to the cloud hackathon workspace.\nRemember your username and password; you will need to be logged in during the workshop!\n\nEarthdata Login account\n\nCreate an Earthdata Login account (if you don’t already have one) at https://urs.earthdata.nasa.gov\nRemember your username and password; you will need to be logged in during the workshop!\n\nZoom\n\nBe prepared to call into Zoom using the link provided in the Slack 2021-nasacloudworkshop-agu Channel.\n\nGet comfortable\n\nConsider your computer set-up in advance, including an external monitor if possible. You can follow along in Jupyter Hub on your own computer while also watching an instructor demo over Zoom (or equivalent), and will also want quick-access to Slack to ask for help and follow links."
  },
  {
    "objectID": "logistics/prerequisites.html#getting-help",
    "href": "logistics/prerequisites.html#getting-help",
    "title": "Prerequisites & help",
    "section": "Getting help",
    "text": "We will use Zoom Chat as our main channel for help. Please use this to post questions and request a breakout room."
  },
  {
    "objectID": "tutorials/index.html",
    "href": "tutorials/index.html",
    "title": "Tutorials Overview",
    "section": "",
    "text": "These tutorials are a combination of narrative, links, code, and outputs. They have been developed for live demos during the Workshop, and are available for self-paced learning.\nTutorials are markdown (.md) and Jupyter (.ipynb) notebooks, and are available on GitHub:\nhttps://github.com/NASA-Openscapes/2021-Cloud-Workshop-AGU/tree/main/tutorials."
  },
  {
    "objectID": "tutorials/01_Earthdata_Search.html",
    "href": "tutorials/01_Earthdata_Search.html",
    "title": "01. Earthdata Search",
    "section": "",
    "text": "This tutorial guides you through using Earthdata Search for NASA Earth observations search and discovery, and how to connect serach output (e.g. download or access links) to a programmatic workflow in the cloud.\n\nStep 1. Go to Earthdata Search and Login\nGo to Earthdata Search https://search.earthdata.nasa.gov/ and use your Earthdata login credentials to log in. If you do not have an Earthdata account, please see the Workshop Prerequisites page for guidance.\n\n\nStep 2. Search for dataset of interest\nUse the search box in the upper left to type key words. In this example we are interested in the ECCO dataset, hosted by the PO.DAAC. This dataset is available from the NASA Earthdata Cloud archive hosted in AWS cloud.\nClick on the “Available from AWS Cloud” filter option.\n\n\n\nSearch for ECCO data available in AWS cloud"
  },
  {
    "objectID": "tutorials/00_Setup.html#step-1.-login-to-the-hub",
    "href": "tutorials/00_Setup.html#step-1.-login-to-the-hub",
    "title": "00. Setup for tutorials",
    "section": "Step 1. Login to the Hub",
    "text": "Please go to https://openscapes.2i2c.cloud/hub/. Log in with your GitHub Account, and select “Small”.\nAlternatively, you can also click this badge to launch the Hub:\n\n\n\n\n\n\nNote: It takes a few minutes for the Hub to load. Please be patient!\n\nWhile the Hub loads, we’ll:\n\nDiscuss cloud environments\nSee how my Desktop is setup\nFork the Hackathon repository at github.com\nDiscuss python and conda environments\n\nThen, when the Hub is loaded, we’ll get oriented in the Hub and clone the forked repository into our cloud environment."
  },
  {
    "objectID": "tutorials/00_Setup.html#discussion-cloud-environment",
    "href": "tutorials/00_Setup.html#discussion-cloud-environment",
    "title": "00. Setup for tutorials",
    "section": "Discussion: Cloud environment",
    "text": "A brief overview about the NASA Openscapes Cloud Environment (following lessons from the Clinic).\n\nCloud infrastructure\n\nCloud: AWS us-west-2\n\nData: AWS S3 (cloud) and NASA DAAC data centers (on-prem).\nCloud compute environment: 2i2c Jupyterhub deployment\n\nIDE: JupyterLab"
  },
  {
    "objectID": "tutorials/00_Setup.html#discussion-my-desktop-setup",
    "href": "tutorials/00_Setup.html#discussion-my-desktop-setup",
    "title": "00. Setup for tutorials",
    "section": "Discussion: My desktop setup",
    "text": "I’ll screenshare to show and/or talk through how I have oriented the following software we’re using:\n\n2i2c Jupyterhub (our main workspace)\nHackathon Repo <> Hackathon Book (my teaching notes, your reference material)\nZoom Chat\nSlack"
  },
  {
    "objectID": "tutorials/00_Setup.html#step-2.-fork-the-hackathon-github-repository",
    "href": "tutorials/00_Setup.html#step-2.-fork-the-hackathon-github-repository",
    "title": "00. Setup for tutorials",
    "section": "Step 2. Fork the Hackathon GitHub repository",
    "text": "“How do I get the tutorial repository into the Hub?”. There are 2 steps. The first is from GitHub.com to fork the tutorial repository so that there is a connected copy in your user account that you can edit and push changes that won’t affect the nasa-openscapes copy.\nGo to https://github.com/nasa-openscapes/2021-Cloud-Hackathon and fork the repository.\n\nNote: if you’ve already done this in the Pre-Hackathon Clinic, you’ll need to make sure you have the latest, following the daily setup instructions."
  },
  {
    "objectID": "tutorials/00_Setup.html#discussion-python-and-conda-environments",
    "href": "tutorials/00_Setup.html#discussion-python-and-conda-environments",
    "title": "00. Setup for tutorials",
    "section": "Discussion: Python and Conda environments",
    "text": "Why Python?\n\n\n\nPython Data Stack. Source: Jake VanderPlas, “The State of the Stack,” SciPy Keynote (SciPy 2015).\n\n\nDefault Python Environment:\nWe’ve set up the Python environment with conda.\n\n\n\n\n\n\nConda environment\n\n\n\n\n\nname: openscapes\nchannels:\n  - conda-forge\ndependencies:\n  - python=3.9\n  - pangeo-notebook\n  - awscli~=1.20\n  - boto3~=1.19\n  - gdal~=3.3\n  - rioxarray~=0.8\n  - xarray~=0.19\n  - h5netcdf~=0.11\n  - netcdf4~=1.5\n  - h5py~=2.10\n  - geoviews~=1.9\n  - matplotlib-base~=3.4\n  - hvplot~=0.7\n  - pyproj~=3.2\n  - bqplot~=0.12\n  - geopandas~=0.10\n  - zarr~=2.10\n  - cartopy~=0.20\n  - shapely==1.7.1\n  - pyresample~=1.22\n  - joblib~=1.1\n  - pystac-client~=0.3\n  - s3fs~=2021.7\n  - ipyleaflet~=0.14\n  - sidecar~=0.5\n  - jupyterlab-geojson~=3.1\n  - jupyterlab-git\n  - jupyter-resource-usage\n  - ipympl~=0.6\n  - conda-lock~=0.12\n  - pooch~=1.5\n  - pip\n  - pip:\n    - tqdm\n    - harmony-py\n    - earthdata\n    - zarr-eosdis-store\n\n\n\n\nBash terminal and installed software\nLibraries that are available from the terminal\n\ngdal 3.3 commands ( gdalinfo, gdaltransform…)\nhdf5 commands ( h5dump, h5ls..)\nnetcdf4 commands (ncdump, ncinfo …)\njq (parsing json files or streams from curl)\ncurl (fetch resources from the web)\nawscli (AWS API client, to interact with AWS cloud services)\nvim (editor)\ntree ( directory tree)\nmore …\n\n\n\nUpdating the environment\nScientific Python is a vast space and we only included libraries that are needed in our tutorials. Our default environment can be updated to include any Python library that’s available on pip or conda.\nThe project used to create our default environment is called corn (as it can include many Python kernels).\nIf we want to update a library or install a whole new environment we need to open an issue on this repository. We can help your teams do this during project hacktime.\n\n\ncorn 🌽"
  },
  {
    "objectID": "tutorials/00_Setup.html#step-3.-jupyterhub-orientation",
    "href": "tutorials/00_Setup.html#step-3.-jupyterhub-orientation",
    "title": "00. Setup for tutorials",
    "section": "Step 3. JupyterHub orientation",
    "text": "Now that the Hub is loaded, let’s get oriented.\n\n\n\n\n\n\nFirst impressions\n\nLauncher & the big blue button\n“home directory”"
  },
  {
    "objectID": "tutorials/00_Setup.html#step-4.-clone-the-hackathon-github-repository",
    "href": "tutorials/00_Setup.html#step-4.-clone-the-hackathon-github-repository",
    "title": "00. Setup for tutorials",
    "section": "Step 4. Clone the Hackathon GitHub repository",
    "text": "Now we’ll clone the GitHub repository, using a git extension for the JupyterHub. Go to your github account, and navigate to the repository that you just created by forking from the Openscapes repository.\nClick to copy the url for cloning the repository.\n\nNow, go to JupyterHub and click on the git extension in the left panel and then click the blue button “Clone a Repository”.\n\nThen, paste the repository link to the forked repository that you copied from your github account into the “Clone a repo” pop up window. Then click the blue “CLONE” button. It will take a few moments to clone the repository into your Hub.\nYour link should look like https://github.com/YOUR-USERNAME/2021-Cloud-Hackathon. For example, the link is https://github.com/virdi/2021-Cloud-Hackathon. Note that it include your github username in the repo link.\n\nAlternatively, you can use the terminal (command line) as per github workflows: first-time setup.\nOnce the repository is cloned, you will see a new directory in the “File Browser” panel on the left named “2021-Cloud-Hackathon”. In this directory, you have all hackathon material including the tutorials and this book to follow along during other Tutorials. You are all set.\n\n\nREMEMBER: This is your copy (or fork) of the hackathon materials and jupyter notebooks. So feel free to make any changes to the content of this repository."
  },
  {
    "objectID": "tutorials/00_Setup.html#jupyter-notebooks",
    "href": "tutorials/00_Setup.html#jupyter-notebooks",
    "title": "00. Setup for tutorials",
    "section": "Jupyter notebooks",
    "text": "Let’s get oriented to Jupyter notebooks, which we’ll use in all the tutorials."
  },
  {
    "objectID": "tutorials/00_Setup.html#how-do-i-end-my-session",
    "href": "tutorials/00_Setup.html#how-do-i-end-my-session",
    "title": "00. Setup for tutorials",
    "section": "How do I end my session?",
    "text": "(Also see How do I end my Openscapes session? Will I lose all of my work?)\nWhen you are finished working for the day it is important to explicitly log out of your Openscapes session. The reason for this is it will save us a bit of money! When you keep a session active it uses up AWS resources and keeps a series of virtual machines deployed.\nStopping the server happens automatically when you log out, so navigate to “File -> Log Out” and just click “Log Out”!\n!!! NOTE “logging out” - Logging out will NOT cause any of your work to be lost or deleted. It simply shuts down some resources. It would be equivalent to turning off your desktop computer at the end of the day."
  },
  {
    "objectID": "tutorials/00_Setup.html#step-5.-tracking-changes-optional",
    "href": "tutorials/00_Setup.html#step-5.-tracking-changes-optional",
    "title": "00. Setup for tutorials",
    "section": "Step 5. Tracking changes (Optional)",
    "text": "Now that you have forked and cloned the repository in your Hub, you can make changes (edit, add, and/or delete content) and track these files using git. In this step, we will provide an overview of how to use git using the graphical interface (the JupyterLab git extension).\n\nStep 5.1. Configure Git (git config)\nConfigure git with your name and email address as shown here.\ngit config --global user.name \"Makhan Virdi\"\ngit config --global user.email \"Makhan.Virdi@gmail.com\"\nOpen a new terminal: File >> New >> Terminal\n\nConfigure git to store your github credentials to avoid having to enter your github username and token each time you push changes to your repository(in Step 5.5, we will describe how to use github token instead of a password)\ngit config --global credential.helper store\n\n\nStep 5.2. Create a new file\nLet’s create a new file: In the left panel on your Hub, click on the “directory” icon and then double click on “2021-Cloud-Hackathon” directory. Then, create a new file using the text editor in your 2i2c JupyterHub (File >> New >> Text File). Add some text to this file, for example: A test file. Save this file and rename it to test.txt.\n\n\n\nStep 5.3. Track the changes to the new file (git add)\nClick the git icon in the left panel. You can see that the newly added file is in the “Untracked” section. You can click the + icon next to the file name to let git track this file for changes.\n\n\n\nStep 5.4. Commit the changes to the new file (git commit)\nNow, you will see that the file is Staged, which means that git is ready to take a snapshot of this file (and the repository) with the changes that you made. This snapshot is called a commit. To commit the changes, add a note (called a commit message) by typing in the text box that say “Summary”.\nNow, click the blue “COMMIT” button to commit this change.\n\nNote: A short message indicating the type of change to this file is a good practice. Optionally, a longer description may be added to the “Description” field.\n\n\n\n\nStep 5.5. Transmit committed changes to your github (git push) {#step-5.5.-transmit-committed-changes-to-your-github-(git-push}\nAt this stage, you have committed the changes to your git repository on your Hub. However, these changes are still on your Hub and needs to be transmitted to your repository on github (so that both the local copy on the JupyterHub and the remote copy on github are in sync).\nAs seen in the picture below, the git extension indicates (with an orange dot on the cloud icon) that it is ready to push your changes to the remote (remote = your repository on github.com). To push to github, click the cloud button with an up arrow (circled in red in the picture).\n\nThe git extension in the Hub will prompt you to enter your github.com credentials. Enter you github.com username and a Personal Access Token (DO NOT use your password). To create a Personal Access Token, visit https://github.com/settings/tokens/new and create a new token with the permission as per the image below and specify its validity for 90 days.\n\n\nIMPORTANT: You will see this token only once, so be sure to copy this. If you do not copy your token at this stage, you will need to generate a new token.\n\nOnce you generate the token, copy it and paste in the Hub window that prompted you to enter the “Personal Access Token”.\n\nGit will show a message at the bottom right telling that the changes were “Successfully pushed”. Also, you will see that the “cloud icon with an up arrow” no longer has an orange dot, indicating that there are no more committed changes to push to the remote (github.com).\n\nNote: You have configured git extension to store your credentials. You will not be prompted for your login/token again!\n\n\nThat’s all. You can use the same workflow (add > commit > push) for any other new or modified files!\n\n\nNote: If you are comfortable with the command line, you can use the Terminal (In Hub, New > Terminal) and follow the steps outlined in the Clinic section."
  },
  {
    "objectID": "external/cof-zarr-reformat.html#getting-started",
    "href": "external/cof-zarr-reformat.html#getting-started",
    "title": "2021 Cloud Workshop at AGU",
    "section": "Getting Started",
    "text": "We will access monthly ocean bottom pressure (OBP) data from ECCO V4r4 (10.5067/ECG5M-OBP44), which are provided as a monthly time series on a 0.5-degree latitude/longitude grid.\nThe data are archived in netCDF format. However, this notebook demonstration will request conversion to Zarr format for files covering the period between 2010 and 2018. Upon receiving our request, Harmony’s backend will convert the files and stage them in S3 for native access in AWS (us-west-2 region, specifically). We will access the new Zarr datasets as an aggregated dataset using xarray, and leverage the S3 native protocols for direct access to the data in an efficient manner.\n\n\nRequirements\n\nAWS\nThis notebook should be running in an EC2 instance in AWS region us-west-2, as previously mentioned. We recommend using an EC2 with at least 8GB of memory available.\nThe notebook was developed and tested using a t2.large instance (2 cpus; 8GB memory).\n\n\nPython 3\nMost of these imports are from the Python standard library. However, you will need to install these packages into your Python 3 environment if you have not already done so:\n\ns3fs\nrequests\npandas\nxarray\nmatplotlib\n\n\n\n\nRequirements\nimport matplotlib.pyplot as plt\nimport xarray as xr\nimport pandas as pd\nimport numpy as np\nimport requests\nimport json\nimport time\nimport s3fs\n\nShortName = \"ECCO_L4_OBP_05DEG_MONTHLY_V4R4\"\n\n\nStudy period\nSet some “master” inputs to define the time and place contexts for our case studies in the ipynb. This example will be requesting time subsets and receiving global data back from the Harmony API.\nstart_date = \"2010-01-01\"\nend_date   = \"2018-12-31\"\n\n\nData Access\nSome features in the Harmony API require us to identify the target dataset/collection by its concept-id (which uniquely idenfifies it among the other datasets in the Common Metadata Repository). Support for selection by the dataset ShortName will be added in a future release.\n\nCommon Metadata Repository (CMR)\nFor now, we will need to get the concept-id that corresponds to our dataset by accessing its metadata from the CMR. Read more about the CMR at: https://cmr.earthdata.nasa.gov/\nRequest the UMM Collection metadata (i.e. metadata about the dataset) from the CMR and select the concept-id as a new variable ccid.\n\nresponse = requests.get(\n    url='https://cmr.earthdata.nasa.gov/search/collections.umm_json', \n    params={'provider': \"POCLOUD\",\n            'ShortName': ShortName,\n            'page_size': 1}\n)\n\nummc = response.json()['items'][0]\n\nccid = ummc['meta']['concept-id']\n\nccid\n\n'C1990404791-POCLOUD'\n\n\n\n\nHarmony API\nAnd get the Harmony API endpoint and zarr parameter like we did for SMAP before:\n\nbase = f\"https://harmony.earthdata.nasa.gov/{ccid}\"\nhreq = f\"{base}/ogc-api-coverages/1.0.0/collections/all/coverage/rangeset\"\nrurl = f\"{hreq}?format=application/x-zarr\"\n\nprint(rurl)\n\nhttps://harmony.earthdata.nasa.gov/C1990404791-POCLOUD/ogc-api-coverages/1.0.0/collections/all/coverage/rangeset?format=application/x-zarr\n\n\nECCO monthly collections have 312 granules in V4r4 (you can confirm with the granule listing from CMR Search API) so we can get the entire time series for 2010 to 2018 with one request to the Harmony API.\nFormat a string of query parameters to limit the processing to the desired time period. Then, append the string of time subset parameters to the variable rurl.\n\nsubs = '&'.join([f'subset=time(\"{start_date}T00:00:00.000Z\":\"{end_date}T23:59:59.999Z\")'])\n\nrurl = f\"{rurl}&{subs}\"\n\nprint(rurl)\n\nhttps://harmony.earthdata.nasa.gov/C1990404791-POCLOUD/ogc-api-coverages/1.0.0/collections/all/coverage/rangeset?format=application/x-zarr&subset=time(\"2010-01-01T00:00:00.000Z\":\"2018-12-31T23:59:59.999Z\")\n\n\nSubmit the request and monitor the processing status in a while loop, breaking it on completion of the request job:\n\nresponse = requests.get(url=rurl).json()\n\n# Monitor status in a while loop. Wait 10 seconds for each check.\nwait = 10\nwhile True:\n    response = requests.get(url=response['links'][0]['href']).json()\n    if response['status']!='running':\n        break\n    print(f\"Job in progress ({response['progress']}%)\")\n    time.sleep(wait)\n\nprint(\"DONE!\")\n\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nDONE!\n\n\nAccess the staged cloud datasets over native AWS interfaces\nCheck the message field in the response for clues about how to proceed:\n\nprint(response['message'])\n\nThe job has completed successfully. Contains results in AWS S3. Access from AWS us-west-2 with keys from https://harmony.earthdata.nasa.gov/cloud-access.sh\n\n\nThe third item in the list of links contains the shell script from the job status message printed above. Let’s download the same information in JSON format. It should be the fourth item; check to be sure:\n\nlen(response['links'])\n\n102\n\n\nSelect the url and download the json, then load to Python dictionary and print the keys:\n\nwith requests.get(response['links'][3]['href']) as r:\n    creds = r.json()\n\nprint(creds.keys())\n\ndict_keys(['AccessKeyId', 'SecretAccessKey', 'SessionToken', 'Expiration'])\n\n\nCheck the expiration timestamp for the temporary credentials:\n\ncreds['Expiration']\n\n'2021-06-11T02:36:29.000Z'\n\n\nOpen zarr datasets with s3fs and xarray\nGet the s3 output directory and list of zarr datasets from the list of links. The s3 directory should be the fifth item; the urls are from item six onward:\n\ns3_dir = response['links'][4]['href']\n\nprint(s3_dir)\n\ns3://harmony-prod-staging/public/harmony/netcdf-to-zarr/2295236b-8086-4543-9482-f524a9f2d0c3/\n\n\nNow select the URLs for the staged files and print the first one:\n\ns3_urls = [u['href'] for u in response['links'][5:]]\n\nprint(s3_urls[0])\n\ns3://harmony-prod-staging/public/harmony/netcdf-to-zarr/2295236b-8086-4543-9482-f524a9f2d0c3/OCEAN_BOTTOM_PRESSURE_mon_mean_2009-12_ECCO_V4r4_latlon_0p50deg.zarr\n\n\nUse the AWS s3fs package and your temporary aws_creds to open the zarr directory storage:\n\ns3 = s3fs.S3FileSystem(\n    key=creds['AccessKeyId'],\n    secret=creds['SecretAccessKey'],\n    token=creds['SessionToken'],\n    client_kwargs={'region_name':'us-west-2'},\n)\n\nlen(s3.ls(s3_dir))\n\n97\n\n\nPlot the first Ocean Bottom Pressure dataset\nCheck out the documentation for xarray’s open_zarr method at this link. Open the first dataset and plot the OBP variable:\n\nds0 = xr.open_zarr(s3.get_mapper(s3_urls[0]), decode_cf=True, mask_and_scale=True)\n\n# Mask the dataset where OBP is not within the bounds of the variable's valid min/max:\nds0_masked = ds0.where((ds0.OBP>=ds0.OBP.valid_min) & (ds0.OBP<=ds0.OBP.valid_max))\n\n# Plot the masked dataset\nds0_masked.OBP.isel(time=0).plot.imshow(size=10)\n\n<matplotlib.image.AxesImage at 0x7f28ed2ba4c0>\n\n\n\n\n\nLoad the zarr datasets into one large xarray dataset\nLoad all the datasets in a loop and concatenate them:\n\nzds = xr.concat([xr.open_zarr(s3.get_mapper(u)) for u in s3_urls], dim=\"time\")\n\nprint(zds)\n\n<xarray.Dataset>\nDimensions:         (latitude: 360, longitude: 720, nv: 2, time: 97)\nCoordinates:\n  * latitude        (latitude) float64 -89.75 -89.25 -88.75 ... 89.25 89.75\n    latitude_bnds   (latitude, nv) float64 -90.0 -89.5 -89.5 ... 89.5 89.5 90.0\n  * longitude       (longitude) float64 -179.8 -179.2 -178.8 ... 179.2 179.8\n    longitude_bnds  (longitude, nv) float64 -180.0 -179.5 -179.5 ... 179.5 180.0\n  * time            (time) datetime64[ns] 2009-12-16T12:00:00 ... 2017-12-16T...\n    time_bnds       (time, nv) datetime64[ns] dask.array<chunksize=(1, 2), meta=np.ndarray>\nDimensions without coordinates: nv\nData variables:\n    OBP             (time, latitude, longitude) float64 dask.array<chunksize=(1, 360, 720), meta=np.ndarray>\n    OBPGMAP         (time, latitude, longitude) float64 dask.array<chunksize=(1, 360, 720), meta=np.ndarray>\nAttributes: (12/57)\n    Conventions:                  CF-1.8, ACDD-1.3\n    acknowledgement:              This research was carried out by the Jet Pr...\n    author:                       Ian Fenty and Ou Wang\n    cdm_data_type:                Grid\n    comment:                      Fields provided on a regular lat-lon grid. ...\n    coordinates_comment:          Note: the global 'coordinates' attribute de...\n    ...                           ...\n    time_coverage_duration:       P1M\n    time_coverage_end:            2010-01-01T00:00:00\n    time_coverage_resolution:     P1M\n    time_coverage_start:          2009-12-01T00:00:00\n    title:                        ECCO Ocean Bottom Pressure - Monthly Mean 0...\n    uuid:                         297c8df0-4158-11eb-b208-0cc47a3f687b\n\n\nReference OBP and mask the dataset according to the valid minimum and maximum:\n\nobp = zds.OBP\n\nprint(obp)\n\n<xarray.DataArray 'OBP' (time: 97, latitude: 360, longitude: 720)>\ndask.array<concatenate, shape=(97, 360, 720), dtype=float64, chunksize=(1, 360, 720), chunktype=numpy.ndarray>\nCoordinates:\n  * latitude   (latitude) float64 -89.75 -89.25 -88.75 ... 88.75 89.25 89.75\n  * longitude  (longitude) float64 -179.8 -179.2 -178.8 ... 178.8 179.2 179.8\n  * time       (time) datetime64[ns] 2009-12-16T12:00:00 ... 2017-12-16T06:00:00\nAttributes:\n    comment:                OBP excludes the contribution from global mean at...\n    coverage_content_type:  modelResult\n    long_name:              Ocean bottom pressure given as equivalent water t...\n    units:                  m\n    valid_max:              72.07011413574219\n    valid_min:              -1.7899188995361328\n\n\nGet the valid min and max from the corresponding CF attributes:\n\nobp_vmin, obp_vmax = obp.valid_min, obp.valid_max\n\nobp_vmin, obp_vmax\n\n(-1.7899188995361328, 72.07011413574219)\n\n\nMask the dataset according to the OBP min and max and plot a series:\n\n# Mask dataset where not inside OBP variable valid min/max:\nzds_masked = zds.where((obp>=obp_vmin)&(obp<=obp_vmax))\n\n# Plot SSH again for the first 12 time slices:\nobpp = zds_masked.OBP.isel(time=slice(0, 6)).plot(\n    x=\"longitude\", \n    y=\"latitude\", \n    col=\"time\",\n    levels=8,\n    col_wrap=3, \n    add_colorbar=False,\n    figsize=(14, 8)\n)\n\n# Plot a colorbar on a secondary axis\nmappable = obpp.axes[0][0].collections[0]\ncax = plt.axes([0.05, -0.04, 0.95, 0.04])\ncbar1 = plt.colorbar(mappable, cax=cax, orientation='horizontal')"
  },
  {
    "objectID": "external/zarr-eosdis-store.html",
    "href": "external/zarr-eosdis-store.html",
    "title": "2021 Cloud Workshop at AGU",
    "section": "",
    "text": "Zarr Example\nimported on: 2021-12-07\n\nThis notebook is from NASA’s Zarr EOSDIS store notebook\n\n\nThe original source for this document is https://github.com/nasa/zarr-eosdis-store\n\n\n\nzarr-eosdis-store example\nInstall dependencies\nimport sys\n\n# zarr and zarr-eosdis-store, the main libraries being demoed\n!{sys.executable} -m pip install zarr zarr-eosdis-store\n\n# Notebook-specific libraries\n!{sys.executable} -m pip install matplotlib\nImportant: To run this, you must first create an Earthdata Login account (https://urs.earthdata.nasa.gov) and place your credentials in ~/.netrc e.g.:\n   machine urs.earthdata.nasa.gov login YOUR_USER password YOUR_PASSWORD\nNever share or commit your password / .netrc file!\nBasic usage. After these lines, we work with ds as though it were a normal Zarr dataset\nimport zarr\nfrom eosdis_store import EosdisStore\n\nurl = 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210715090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc'\n\nds = zarr.open(EosdisStore(url))\nView the file’s variable structure\n\nprint(ds.tree())\n\n/\n ├── analysed_sst (1, 17999, 36000) int16\n ├── analysis_error (1, 17999, 36000) int16\n ├── dt_1km_data (1, 17999, 36000) int16\n ├── lat (17999,) float32\n ├── lon (36000,) float32\n ├── mask (1, 17999, 36000) int16\n ├── sea_ice_fraction (1, 17999, 36000) int16\n ├── sst_anomaly (1, 17999, 36000) int16\n └── time (1,) int32\n\n\nFetch the latitude and longitude arrays and determine start and end indices for our area of interest. In this case, we’re looking at the Great Lakes, which have a nice, recognizeable shape. Latitudes 41 to 49, longitudes -93 to 76.\nlats = ds['lat'][:]\nlons = ds['lon'][:]\nlat_range = slice(lats.searchsorted(41), lats.searchsorted(49))\nlon_range = slice(lons.searchsorted(-93), lons.searchsorted(-76))\nGet the analysed sea surface temperature variable over our area of interest and apply scale factor and offset from the file metadata. In a future release, scale factor and add offset will be automatically applied.\nvar = ds['analysed_sst']\nanalysed_sst = var[0, lat_range, lon_range] * var.attrs['scale_factor'] + var.attrs['add_offset']\nDraw a pretty picture\n\nfrom matplotlib import pyplot as plt\n\nplt.rcParams[\"figure.figsize\"] = [16, 8]\nplt.imshow(analysed_sst[::-1, :])\nNone\n\n\n\n\nIn a dozen lines of code and a few seconds, we have managed to fetch and visualize the 3.2 megabyte we needed from a 732 megabyte file using the original archive URL and no processing services"
  }
]