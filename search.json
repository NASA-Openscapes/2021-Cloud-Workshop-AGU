[
  {
    "objectID": "cloud-paradigm.html",
    "href": "cloud-paradigm.html",
    "title": "NASA and the Cloud Paradigm",
    "section": "",
    "text": "Slides that introduce NASA Earthdata Cloud & the Cloud Paradigm."
  },
  {
    "objectID": "logistics/prerequisites.html#prerequisites",
    "href": "logistics/prerequisites.html#prerequisites",
    "title": "Prerequisites & help",
    "section": "Prerequisites",
    "text": "Prerequisites\nIf you would like to follow along hands-on during the Workshop, please do the following (20 minutes). All software or accounts are free.\n\nGitHub username\n\nCreate a GitHub account (if you don’t already have one) at https://github.com. Follow optional advice on choosing your username\nWe will ask for your username at the beginning of the workshop\nRemember your username and password; you will need to be logged in during the workshop!\n\nEarthdata Login account\n\nCreate an Earthdata Login account (if you don’t already have one) at https://urs.earthdata.nasa.gov\nRemember your username and password; you will need to be logged in during the workshop!\n\nGet comfortable\n\nConsider your computer set-up in advance, including an external monitor if possible. You can follow along in Jupyter Hub on your own computer while also watching an instructor demo over Zoom (or equivalent), and will also want quick-access to Slack to ask for help and follow links."
  },
  {
    "objectID": "logistics/prerequisites.html#getting-help",
    "href": "logistics/prerequisites.html#getting-help",
    "title": "Prerequisites & help",
    "section": "Getting help",
    "text": "Getting help\nWe will use Zoom Chat and Google Doc notes as our main channels for help. Please use Zoom Chat to post questions, direct helpers to any screenshots you’ve pasted or conversations in the Google Doc, and request a breakout room.\nTo create a screenshot:\n\nOn your Mac - Screenshot\nOn your PC - Snipping Tool"
  },
  {
    "objectID": "logistics/index.html#for-workshop-participants",
    "href": "logistics/index.html#for-workshop-participants",
    "title": "Logistics overview",
    "section": "For Workshop Participants",
    "text": "For Workshop Participants\nBefore the workshop, please complete the prerequisites."
  },
  {
    "objectID": "logistics/schedule.html#workshop-schedule",
    "href": "logistics/schedule.html#workshop-schedule",
    "title": "Schedule",
    "section": "Workshop Schedule",
    "text": "Workshop Schedule\n\n\n\nTime, CST (UTC-6)\nEvent\nLeads/Instructors\n\n\n\n\n8:00 am\nWelcome\nErin Robinson & Julie Lowndes, Openscapes\n\n\n8:05 am\nNASA and the Cloud Paradigm\nCatalina Oaida, PO.DAAC & Cyndi Hall, NASA\n\n\n8:20 am\nTutorial 0: Orientation and setup\nLuis Lopez, NSIDC\n\n\n8:35 am\nTutorial 1: EarthData Search\nCatalina Oaida, PO.DAAC\n\n\n9:05 am\nTutorial 2: EarthData Authentication\nAaron Friesz, LP DAAC\n\n\n9:20 am\nBreak\n\n\n\n9:35 am\nTutorial 3: Direct S3 Access\nAaron Friesz, LP DAAC\n\n\n10:25 am\nTutorial 4: Cloud-non-cloud\nAmy Steiker, NSIDC\n\n\n11:15 am\nQ&A\nAll\n\n\n11:45 am\nClosing (survey and next opportunities)\nCatalina Oaida, PO.DAAC, Erin Robinson, Openscapes\n\n\n\n\nWelcome\n\nWelcome and Code of Conduct\n\nPlease see the NASA_CloudWorkshop_Notes Google Doc\n\n\n\n\n\nClosing\nSlides - Erin Robinson & Julie Lowndes\n\nThank you\nContinued hacking on the cloud - one week. You will continue to have access to the 2i2c JupyterHub in AWS for one week following the Cloud Hackathon so you can continue to work and we all learn more about what is involved with migrating data access and science workflows to the Cloud. This cloud compute environment is supported by the NASA Openscapes project.\nUpcoming events - all virtual\n\nAGU Open science in action session - December 17, 2021. Talks and tutorials by Hackathon Mentors, among other leaders in open science.\nNASA Openscapes Champions Cohort - March-April 2022. Openscapes will lead a NASA Champions Cohort for 7 research teams. This is a professional development and leadership opportunity for scientists that use data from NASA DAACs and are interested in collaborative open data science practices and migrating their workflows to the cloud. Nominate your team by February 1."
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "2021 Cloud Workshop at AGU",
    "section": "Welcome",
    "text": "Welcome\n\nWelcome to the 2021 Cloud Workshop at AGU: Enabling Analysis in the Cloud Using NASA Earth Science Data, co-hosted by the NASA EOSDIS Land Processes Distributed Active Archive Center (LP.DAAC), National Snow and Ice Data Center DAAC (NSIDC DAAC), Physical Oceanography Distributed Active Archive Center (PO.DAAC), with support provided by ASDC DAAC, GES DISC, IMPACT, and NASA Openscapes.\nThe Cloud Workshop will take place virtually on December 12, 2021, from 8am-12pm CST (UTC-6) in AGU session SCIWS31. Registration is required."
  },
  {
    "objectID": "index.html#about",
    "href": "index.html#about",
    "title": "2021 Cloud Workshop at AGU",
    "section": "About",
    "text": "About\nThe 2021 Cloud Workshop at AGU: Enabling Analysis in the Cloud Using NASA Earth Science Data is a virtual half-day collaborative open science learning experience aimed at exploring, learning, and promoting effective cloud-based science and applications workflows using NASA Earthdata Cloud data, tools, and services (among others), in support of Earth science data processing and analysis in the era of big data."
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "2021 Cloud Workshop at AGU",
    "section": "Acknowledgements",
    "text": "Acknowledgements\n2021 Cloud Workshop at AGU: Enabling Analysis in the Cloud Using NASA Earth Science Data is co-hosted by NASA’s PO.DAAC, NSIDC DAAC, LP.DAAC, with support from ASDC DAAC, GES DISC and the NASA Openscapes Project, and cloud computing infrastructure by 2i2c."
  },
  {
    "objectID": "tutorials/01_Earthdata_Search.html",
    "href": "tutorials/01_Earthdata_Search.html",
    "title": "01. Earthdata Search",
    "section": "",
    "text": "This tutorial guides you through how to use Earthdata Search for NASA Earth observations search and discovery, and how to connect the search output (e.g. download or access links) to a programmatic workflow (locally or from within the cloud).\n\nStep 1. Go to Earthdata Search and Login\nGo to Earthdata Search https://search.earthdata.nasa.gov and use your Earthdata login credentials to log in. If you do not have an Earthdata account, please see the Workshop Prerequisites for guidance.\n\n\nStep 2. Search for dataset of interest\nUse the search box in the upper left to type key words. In this example we are interested in the ECCO dataset, hosted by the PO.DAAC. This dataset is available from the NASA Earthdata Cloud archive hosted in AWS cloud.\nClick on the “Available from AWS Cloud” filter option on the left. Here, 104 matching collections were found with the basic ECCO search.\n\n\n\nFigure caption: Search for ECCO data available in AWS cloud in Earthdata Search portal\n\n\nLet’s refine our search further. Let’s search for ECCO monthly SSH in the search box (which will produce 39 matching collections), and for the time period for year 2015. The latter can be done using the calendar icon on the left under the search box.\nScroll down the list of returned matches until we see the dataset of interest, in this case ECCO Sea Surface Height - Monthly Mean 0.5 Degree (Version 4 Release 4).\nWe can click on the (i) icon for the dataset to read more details, including the dataset shortname (helpful for programmatic workflows) just below the dataset name; here ECCO_L4_SSH_05DEG_MONTHLY_V4R4.\n\n\n\nFigure caption: Refine search, set temporal bounds, get more information\n\n\n\n\nStep 3. Explore the dataset details, including Cloud Access information\nOnce we clicked the (i), scrolling down the info page for the dataset we will see Cloud Access information, such as:\n\nwhether the dataset is available in the cloud\nthe cloud Region (all NASA Earthdata Cloud data is/will be in us-west-2 region)\nthe S3 storage bucket and object prefix where this data is located\nlink that generates AWS S3 Credentials for in-cloud data access (we will cover this in the Direct Data Access Tutorials)\nlink to documentation describing the In-region Direct S3 Access to Buckets. Note: these will be unique depending on the DAAC where the data is archived. (We will show examples of direct in-region access in Tutorial 3.)\n\n\n\n\nFigure caption: Cloud access info in EDS\n\n\n\n\n\nFigure caption: Documentation describing the In-region Direct S3 Access to Buckets\n\n\nPro Tip: Clicking on “For Developers” to exapnd will provide programmatic endpoints such as those for the CMR API, and more. CMR API and CMR STAC API tutorials can be found on the 2021 Cloud Hackathon website.\nFor now, let’s say we are intersted in getting download link(s) or access link(s) for specific data files (granules) within this collection.\nAt the top of the dataset info section, click on Search Results, which will take us back to the list of datasets matching our search parameters. Clicking on the dataset (here again it’s the same ECCO Sea Surface Height - Monthly Mean 0.5 Degree (Version 4 Release 4)) we now see a list of files (granules) that are part of the dataset (collection).\n\n\nStep 4. Customize the download or data access\nClick on the green + symbol to add a few files to our project. Here we added the first 3 listed for 2015. Then click on the green button towards the bottom that says “Download”. This will take us to another page with options to customize our download or access link(s).\n\n\n\nFigure caption: Select granules and click download\n\n\n\n4.a. Entire file content\nLet’s stay we are interested in the entire file content, so we select the “Direct Download” option (as opposed to other options to subset or transform the data):\n\n\n\nFigure caption: Customize your download or access\n\n\nClicking the green Download Data button again, will take us to the final page for instructions to download and links for data access in the cloud. You should see three tabs: Download Files, AWS S3 Access, Download Script:\n  \nThe Download Files tab provides the https:// links for downloading the files locally. E.g.: https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/ECCO_L4_SSH_05DEG_MONTHLY_V4R4/SEA_SURFACE_HEIGHT_mon_mean_2015-09_ECCO_V4r4_latlon_0p50deg.nc\nThe AWS S3 Access tab provides the S3:// links, which is what we would use to access the data directly in-region (us-west-2) within the AWS cloud (an example will be shown in Tutorial 3). E.g.: s3://podaac-ops-cumulus-protected/ECCO_L4_SSH_05DEG_MONTHLY_V4R4/SEA_SURFACE_HEIGHT_mon_mean_2015-09_ECCO_V4r4_latlon_0p50deg.nc where s3 indicates data is stored in AWS S3 storage, podaac-ops-cumulus-protected is the bucket, and ECCO_L4_SSH_05DEG_MONTHLY_V4R4 is the object prefix (the latter two are also listed in the dataset collection information under Cloud Access (step 3 above)).\nTip: Another quicker way to find the bucket and object prefix is from the list of data files the search returns. Next to the + green button is a grey donwload symbol. Click on that to see the Download Files https:// links or on the AWS S3 Access to get the direct S3:// access links, which contain the bucket and object prefix where data is stored.\n\n\n4.b. Subset or transform before download or access\nDAAC tools and services are also being migrated or developed in the cloud, next to that data. These include the Harmony API and OPeNDAP in the cloud, as a few examples.\nWe can leverage these cloud-based services on cloud-archived data to reduce or transform the data (depending on need) before getting the access links regardless of whether we prefer to download the data and work on a local machine or whether we want to access the data in the cloud (from a cloud workspace). These can be useful data reduction services that support a faster time to science.\nHarmony\nHarmony allows you to seamlessly analyze Earth observation data from different NASA data centers. These services (API endpoints) provide data reduction (e.g. subsetting) and transfromation services (e.g. convert netCDF data to Zarr cloud optimized format).\n\n\n\nFigure caption: Leverage Harmony cloud-based data transformation services\n\n\nWhen you click the final green Download button, the links provided are to data that had been transformed based on our selections on the previous screen (here chosing to use the Harmony service to reformat the data to Zarr). These data are staged for us in an S3 bucket in AWS, and we can use the s3:// links to access those specific data. This service also provides STAC access links. This particular example is applicable if your workflow is in the AWS us-west-2 region.\n\n\n\nFigure caption: Harmony-staged data in S3\n\n\n\n\n\nStep 5. Integrate file links into programmatic workflow, locally or in the AWS cloud.\nIn tutorial 3 Direct Data Access, we will work programmatically in the cloud to access datasets of interest, to get us set up for further scientific analysis of choice. There are several ways to do this. One way to connect the search part of the workflow we just did in Earthdata Search to our next steps working in the cloud is to simply copy/paste the s3:// links provides in Step 4 above into a JupyterHub notebook or script in our cloud workspace, and continue the data analysis from there.\nOne could also copy/paste the s3:// links and save them in a text file, then open and read the text file in the notebook or script in the JupyterHub in the cloud.\nTutorial 3 will pick up from here and cover these next steps in more detail."
  },
  {
    "objectID": "tutorials/index.html",
    "href": "tutorials/index.html",
    "title": "Tutorials Overview",
    "section": "",
    "text": "These tutorials are a combination of narrative, links, code, and outputs. They have been developed for live demos during the Workshop, and are available for self-paced learning.\nTutorials are markdown (.md) and Jupyter (.ipynb) notebooks, and are available on GitHub:\nhttps://github.com/NASA-Openscapes/2021-Cloud-Workshop-AGU/tree/main/tutorials."
  },
  {
    "objectID": "tutorials/00_Setup.html#step-1.-login-to-the-hub",
    "href": "tutorials/00_Setup.html#step-1.-login-to-the-hub",
    "title": "00. Setup for tutorials",
    "section": "Step 1. Login to the Hub",
    "text": "Step 1. Login to the Hub\nPlease go to the Jupyter Hub and Log in with your GitHub Account, and select “Small”.\nAlternatively, you can also click this badge to launch the Hub:\n\n\n\n\n\n\n\n\n\n\nNote: It takes a few minutes for the Hub to load. Please be patient!\n\nWhile the Hub loads, we’ll:\n\nDiscuss cloud environments\nSee how my Desktop is setup\nDiscuss python and conda environments\n\nThen, when the Hub is loaded, we’ll get oriented in the Hub."
  },
  {
    "objectID": "tutorials/00_Setup.html#discussion-cloud-environment",
    "href": "tutorials/00_Setup.html#discussion-cloud-environment",
    "title": "00. Setup for tutorials",
    "section": "Discussion: Cloud environment",
    "text": "Discussion: Cloud environment\nA brief overview. See NASA Openscapes Cloud Environment in the 2021-Cloud-Hackathon book for more detail.\n\nCloud infrastructure\n\nCloud: AWS us-west-2\n\nData: AWS S3 (cloud) and NASA DAAC data centers (on-prem).\nCloud compute environment: 2i2c Jupyterhub deployment\n\nIDE: JupyterLab"
  },
  {
    "objectID": "tutorials/00_Setup.html#discussion-my-desktop-setup",
    "href": "tutorials/00_Setup.html#discussion-my-desktop-setup",
    "title": "00. Setup for tutorials",
    "section": "Discussion: My desktop setup",
    "text": "Discussion: My desktop setup\nI’ll screenshare to show and/or talk through how I have oriented the following software we’re using:\n\nWorkshop Book (my teaching notes, your reference material)\nZoom Chat"
  },
  {
    "objectID": "tutorials/00_Setup.html#discussion-python-and-conda-environments",
    "href": "tutorials/00_Setup.html#discussion-python-and-conda-environments",
    "title": "00. Setup for tutorials",
    "section": "Discussion: Python and Conda environments",
    "text": "Discussion: Python and Conda environments\nWhy Python?\n\n\n\nPython Data Stack. Source: Jake VanderPlas, “The State of the Stack,” SciPy Keynote (SciPy 2015).\n\n\nDefault Python Environment:\nWe’ve set up the Python environment with conda.\n\n\n\n\n\n\nConda environment\n\n\n\n\n\nname: openscapes\nchannels:\n  - conda-forge\ndependencies:\n  - python=3.9\n  - pangeo-notebook\n  - awscli~=1.20\n  - boto3~=1.19\n  - gdal~=3.3\n  - rioxarray~=0.8\n  - xarray~=0.19\n  - h5netcdf~=0.11\n  - netcdf4~=1.5\n  - h5py~=2.10\n  - geoviews~=1.9\n  - matplotlib-base~=3.4\n  - hvplot~=0.7\n  - pyproj~=3.2\n  - bqplot~=0.12\n  - geopandas~=0.10\n  - zarr~=2.10\n  - cartopy~=0.20\n  - shapely==1.7.1\n  - pyresample~=1.22\n  - joblib~=1.1\n  - pystac-client~=0.3\n  - s3fs~=2021.7\n  - ipyleaflet~=0.14\n  - sidecar~=0.5\n  - jupyterlab-geojson~=3.1\n  - jupyterlab-git\n  - jupyter-resource-usage\n  - ipympl~=0.6\n  - conda-lock~=0.12\n  - pooch~=1.5\n  - pip\n  - pip:\n    - tqdm\n    - harmony-py\n    - earthdata\n    - zarr-eosdis-store\n\n\n\n\nBash terminal and installed software\nLibraries that are available from the terminal\n\ngdal 3.3 commands ( gdalinfo, gdaltransform…)\nhdf5 commands ( h5dump, h5ls..)\nnetcdf4 commands (ncdump, ncinfo …)\njq (parsing json files or streams from curl)\ncurl (fetch resources from the web)\nawscli (AWS API client, to interact with AWS cloud services)\nvim (editor)\ntree ( directory tree)\nmore …\n\n\n\nUpdating the environment\nScientific Python is a vast space and we only included libraries that are needed in our tutorials. Our default environment can be updated to include any Python library that’s available on pip or conda.\nThe project used to create our default environment is called corn (as it can include many Python kernels).\nIf we want to update a library or install a whole new environment we need to open an issue on this repository.\n\n\ncorn 🌽"
  },
  {
    "objectID": "tutorials/00_Setup.html#step-2.-jupyterhub-orientation",
    "href": "tutorials/00_Setup.html#step-2.-jupyterhub-orientation",
    "title": "00. Setup for tutorials",
    "section": "Step 2. JupyterHub orientation",
    "text": "Step 2. JupyterHub orientation\nNow that the Hub is loaded, let’s get oriented.\n\n\n\n\n\n\nFirst impressions\n\nLauncher & the big blue button\n“home directory”"
  },
  {
    "objectID": "tutorials/00_Setup.html#step-3.-navigate-to-the-workshop-folder",
    "href": "tutorials/00_Setup.html#step-3.-navigate-to-the-workshop-folder",
    "title": "00. Setup for tutorials",
    "section": "Step 3. Navigate to the Workshop folder",
    "text": "Step 3. Navigate to the Workshop folder\nThe workshop folder 2021-Cloud-Workshop-AGU is in the shared folder on JupyterHub."
  },
  {
    "objectID": "tutorials/00_Setup.html#jupyter-notebooks",
    "href": "tutorials/00_Setup.html#jupyter-notebooks",
    "title": "00. Setup for tutorials",
    "section": "Jupyter notebooks",
    "text": "Jupyter notebooks\nLet’s get oriented to Jupyter notebooks, which we’ll use in all the tutorials."
  },
  {
    "objectID": "tutorials/00_Setup.html#how-do-i-end-my-session",
    "href": "tutorials/00_Setup.html#how-do-i-end-my-session",
    "title": "00. Setup for tutorials",
    "section": "How do I end my session?",
    "text": "How do I end my session?\n(Also see How do I end my Openscapes session? Will I lose all of my work?)\nWhen you are finished working for the day it is important to explicitly log out of your Openscapes session. The reason for this is it will save us a bit of money! When you keep a session active it uses up AWS resources and keeps a series of virtual machines deployed.\nStopping the server happens automatically when you log out, so navigate to “File -> Log Out” and just click “Log Out”!\n!!! NOTE “logging out” - Logging out will NOT cause any of your work to be lost or deleted. It simply shuts down some resources. It would be equivalent to turning off your desktop computer at the end of the day."
  },
  {
    "objectID": "tutorials/02_NASA_Earthdata_Authentication.html#summary",
    "href": "tutorials/02_NASA_Earthdata_Authentication.html#summary",
    "title": "2021 Cloud Workshop at AGU",
    "section": "Summary",
    "text": "Summary\nThis notebook creates a hidden .netrc file (_netrc for Window OS) with Earthdata login credentials in your home directory. This file is needed to access NASA Earthdata assets from a scripting environment like Python.\n\nEarthdata Login\nAn Earthdata Login account is required to access data, as well as discover restricted data, from the NASA Earthdata system. Thus, to access NASA data, you need Earthdata Login. Please visit https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account. This account is free to create and only takes a moment to set up.\n\n\nAuthentication via netrc File\nYou will need a netrc file containing your NASA Earthdata Login credentials in order to execute the notebooks. A netrc file can be created manually within text editor and saved to your home directory. An example of the required content is below.\nmachine urs.earthdata.nasa.gov\nlogin <USERNAME>\npassword <PASSWORD>\n<USERNAME> and <PASSWORD> would be replaced by your actual Earthdata Login username and password respectively."
  },
  {
    "objectID": "tutorials/02_NASA_Earthdata_Authentication.html#import-required-packages",
    "href": "tutorials/02_NASA_Earthdata_Authentication.html#import-required-packages",
    "title": "2021 Cloud Workshop at AGU",
    "section": "Import Required Packages",
    "text": "Import Required Packages\n\nfrom netrc import netrc\nfrom subprocess import Popen\nfrom platform import system\nfrom getpass import getpass\nimport os\n\nThe code below will:\n\ncheck what operating system (OS) is being used to determine which netrc file to check for/create (.netrc or _netrc)\ncheck if you have an netrc file, and if so, varify if those credentials are for the Earthdata endpoint\ncreate a netrc file if a netrc file is not present.\n\n\nurs = 'urs.earthdata.nasa.gov'    # Earthdata URL endpoint for authentication\nprompts = ['Enter NASA Earthdata Login Username: ',\n           'Enter NASA Earthdata Login Password: ']\n\n# Determine the OS (Windows machines usually use an '_netrc' file)\nnetrc_name = \"_netrc\" if system()==\"Windows\" else \".netrc\"\n\n# Determine if netrc file exists, and if so, if it includes NASA Earthdata Login Credentials\ntry:\n    netrcDir = os.path.expanduser(f\"~/{netrc_name}\")\n    netrc(netrcDir).authenticators(urs)[0]\n\n# Below, create a netrc file and prompt user for NASA Earthdata Login Username and Password\nexcept FileNotFoundError:\n    homeDir = os.path.expanduser(\"~\")\n    Popen('touch {0}{2} | echo machine {1} >> {0}{2}'.format(homeDir + os.sep, urs, netrc_name), shell=True)\n    Popen('echo login {} >> {}{}'.format(getpass(prompt=prompts[0]), homeDir + os.sep, netrc_name), shell=True)\n    Popen('echo \\'password {} \\'>> {}{}'.format(getpass(prompt=prompts[1]), homeDir + os.sep, netrc_name), shell=True)\n    # Set restrictive permissions\n    Popen('chmod 0600 {0}{1}'.format(homeDir + os.sep, netrc_name), shell=True)\n\n    # Determine OS and edit netrc file if it exists but is not set up for NASA Earthdata Login\nexcept TypeError:\n    homeDir = os.path.expanduser(\"~\")\n    Popen('echo machine {1} >> {0}{2}'.format(homeDir + os.sep, urs, netrc_name), shell=True)\n    Popen('echo login {} >> {}{}'.format(getpass(prompt=prompts[0]), homeDir + os.sep, netrc_name), shell=True)\n    Popen('echo \\'password {} \\'>> {}{}'.format(getpass(prompt=prompts[1]), homeDir + os.sep, netrc_name), shell=True)\n\n\nSee if the file was created\nIf the file was created, we’ll see a .netrc file (_netrc for Window OS) in the list printed below. To view the contents from a Jupyter environment, click File on the top toolbar, select Open from Path…, type .netrc, and click Open. The .netrc file will open within the text editor.\n\n!!! Beware, your password will be visible if the .netrc file is opened in the text editor.\n\n\n!ls -al ~/"
  },
  {
    "objectID": "tutorials/04_On-Prem_Cloud.html#accessing-and-harmonizing-data-located-within-and-outside-of-the-nasa-earthdata-cloud",
    "href": "tutorials/04_On-Prem_Cloud.html#accessing-and-harmonizing-data-located-within-and-outside-of-the-nasa-earthdata-cloud",
    "title": "2021 Cloud Workshop at AGU",
    "section": "Accessing and harmonizing data located within and outside of the NASA Earthdata Cloud",
    "text": "Accessing and harmonizing data located within and outside of the NASA Earthdata Cloud"
  },
  {
    "objectID": "tutorials/04_On-Prem_Cloud.html#summary",
    "href": "tutorials/04_On-Prem_Cloud.html#summary",
    "title": "2021 Cloud Workshop at AGU",
    "section": "Summary",
    "text": "Summary\n\nThis tutorial will combine several workflow steps and components from the previous days, demonstrating the process of using the geolocation of data available outside of the Earthdata Cloud to then access coincident variables of cloud-accessible data. This may be a common use case as NASA Earthdata continues to migrate to the cloud, producing a “hybrid” data archive across Amazon Web Services (AWS) and original on-premise data storage systems. Additionally, you may also want to combine field measurements with remote sensing data available on the Earthdata Cloud.\nThis specific example explores the pairing of the ICESat-2 ATL07 Sea Ice Height data product, currently (as of November 2021) available publicly via direct download at the NSIDC DAAC, along with Sea Surface Temperature (SST) from the GHRSST MODIS L2 dataset (MODIS_A-JPL-L2P-v2019.0) available from PO.DAAC on the Earthdata Cloud.\nThe use case we’re looking at today centers over an area north of Greenland for a single day in June, where a melt pond was observed using the NASA OpenAltimetry application. Melt ponds are an important feature of Arctic sea ice dynamics, leading to an decrease in sea ice albedo and other changes in heat balance. Many NASA Earthdata datasets produce variables including sea ice albedo, sea surface temperature, air temperature, and sea ice height, which can be used to better understand these dynamics."
  },
  {
    "objectID": "tutorials/04_On-Prem_Cloud.html#requirements",
    "href": "tutorials/04_On-Prem_Cloud.html#requirements",
    "title": "2021 Cloud Workshop at AGU",
    "section": "Requirements",
    "text": "Requirements\n\n\nAWS instance running in us-west 2\nEarthdata Login\n.netrc file"
  },
  {
    "objectID": "tutorials/04_On-Prem_Cloud.html#learning-objectives",
    "href": "tutorials/04_On-Prem_Cloud.html#learning-objectives",
    "title": "2021 Cloud Workshop at AGU",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\n\nSearch for data programmatically using the Common Metadata Repository (CMR), determining granule (file) coverage across two datasets over an area of interest.\nDownload data from an on-premise storage system to our cloud environment.\nRead in 1-dimensional trajectory data (ICESat-2 ATL07) into xarray and perform attribute conversions.\nSelect and read in sea surface temperature (SST) data (MODIS_A-JPL-L2P-v2019.0) from the Earthdata Cloud into xarray.\nExtract, resample, and plot coincident SST data based on ICESat-2 geolocation."
  },
  {
    "objectID": "tutorials/04_On-Prem_Cloud.html#import-packages",
    "href": "tutorials/04_On-Prem_Cloud.html#import-packages",
    "title": "2021 Cloud Workshop at AGU",
    "section": "Import packages",
    "text": "Import packages\n\nimport os\nfrom pathlib import Path\nfrom pprint import pprint\n\n# Access via download\nimport requests\n\n# Access AWS S3\nimport s3fs\n\n# Read and work with datasets\nimport xarray as xr\nimport numpy as np\nimport h5py\n\n# For plotting\nimport matplotlib.pyplot as plt\nimport cartopy.crs as ccrs\nfrom shapely.geometry import box\n\n# For resampling\nimport pyresample"
  },
  {
    "objectID": "tutorials/04_On-Prem_Cloud.html#specify-data-time-range-and-area-of-interest",
    "href": "tutorials/04_On-Prem_Cloud.html#specify-data-time-range-and-area-of-interest",
    "title": "2021 Cloud Workshop at AGU",
    "section": "Specify data, time range, and area of interest",
    "text": "Specify data, time range, and area of interest\nWe are going to focus on getting data for an area north of Greenland for a single day in June.\nThese bounding_box and temporal variables will be used for data search, subset, and access below.\nThe same search and access steps for both datasets can be performed via Earthdata Search using the same spatial and temporal filtering options. See the Earthdata Search tutorial for more information on how to use Earthdata Search to discover and access data from the Earthdata Cloud.\n\n# Bounding Box spatial parameter in decimal degree 'W,S,E,N' format.\nbounding_box = '-62.8,81.7,-56.4,83'\n\n# Each date in yyyy-MM-ddTHH:mm:ssZ format; date range in start,end format\ntemporal = '2019-06-22T00:00:00Z,2019-06-22T23:59:59Z'\n\nSee the Data Discovery with CMR tutorial for more details on how to navigate the NASA Common Metadata Repository (CMR) Application Programming Interface, or API. For some background, the CMR catalogs all data for NASA’s Earth Observing System Data and Information System (EOSDIS). The CMR API allows applications (software, services, etc) to send information to each other. A helpful analogy is a waiter in a restaurant. The waiter takes your drink or food order that you select from the menu, often translated into short-hand, to the bar or kitchen, and then returns (hopefully) with what you ordered when it is ready.\nThe CMR API accepts search terms such as collection name, keywords, datetime range, and location, queries the CMR database and returns the results.\nFor this tutorial, we have already identified the unique identifier, or concept_id for each dataset:\n\nmodis_concept_id = 'C1940473819-POCLOUD'\nicesat2_concept_id = 'C2003771980-NSIDC_ECS'\n\nThis Earthdata Search Project also provides the same data access links that we will identify in the following steps for each dataset (note that you will need an Earthdata Login account to access this project)."
  },
  {
    "objectID": "tutorials/04_On-Prem_Cloud.html#search-and-download-icesat-2-atl07-files",
    "href": "tutorials/04_On-Prem_Cloud.html#search-and-download-icesat-2-atl07-files",
    "title": "2021 Cloud Workshop at AGU",
    "section": "Search and download ICESat-2 ATL07 files",
    "text": "Search and download ICESat-2 ATL07 files\nPerform a granule search over our time and area of interest. How many granules are returned?\n\ngranule_url = 'https://cmr.earthdata.nasa.gov/search/granules'\n\n\nresponse = requests.get(granule_url,\n                       params={\n                           'concept_id': icesat2_concept_id,\n                           'temporal': temporal,\n                           'bounding_box': bounding_box,\n                           'page_size': 200,\n                       },\n                       headers={\n                           'Accept': 'application/json'\n                       }\n                      )\nprint(response.headers['CMR-Hits'])\n\n2\n\n\nPrint the file names, size, and links:\n\ngranules = response.json()['feed']['entry']\nfor granule in granules:\n    print(f'{granule[\"producer_granule_id\"]} {granule[\"granule_size\"]} {granule[\"links\"][0][\"href\"]}')\n\nATL07-01_20190622055317_12980301_004_01.h5 237.0905504227 https://n5eil01u.ecs.nsidc.org/DP9/ATLAS/ATL07.004/2019.06.22/ATL07-01_20190622055317_12980301_004_01.h5\nATL07-01_20190622200154_13070301_004_01.h5 230.9151573181 https://n5eil01u.ecs.nsidc.org/DP9/ATLAS/ATL07.004/2019.06.22/ATL07-01_20190622200154_13070301_004_01.h5\n\n\n\nDownload ATL07 files\nAlthough several services are supported for ICESat-2 data, we are demonstrating direct access through the “on-prem” file system at NSIDC for simplicity.\nSome of these services include: - icepyx - From the icepyx documentation: “icepyx is both a software library and a community composed of ICESat-2 data users, developers, and the scientific community. We are working together to develop a shared library of resources - including existing resources, new code, tutorials, and use-cases/examples - that simplify the process of querying, obtaining, analyzing, and manipulating ICESat-2 datasets to enable scientific discovery.” - NSIDC DAAC Data Access and Service API - The API provided by the NSIDC DAAC allows you to access data programmatically using specific temporal and spatial filters. The same subsetting, reformatting, and reprojection services available on select data sets through NASA Earthdata Search can also be applied using this API. - IceFlow - The IceFlow python library simplifies accessing and combining data from several of NASA’s cryospheric altimetry missions, including ICESat/GLAS, Operation IceBridge, and ICESat-2. In particular, IceFlow harmonizes the various file formats and georeferencing parameters across several of the missions’ data sets, allowing you to analyze data across the multi-decadal time series.\nWe’ve found 2 granules. We’ll download the first one and write it to a file with the same name as the producer_granule_id.\nWe need the url for the granule as well. This is href links we printed out above.\n\nicesat_id = granules[0]['producer_granule_id']\nicesat_url = granules[0]['links'][0]['href']\n\nTo retrieve the granule data, we use the requests.get() method, which will utilize the .netrc file on the backend to authenticate the request against Earthdata Login.\n\nr = requests.get(icesat_url)\n\nThe response returned by requests has the same structure as all the other responses: a header and contents. The header information has information about the response, including the size of the data we downloaded in bytes.\n\nfor k, v in r.headers.items():\n    print(f'{k}: {v}')\n\nDate: Sun, 12 Dec 2021 01:52:31 GMT\nServer: Apache\nVary: User-Agent\nContent-Disposition: attachment\nContent-Length: 248607461\nKeep-Alive: timeout=15, max=100\nConnection: Keep-Alive\n\n\nThe contents needs to be saved to a file. To keep the directory clean, we will create a downloads directory to store the file. We can use a shell command to do this or use the makedirs method from the os package.\n\nos.makedirs(\"downloads\", exist_ok=True)\n\nYou should see a downloads directory in the file browser.\nTo write the data to a file, we use open to open a file. We need to specify that the file is open for writing by using the write-mode w. We also need to specify that we want to write bytes by setting the binary-mode b. This is important because the response contents are bytes. The default mode for open is text-mode. So make sure you use b.\nWe’ll use the with statement context-manager to open the file, write the contents of the response, and then close the file. Once the data in r.content is written sucessfully to the file, or if there is an error, the file is closed by the context-manager.\nWe also need to prepend the downloads path to the filename. We do this using Path from the pathlib package in the standard library.\n\noutfile = Path('downloads', icesat_id)\n\n\nif not outfile.exists():\n    with open(outfile, 'wb') as f:\n        f.write(r.content)\n\nATL07-01_20190622055317_12980301_004_01.h5 is an HDF5 file. xarray can open this but you need to tell it which group to read the data from. In this case we read the sea ice segment height data for ground-track 1 left-beam. You can explore the variable hierarchy in Earthdata Search, by selecting the Customize option under Download Data.\nThis code block performs the following operations: - Extracts the height_segment_height variable from the heights group, along with the dimension variables contained in the higher level sea_ice_segments group, - Convert attributes from bytestrings to strings, - Drops the HDF attribute DIMENSION_LIST, - Sets _FillValue to NaN\n\nvariable_names = [\n    '/gt1l/sea_ice_segments/latitude',\n    '/gt1l/sea_ice_segments/longitude',\n    '/gt1l/sea_ice_segments/delta_time',\n    '/gt1l/sea_ice_segments/heights/height_segment_height'\n    ]\nwith h5py.File(outfile, 'r') as h5:\n    data_vars = {}\n    for varname in variable_names:\n        var = h5[varname]\n        name = varname.split('/')[-1]\n        # Convert attributes\n        attrs = {}\n        for k, v in var.attrs.items():\n            if k != 'DIMENSION_LIST':\n                if isinstance(v, bytes):\n                    attrs[k] = v.decode('utf-8')\n                else:\n                    attrs[k] = v\n        data = var[:]\n        if '_FillValue' in attrs:\n            data = np.where(data < attrs['_FillValue'], data, np.nan)\n        data_vars[name] = (['segment'], data, attrs)\n    is2_ds = xr.Dataset(data_vars)\n    \nis2_ds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:                (segment: 235584)\nDimensions without coordinates: segment\nData variables:\n    latitude               (segment) float64 82.38 82.38 82.38 ... 72.61 72.61\n    longitude              (segment) float64 -55.11 -55.11 ... 145.1 145.1\n    delta_time             (segment) float64 4.642e+07 4.642e+07 ... 4.642e+07\n    height_segment_height  (segment) float32 nan nan nan ... -0.4335 -0.4463xarray.DatasetDimensions:segment: 235584Coordinates: (0)Data variables: (4)latitude(segment)float6482.38 82.38 82.38 ... 72.61 72.61contentType :referenceInformationcoordinates :delta_time longitudedescription :Latitude, WGS84, North=+, Lat of segment centerlong_name :Latitudesource :ATBD, section 4.4standard_name :latitudeunits :degrees_northvalid_max :90.0valid_min :-90.0array([82.38431982, 82.38431982, 82.38431982, ..., 72.60984638,\n       72.60977493, 72.60970985])longitude(segment)float64-55.11 -55.11 ... 145.1 145.1contentType :referenceInformationcoordinates :delta_time latitudedescription :Longitude, WGS84, East=+,Lon of segment centerlong_name :Longitudesource :ATBD, section 4.4standard_name :longitudeunits :degrees_eastvalid_max :180.0valid_min :-180.0array([-55.10896068, -55.10896068, -55.10896068, ..., 145.05396164,\n       145.05392851, 145.05389832])delta_time(segment)float644.642e+07 4.642e+07 ... 4.642e+07CLASS :DIMENSION_SCALENAME :gt1l/sea_ice_segments/delta_timeREFERENCE_LIST :[(<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)]contentType :physicalMeasurementcoordinates :latitude longitudedescription :Number of GPS seconds since the ATLAS SDP epoch. The ATLAS Standard Data Products (SDP) epoch offset is defined within /ancillary_data/atlas_sdp_gps_epoch as the number of GPS seconds between the GPS epoch (1980-01-06T00:00:00.000000Z UTC) and the ATLAS SDP epoch. By adding the offset contained within atlas_sdp_gps_epoch to delta time parameters, the time in gps_seconds relative to the GPS epoch can be computed.long_name :Elapsed GPS secondssource :telemetrystandard_name :timeunits :seconds since 2018-01-01array([46419293.64266939, 46419293.64266939, 46419293.64266939, ...,\n       46419681.87646231, 46419681.87759533, 46419681.87862704])height_segment_height(segment)float32nan nan nan ... -0.4335 -0.4463_FillValue :3.4028235e+38contentType :referenceInformationcoordinates :../delta_time ../latitude ../longitudedescription :Mean height from along-track segment fit detremined by the sea ice algorithm. The sea ice height is relative to the tide-free MSS.long_name :height of segment surfacesource :ATBD, section 4.2.2.4units :metersarray([        nan,         nan,         nan, ..., -0.46550068,\n       -0.43347716, -0.4462675 ], dtype=float32)Attributes: (0)\n\n\n\nis2_ds.height_segment_height.plot() ;"
  },
  {
    "objectID": "tutorials/04_On-Prem_Cloud.html#determine-the-ghrsst-modis-l2-granules-returned-from-our-time-and-area-of-interest",
    "href": "tutorials/04_On-Prem_Cloud.html#determine-the-ghrsst-modis-l2-granules-returned-from-our-time-and-area-of-interest",
    "title": "2021 Cloud Workshop at AGU",
    "section": "Determine the GHRSST MODIS L2 granules returned from our time and area of interest",
    "text": "Determine the GHRSST MODIS L2 granules returned from our time and area of interest\n\nresponse = requests.get(granule_url, \n                        params={\n                            'concept_id': modis_concept_id,\n                            'temporal': temporal,\n                            'bounding_box': bounding_box,\n                            'page_size': 200,\n                            },\n                        headers={\n                            'Accept': 'application/json'\n                            }\n                       )\nprint(response.headers['CMR-Hits'])\n\n14\n\n\n\ngranules = response.json()['feed']['entry']\nfor granule in granules:\n    print(f'{granule[\"title\"]} {granule[\"granule_size\"]} {granule[\"links\"][0][\"href\"]}')\n\n20190622000501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 20.71552562713623 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622000501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622014501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 9.34600830078125E-5 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622014501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622032501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 21.307741165161133 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622032501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622050001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 9.34600830078125E-5 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622050001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622050501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 20.065649032592773 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622050501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622064001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 9.34600830078125E-5 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622064001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622064501-JPL-L2P_GHRSST-SSTskin-MODIS_A-N-v02.0-fv01.0 18.602201461791992 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622064501-JPL-L2P_GHRSST-SSTskin-MODIS_A-N-v02.0-fv01.0.nc\n20190622064501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 18.665077209472656 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622064501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622082001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 19.782299995422363 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622082001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622100001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 19.13440227508545 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622100001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622113501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 20.3239164352417 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622113501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622114001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 9.34600830078125E-5 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622114001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622163001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 19.257243156433105 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622163001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622181001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 19.93498420715332 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622181001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc"
  },
  {
    "objectID": "tutorials/04_On-Prem_Cloud.html#load-data-into-xarray-via-s3-direct-access",
    "href": "tutorials/04_On-Prem_Cloud.html#load-data-into-xarray-via-s3-direct-access",
    "title": "2021 Cloud Workshop at AGU",
    "section": "Load data into xarray via S3 direct access",
    "text": "Load data into xarray via S3 direct access\nOur CMR granule search returned 14 files for our time and area of interest. However, not all granules will be suitable for analysis.\nI’ve identified the image with granule id G1956158784-POCLOUD as a good candidate, this is the 9th granule. In this image, our area of interest is close to nadir. This means that the instantaneous field of view over the area of interest cover a smaller area than at the edge of the image.\nWe are looking for the link for direct download access via s3. This is a url but with a prefix s3://. This happens to be the first href link in the metadata.\nFor a single granule we can cut and paste the s3 link. If we have several granules, the s3 links can be extracted with some simple code.\n\ngranule = granules[9]\n\nfor link in granule['links']:\n    if link['href'].startswith('s3://'):\n        s3_link = link['href']\n        \ns3_link\n\n's3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622100001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc'\n\n\n\nGet S3 credentials\nAs with the previous S3 download tutorials we need credentials to access data from s3: access keys and tokens.\n\ns3_credentials = requests.get('https://archive.podaac.earthdata.nasa.gov/s3credentials').json()\n\nEssentially, what we are doing in this step is to “mount” the s3 bucket as a file system. This allows us to treat the S3 bucket in a similar way to a local file system.\n\ns3_fs = s3fs.S3FileSystem(\n    key=s3_credentials[\"accessKeyId\"],\n    secret=s3_credentials[\"secretAccessKey\"],\n    token=s3_credentials[\"sessionToken\"],\n)\n\n\n\nOpen a s3 file\nNow we have the S3FileSystem set up, we can access the granule. xarray cannot open a S3File directly, so we use the open method for the S3FileSystem to open the granule using the endpoint url we extracted from the metadata. We also have to set the mode='rb'. This opens the granule in read-only mode and in byte-mode. Byte-mode is important. By default, open opens a file as text - in this case it would just be a string of characters - and xarray doesn’t know what to do with that.\nWe then pass the S3File object f to xarray.open_dataset. For this dataset, we also have to set decode_cf=False. This switch tells xarray not to use information contained in variable attributes to generate human readable coordinate variables. Normally, this should work for netcdf files but for this particular cloud-hosted dataset, variable attribute data is not in the form expected by xarray. We’ll fix this.\n\nf = s3_fs.open(s3_link, mode='rb')\nmodis_ds = xr.open_dataset(f, decode_cf=False)\n\nIf you click on the Show/Hide Attributes icon (the first document-like icon to the right of coordinate variable metadata) you can see that attributes are one-element arrays containing bytestrings.\n\nmodis_ds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:                  (nj: 2030, ni: 1354, time: 1)\nCoordinates:\n  * time                     (time) int32 1214042401\nDimensions without coordinates: nj, ni\nData variables:\n    lat                      (nj, ni) float32 ...\n    lon                      (nj, ni) float32 ...\n    sea_surface_temperature  (time, nj, ni) int16 ...\n    sst_dtime                (time, nj, ni) int16 ...\n    quality_level            (time, nj, ni) int8 ...\n    sses_bias                (time, nj, ni) int8 ...\n    sses_standard_deviation  (time, nj, ni) int8 ...\n    l2p_flags                (time, nj, ni) int16 ...\n    chlorophyll_a            (time, nj, ni) float32 ...\n    K_490                    (time, nj, ni) int16 ...\n    wind_speed               (time, nj, ni) int8 ...\n    dt_analysis              (time, nj, ni) int8 ...\nAttributes: (12/49)\n    Conventions:                [b'CF-1.7, ACDD-1.3']\n    title:                      [b'MODIS Aqua L2P SST']\n    summary:                    [b'Sea surface temperature retrievals produce...\n    references:                 [b'GHRSST Data Processing Specification v2r5']\n    institution:                [b'NASA/JPL/OBPG/RSMAS']\n    history:                    [b'MODIS L2P created at JPL PO.DAAC']\n    ...                         ...\n    publisher_email:            [b'ghrsst-po@nceo.ac.uk']\n    processing_level:           [b'L2P']\n    cdm_data_type:              [b'swath']\n    startDirection:             [b'Ascending']\n    endDirection:               [b'Descending']\n    day_night_flag:             [b'Day']xarray.DatasetDimensions:nj: 2030ni: 1354time: 1Coordinates: (1)time(time)int321214042401long_name :[b'reference time of sst file']standard_name :[b'time']units :[b'seconds since 1981-01-01 00:00:00']comment :[b'time of first sensor observation']coverage_content_type :[b'coordinate']array([1214042401], dtype=int32)Data variables: (12)lat(nj, ni)float32...long_name :[b'latitude']standard_name :[b'latitude']units :[b'degrees_north']_FillValue :[-999.]valid_min :[-90.]valid_max :[90.]comment :[b'geographical coordinates, WGS84 projection']coverage_content_type :[b'coordinate'][2748620 values with dtype=float32]lon(nj, ni)float32...long_name :[b'longitude']standard_name :[b'longitude']units :[b'degrees_east']_FillValue :[-999.]valid_min :[-180.]valid_max :[180.]comment :[b'geographical coordinates, WGS84 projection']coverage_content_type :[b'coordinate'][2748620 values with dtype=float32]sea_surface_temperature(time, nj, ni)int16...long_name :[b'sea surface temperature']standard_name :[b'sea_surface_skin_temperature']units :[b'kelvin']_FillValue :[-32767]valid_min :[-1000]valid_max :[10000]comment :[b'sea surface temperature from thermal IR (11 um) channels']scale_factor :[0.005]add_offset :[273.15]source :[b'NASA and University of Miami']coordinates :[b'lon lat']coverage_content_type :[b'physicalMeasurement'][2748620 values with dtype=int16]sst_dtime(time, nj, ni)int16...long_name :[b'time difference from reference time']units :[b'seconds']_FillValue :[-32768]valid_min :[-32767]valid_max :[32767]comment :[b'time plus sst_dtime gives seconds after 00:00:00 UTC January 1, 1981']coordinates :[b'lon lat']coverage_content_type :[b'referenceInformation'][2748620 values with dtype=int16]quality_level(time, nj, ni)int8...long_name :[b'quality level of SST pixel']_FillValue :[-128]valid_min :[0]valid_max :[5]comment :[b'thermal IR SST proximity confidence value; signed byte array: WARNING Some applications are unable to properly handle signed byte values. If values are encountered > 127, please subtract 256 from this reported value']coordinates :[b'lon lat']flag_values :[0 1 2 3 4 5]flag_meanings :[b'no_data bad_data worst_quality low_quality acceptable_quality best_quality']coverage_content_type :[b'qualityInformation'][2748620 values with dtype=int8]sses_bias(time, nj, ni)int8...long_name :[b'SSES bias error based on proximity confidence flags']units :[b'kelvin']_FillValue :[-128]valid_min :[-127]valid_max :[127]comment :[b'thermal IR SST bias error; signed byte array: WARNING Some applications are unable to properly handle signed byte values. If values are encountered > 127, please subtract 256 from this reported value']scale_factor :[0.15748031]add_offset :[0.]coordinates :[b'lon lat']coverage_content_type :[b'auxiliaryInformation'][2748620 values with dtype=int8]sses_standard_deviation(time, nj, ni)int8...long_name :[b'SSES standard deviation error based on proximity confidence flags']units :[b'kelvin']_FillValue :[-128]valid_min :[-127]valid_max :[127]comment :[b'thermal IR SST standard deviation error; signed byte array: WARNING Some applications are unable to properly handle signed byte values. If values are encountered > 127, please subtract 256 from this reported value']scale_factor :[0.07874016]add_offset :[10.]coordinates :[b'lon lat']coverage_content_type :[b'auxiliaryInformation'][2748620 values with dtype=int8]l2p_flags(time, nj, ni)int16...long_name :[b'L2P flags']valid_min :[0]valid_max :[16]comment :[b'These flags can be used to further filter data variables']coordinates :[b'lon lat']flag_meanings :[b'microwave land ice lake river']flag_masks :[ 1  2  4  8 16]coverage_content_type :[b'qualityInformation'][2748620 values with dtype=int16]chlorophyll_a(time, nj, ni)float32...long_name :[b'Chlorophyll Concentration, OC3 Algorithm']units :[b'mg m^-3']_FillValue :[-32767.]valid_min :[0.001]valid_max :[100.]comment :[b'non L2P core field']coordinates :[b'lon lat']coverage_content_type :[b'auxiliaryInformation'][2748620 values with dtype=float32]K_490(time, nj, ni)int16...long_name :[b'Diffuse attenuation coefficient at 490 nm (OBPG)']units :[b'm^-1']_FillValue :[-32767]valid_min :[50]valid_max :[30000]comment :[b'non L2P core field']scale_factor :[0.0002]add_offset :[0.]coordinates :[b'lon lat']coverage_content_type :[b'auxiliaryInformation'][2748620 values with dtype=int16]wind_speed(time, nj, ni)int8...long_name :[b'10m wind speed']standard_name :[b'wind_speed']units :[b'm s-1']_FillValue :[-128]valid_min :[-127]valid_max :[127]comment :[b'Wind at 10 meters above the sea surface']scale_factor :[0.2]add_offset :[25.]source :[b'TBD.  Placeholder.  Currently empty']coordinates :[b'lon lat']grid_mapping :[b'TBD']time_offset :[2.]height :[b'10 m']coverage_content_type :[b'auxiliaryInformation'][2748620 values with dtype=int8]dt_analysis(time, nj, ni)int8...long_name :[b'deviation from SST reference climatology']units :[b'kelvin']_FillValue :[-128]valid_min :[-127]valid_max :[127]comment :[b'TBD']scale_factor :[0.1]add_offset :[0.]source :[b'TBD. Placeholder.  Currently empty']coordinates :[b'lon lat']coverage_content_type :[b'auxiliaryInformation'][2748620 values with dtype=int8]Attributes: (49)Conventions :[b'CF-1.7, ACDD-1.3']title :[b'MODIS Aqua L2P SST']summary :[b'Sea surface temperature retrievals produced at the NASA OBPG for the MODIS Aqua sensor.  These have been reformatted to GHRSST GDS specifications by the JPL PO.DAAC']references :[b'GHRSST Data Processing Specification v2r5']institution :[b'NASA/JPL/OBPG/RSMAS']history :[b'MODIS L2P created at JPL PO.DAAC']comment :[b'L2P Core without DT analysis or other ancillary fields; Day, Start Node:Ascending, End Node:Descending; WARNING Some applications are unable to properly handle signed byte values. If values are encountered > 127, please subtract 256 from this reported value; Refined']license :[b'GHRSST and PO.DAAC protocol allow data use as free and open.']id :[b'MODIS_A-JPL-L2P-v2019.0']naming_authority :[b'org.ghrsst']product_version :[b'2019.0']uuid :[b'f6e1f61d-c4a4-4c17-8354-0c15e12d688b']gds_version_id :[b'2.0']netcdf_version_id :[b'4.1']date_created :[b'20200221T085224Z']file_quality_level :[3]spatial_resolution :[b'1km']start_time :[b'20190622T100001Z']time_coverage_start :[b'20190622T100001Z']stop_time :[b'20190622T100459Z']time_coverage_end :[b'20190622T100459Z']northernmost_latitude :[89.9862]southernmost_latitude :[66.2723]easternmost_longitude :[-45.9467]westernmost_longitude :[152.489]source :[b'MODIS sea surface temperature observations for the OBPG']platform :[b'Aqua']sensor :[b'MODIS']metadata_link :[b'http://podaac.jpl.nasa.gov/ws/metadata/dataset/?format=iso&shortName=MODIS_A-JPL-L2P-v2019.0']keywords :[b'Oceans > Ocean Temperature > Sea Surface Temperature']keywords_vocabulary :[b'NASA Global Change Master Directory (GCMD) Science Keywords']standard_name_vocabulary :[b'NetCDF Climate and Forecast (CF) Metadata Convention']geospatial_lat_units :[b'degrees_north']geospatial_lat_resolution :[0.01]geospatial_lon_units :[b'degrees_east']geospatial_lon_resolution :[0.01]acknowledgment :[b'The MODIS L2P sea surface temperature data are sponsored by NASA']creator_name :[b'Ed Armstrong, JPL PO.DAAC']creator_email :[b'edward.m.armstrong@jpl.nasa.gov']creator_url :[b'http://podaac.jpl.nasa.gov']project :[b'Group for High Resolution Sea Surface Temperature']publisher_name :[b'The GHRSST Project Office']publisher_url :[b'http://www.ghrsst.org']publisher_email :[b'ghrsst-po@nceo.ac.uk']processing_level :[b'L2P']cdm_data_type :[b'swath']startDirection :[b'Ascending']endDirection :[b'Descending']day_night_flag :[b'Day']\n\n\nTo fix this, we need to extract array elements as scalars, and convert those scalars from bytestrings to strings. We use the decode method to do this. The bytestrings are encoded as utf-8, which is a unicode character format. This is the default encoding for decode but we’ve included it as an argument to be explicit.\nNot all attributes are bytestrings. Some are floats. Take a look at _FillValue, and valid_min and valid_max. To avoid an error, we use the isinstance function to check if the value of an attributes is type bytes - a bytestring. If it is, then we decode it. If not, we just extract the scalar and do nothing else.\nWe also fix the global attributes.\n\ndef fix_attributes(da):\n    '''Decodes bytestring attributes to strings'''\n    for attr, value in da.attrs.items():\n        if isinstance(value[0], bytes):\n            da.attrs[attr] = value[0].decode('utf-8')\n        else:\n            da.attrs[attr] = value[0]\n    return\n\n# Fix variable attributes\nfor var in modis_ds.variables:\n    da = modis_ds[var]\n    fix_attributes(da)\n            \n# Fix global attributes\nfix_attributes(modis_ds)\n\nWith this done, we can use the xarray function decode_cf to convert the attributes.\n\nmodis_ds = xr.decode_cf(modis_ds)\n\n\nmodis_ds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:                  (nj: 2030, ni: 1354, time: 1)\nCoordinates:\n    lat                      (nj, ni) float32 ...\n    lon                      (nj, ni) float32 ...\n  * time                     (time) datetime64[ns] 2019-06-22T10:00:01\nDimensions without coordinates: nj, ni\nData variables:\n    sea_surface_temperature  (time, nj, ni) float32 ...\n    sst_dtime                (time, nj, ni) timedelta64[ns] ...\n    quality_level            (time, nj, ni) float32 ...\n    sses_bias                (time, nj, ni) float32 ...\n    sses_standard_deviation  (time, nj, ni) float32 ...\n    l2p_flags                (time, nj, ni) int16 ...\n    chlorophyll_a            (time, nj, ni) float32 ...\n    K_490                    (time, nj, ni) float32 ...\n    wind_speed               (time, nj, ni) float32 ...\n    dt_analysis              (time, nj, ni) float32 ...\nAttributes: (12/49)\n    Conventions:                CF-1.7, ACDD-1.3\n    title:                      MODIS Aqua L2P SST\n    summary:                    Sea surface temperature retrievals produced a...\n    references:                 GHRSST Data Processing Specification v2r5\n    institution:                NASA/JPL/OBPG/RSMAS\n    history:                    MODIS L2P created at JPL PO.DAAC\n    ...                         ...\n    publisher_email:            ghrsst-po@nceo.ac.uk\n    processing_level:           L2P\n    cdm_data_type:              swath\n    startDirection:             Ascending\n    endDirection:               Descending\n    day_night_flag:             Dayxarray.DatasetDimensions:nj: 2030ni: 1354time: 1Coordinates: (3)lat(nj, ni)float32...long_name :latitudestandard_name :latitudeunits :degrees_northvalid_min :-90.0valid_max :90.0comment :geographical coordinates, WGS84 projectioncoverage_content_type :coordinate[2748620 values with dtype=float32]lon(nj, ni)float32...long_name :longitudestandard_name :longitudeunits :degrees_eastvalid_min :-180.0valid_max :180.0comment :geographical coordinates, WGS84 projectioncoverage_content_type :coordinate[2748620 values with dtype=float32]time(time)datetime64[ns]2019-06-22T10:00:01long_name :reference time of sst filestandard_name :timecomment :time of first sensor observationcoverage_content_type :coordinatearray(['2019-06-22T10:00:01.000000000'], dtype='datetime64[ns]')Data variables: (10)sea_surface_temperature(time, nj, ni)float32...long_name :sea surface temperaturestandard_name :sea_surface_skin_temperatureunits :kelvinvalid_min :-1000valid_max :10000comment :sea surface temperature from thermal IR (11 um) channelssource :NASA and University of Miamicoverage_content_type :physicalMeasurement[2748620 values with dtype=float32]sst_dtime(time, nj, ni)timedelta64[ns]...long_name :time difference from reference timevalid_min :-32767valid_max :32767comment :time plus sst_dtime gives seconds after 00:00:00 UTC January 1, 1981coverage_content_type :referenceInformation[2748620 values with dtype=timedelta64[ns]]quality_level(time, nj, ni)float32...long_name :quality level of SST pixelvalid_min :0valid_max :5comment :thermal IR SST proximity confidence value; signed byte array: WARNING Some applications are unable to properly handle signed byte values. If values are encountered > 127, please subtract 256 from this reported valueflag_values :0flag_meanings :no_data bad_data worst_quality low_quality acceptable_quality best_qualitycoverage_content_type :qualityInformation[2748620 values with dtype=float32]sses_bias(time, nj, ni)float32...long_name :SSES bias error based on proximity confidence flagsunits :kelvinvalid_min :-127valid_max :127comment :thermal IR SST bias error; signed byte array: WARNING Some applications are unable to properly handle signed byte values. If values are encountered > 127, please subtract 256 from this reported valuecoverage_content_type :auxiliaryInformation[2748620 values with dtype=float32]sses_standard_deviation(time, nj, ni)float32...long_name :SSES standard deviation error based on proximity confidence flagsunits :kelvinvalid_min :-127valid_max :127comment :thermal IR SST standard deviation error; signed byte array: WARNING Some applications are unable to properly handle signed byte values. If values are encountered > 127, please subtract 256 from this reported valuecoverage_content_type :auxiliaryInformation[2748620 values with dtype=float32]l2p_flags(time, nj, ni)int16...long_name :L2P flagsvalid_min :0valid_max :16comment :These flags can be used to further filter data variablesflag_meanings :microwave land ice lake riverflag_masks :1coverage_content_type :qualityInformation[2748620 values with dtype=int16]chlorophyll_a(time, nj, ni)float32...long_name :Chlorophyll Concentration, OC3 Algorithmunits :mg m^-3valid_min :0.001valid_max :100.0comment :non L2P core fieldcoverage_content_type :auxiliaryInformation[2748620 values with dtype=float32]K_490(time, nj, ni)float32...long_name :Diffuse attenuation coefficient at 490 nm (OBPG)units :m^-1valid_min :50valid_max :30000comment :non L2P core fieldcoverage_content_type :auxiliaryInformation[2748620 values with dtype=float32]wind_speed(time, nj, ni)float32...long_name :10m wind speedstandard_name :wind_speedunits :m s-1valid_min :-127valid_max :127comment :Wind at 10 meters above the sea surfacesource :TBD.  Placeholder.  Currently emptygrid_mapping :TBDtime_offset :2.0height :10 mcoverage_content_type :auxiliaryInformation[2748620 values with dtype=float32]dt_analysis(time, nj, ni)float32...long_name :deviation from SST reference climatologyunits :kelvinvalid_min :-127valid_max :127comment :TBDsource :TBD. Placeholder.  Currently emptycoverage_content_type :auxiliaryInformation[2748620 values with dtype=float32]Attributes: (49)Conventions :CF-1.7, ACDD-1.3title :MODIS Aqua L2P SSTsummary :Sea surface temperature retrievals produced at the NASA OBPG for the MODIS Aqua sensor.  These have been reformatted to GHRSST GDS specifications by the JPL PO.DAACreferences :GHRSST Data Processing Specification v2r5institution :NASA/JPL/OBPG/RSMAShistory :MODIS L2P created at JPL PO.DAACcomment :L2P Core without DT analysis or other ancillary fields; Day, Start Node:Ascending, End Node:Descending; WARNING Some applications are unable to properly handle signed byte values. If values are encountered > 127, please subtract 256 from this reported value; Refinedlicense :GHRSST and PO.DAAC protocol allow data use as free and open.id :MODIS_A-JPL-L2P-v2019.0naming_authority :org.ghrsstproduct_version :2019.0uuid :f6e1f61d-c4a4-4c17-8354-0c15e12d688bgds_version_id :2.0netcdf_version_id :4.1date_created :20200221T085224Zfile_quality_level :3spatial_resolution :1kmstart_time :20190622T100001Ztime_coverage_start :20190622T100001Zstop_time :20190622T100459Ztime_coverage_end :20190622T100459Znorthernmost_latitude :89.9862southernmost_latitude :66.2723easternmost_longitude :-45.9467westernmost_longitude :152.489source :MODIS sea surface temperature observations for the OBPGplatform :Aquasensor :MODISmetadata_link :http://podaac.jpl.nasa.gov/ws/metadata/dataset/?format=iso&shortName=MODIS_A-JPL-L2P-v2019.0keywords :Oceans > Ocean Temperature > Sea Surface Temperaturekeywords_vocabulary :NASA Global Change Master Directory (GCMD) Science Keywordsstandard_name_vocabulary :NetCDF Climate and Forecast (CF) Metadata Conventiongeospatial_lat_units :degrees_northgeospatial_lat_resolution :0.01geospatial_lon_units :degrees_eastgeospatial_lon_resolution :0.01acknowledgment :The MODIS L2P sea surface temperature data are sponsored by NASAcreator_name :Ed Armstrong, JPL PO.DAACcreator_email :edward.m.armstrong@jpl.nasa.govcreator_url :http://podaac.jpl.nasa.govproject :Group for High Resolution Sea Surface Temperaturepublisher_name :The GHRSST Project Officepublisher_url :http://www.ghrsst.orgpublisher_email :ghrsst-po@nceo.ac.ukprocessing_level :L2Pcdm_data_type :swathstartDirection :AscendingendDirection :Descendingday_night_flag :Day\n\n\nLet’s make a quick plot to take a look at the sea_surface_temperature variable.\n\nmodis_ds.sea_surface_temperature.plot() ;\n\n\n\n\n\n\nPlot MODIS and ICESat-2 data on a map\n\nmap_proj = ccrs.NorthPolarStereo()\n\nfig = plt.figure(figsize=(10,5))\nax = fig.add_subplot(projection=map_proj)\nax.coastlines()\n\n# Plot MODIS sst, save object as sst_img, so we can add colorbar\nsst_img = ax.pcolormesh(modis_ds.lon, modis_ds.lat, modis_ds.sea_surface_temperature[0,:,:], \n                        vmin=240, vmax=270,  # Set max and min values for plotting\n                        cmap='viridis', shading='auto',   # shading='auto' to avoid warning\n                        transform=ccrs.PlateCarree())  # coords are lat,lon but map if NPS \n\n# Plot IS2 surface height \nis2_img = ax.scatter(is2_ds.longitude, is2_ds.latitude,\n                     c=is2_ds.height_segment_height, \n                     vmax=1.5,  # Set max height to plot\n                     cmap='Reds', alpha=0.6, s=2,\n                     transform=ccrs.PlateCarree())\n\n# Add colorbars\nfig.colorbar(sst_img, label='MODIS SST (K)')\nfig.colorbar(is2_img, label='ATL07 Height (m)')\n\n\n<matplotlib.colorbar.Colorbar at 0x7fb3944adb50>\n\n\n\n\n\n\n\nExtract SST coincident with ICESat-2 track\nThe MODIS SST is swath data, not a regularly-spaced grid of sea surface temperatures. ICESat-2 sea surface heights are irregularly spaced segments along one ground-track traced by the ATLAS instrument on-board ICESat-2. Fortunately, pyresample allows us to resample swath data.\npyresample has many resampling methods. We’re going to use the nearest neighbour resampling method, which is implemented using a k-dimensional tree algorithm or K-d tree. K-d trees are data structures that improve search efficiency for large data sets.\nThe first step is to define the geometry of the ICESat-2 and MODIS data. To do this we use the latitudes and longitudes of the datasets.\n\nis2_geometry = pyresample.SwathDefinition(lons=is2_ds.longitude,\n                                          lats=is2_ds.latitude)\n\n\nmodis_geometry = pyresample.SwathDefinition(lons=modis_ds.lon, lats=modis_ds.lat)\n\nWe then implement the resampling method, passing the two geometries we have defined, the data array we want to resample - in this case sea surface temperature, and a search radius. The resampling method expects a numpy.Array rather than an xarray.DataArray, so we use values to get the data as a numpy.Array.\nWe set the search radius to 1000 m. The MODIS data is nominally 1km spacing.\n\nsearch_radius=1000.\nfill_value = np.nan\nis2_sst = pyresample.kd_tree.resample_nearest(\n    modis_geometry,\n    modis_ds.sea_surface_temperature.values,\n    is2_geometry,\n    search_radius,\n    fill_value=fill_value\n)\n\n\nis2_sst\n\narray([263.375, 263.375, 263.375, ...,     nan,     nan,     nan],\n      dtype=float32)\n\n\n\nis2_ds['sea_surface_temperature'] = xr.DataArray(is2_sst, dims='segment')\nis2_ds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:                  (segment: 235584)\nDimensions without coordinates: segment\nData variables:\n    latitude                 (segment) float64 82.38 82.38 82.38 ... 72.61 72.61\n    longitude                (segment) float64 -55.11 -55.11 ... 145.1 145.1\n    delta_time               (segment) float64 4.642e+07 4.642e+07 ... 4.642e+07\n    height_segment_height    (segment) float32 nan nan nan ... -0.4335 -0.4463\n    sea_surface_temperature  (segment) float32 263.4 263.4 263.4 ... nan nan nanxarray.DatasetDimensions:segment: 235584Coordinates: (0)Data variables: (5)latitude(segment)float6482.38 82.38 82.38 ... 72.61 72.61contentType :referenceInformationcoordinates :delta_time longitudedescription :Latitude, WGS84, North=+, Lat of segment centerlong_name :Latitudesource :ATBD, section 4.4standard_name :latitudeunits :degrees_northvalid_max :90.0valid_min :-90.0array([82.38431982, 82.38431982, 82.38431982, ..., 72.60984638,\n       72.60977493, 72.60970985])longitude(segment)float64-55.11 -55.11 ... 145.1 145.1contentType :referenceInformationcoordinates :delta_time latitudedescription :Longitude, WGS84, East=+,Lon of segment centerlong_name :Longitudesource :ATBD, section 4.4standard_name :longitudeunits :degrees_eastvalid_max :180.0valid_min :-180.0array([-55.10896068, -55.10896068, -55.10896068, ..., 145.05396164,\n       145.05392851, 145.05389832])delta_time(segment)float644.642e+07 4.642e+07 ... 4.642e+07CLASS :DIMENSION_SCALENAME :gt1l/sea_ice_segments/delta_timeREFERENCE_LIST :[(<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)]contentType :physicalMeasurementcoordinates :latitude longitudedescription :Number of GPS seconds since the ATLAS SDP epoch. The ATLAS Standard Data Products (SDP) epoch offset is defined within /ancillary_data/atlas_sdp_gps_epoch as the number of GPS seconds between the GPS epoch (1980-01-06T00:00:00.000000Z UTC) and the ATLAS SDP epoch. By adding the offset contained within atlas_sdp_gps_epoch to delta time parameters, the time in gps_seconds relative to the GPS epoch can be computed.long_name :Elapsed GPS secondssource :telemetrystandard_name :timeunits :seconds since 2018-01-01array([46419293.64266939, 46419293.64266939, 46419293.64266939, ...,\n       46419681.87646231, 46419681.87759533, 46419681.87862704])height_segment_height(segment)float32nan nan nan ... -0.4335 -0.4463_FillValue :3.4028235e+38contentType :referenceInformationcoordinates :../delta_time ../latitude ../longitudedescription :Mean height from along-track segment fit detremined by the sea ice algorithm. The sea ice height is relative to the tide-free MSS.long_name :height of segment surfacesource :ATBD, section 4.2.2.4units :metersarray([        nan,         nan,         nan, ..., -0.46550068,\n       -0.43347716, -0.4462675 ], dtype=float32)sea_surface_temperature(segment)float32263.4 263.4 263.4 ... nan nan nanarray([263.375, 263.375, 263.375, ...,     nan,     nan,     nan],\n      dtype=float32)Attributes: (0)\n\n\n\n\nPlot SST and Height along track\nThis is a quick plot of the extracted data. We’re using matplotlib so we can use latitude as the x-value:\n\nis2_ds = is2_ds.set_coords(['latitude'])\n\nfig, ax1 = plt.subplots(figsize=(15, 7))\nax1.set_xlim(82.,88.)\nax1.plot(is2_ds.latitude, is2_ds.sea_surface_temperature, \n         color='orange', label='SST', zorder=3)\nax1.set_ylabel('SST (K)')\n\nax2 = ax1.twinx()\nax2.plot(is2_ds.latitude, is2_ds.height_segment_height, label='Height')\nax2.set_ylabel('Height (m)')\n\nfig.legend()\n\n<matplotlib.legend.Legend at 0x7fb39fcd8040>"
  },
  {
    "objectID": "tutorials/Direct_Access_netCDF_simple.html",
    "href": "tutorials/Direct_Access_netCDF_simple.html",
    "title": "2021 Cloud Workshop at AGU",
    "section": "",
    "text": "Direct Access - ECCO netCDF example\n\nGetting Started\nIn this notebook, we will access monthly sea surface height from ECCO V4r4 (10.5067/ECG5D-SSH44). The data are provided as a time series of monthly netCDFs on a 0.5-degree latitude/longitude grid.\nWe will access the data from inside the AWS cloud (us-west-2 region, specifically) and load a time series made of multiple netCDF datasets into a single xarray dataset. This approach leverages S3 native protocols for efficient access to the data.\n\n\nRequirements\n\nAWS\nThis notebook should be running in an EC2 instance in AWS region us-west-2, as previously mentioned. We recommend using an EC2 with at least 8GB of memory available.\nThe notebook was developed and tested using a t2.small instance (_ CPUs; 8GB memory). Python 3\nMost of these imports are from the Python standard library. However, you will need to install these packages into your Python 3 environment if you have not already done so:\n\ns3fs\nrequests\npandas\nxarray\nmatplotlib\ncartopy\n\n\n\n\nLearning Objectives\n\nimport needed libraries\ndefine dataset of interest\nauthenticate for NASA Earthdata archive (Earthdata Login)\nobtain AWS credentials for Earthdata DAAC archive in AWS S3\naccess DAAC data directly from the in-region S3 bucket without moving or downloading any files to your local (cloud) workspace\nplot the first time step in the data\n\n\nimport os\nimport subprocess\nfrom os.path import dirname, join\n\n# Access EDS\nimport requests\n\n# Access AWS S3\nimport boto3\nimport s3fs\n\n# Read and work with datasets\nimport pandas as pd\nimport numpy as np\nimport xarray as xr\n\n# Plotting\nimport matplotlib.pyplot as plt\nimport cartopy\nimport cartopy.crs as ccrs\nimport cartopy.feature as cfeat\n\n\nDefine dataset of interest\nIn this case it’s the following string that unique identifies the collection of monthly, 0.5-degree sea surface height data.\n\nShortName = \"ECCO_L4_SSH_05DEG_MONTHLY_V4R4\"\n\n\n\n\nEarthdata login\nYou should have a .netrc file set up like:\nmachine urs.earthdata.nasa.gov login <username> password <password>\nSee the following (Authentication for NASA Earthdata tutorial)[https://nasa-openscapes.github.io/2021-Cloud-Hackathon/tutorials/04_NASA_Earthdata_Authentication.html]\n\n\nAWS credentials to Access Data from S3\nPass credentials and configuration to AWS so we can interact with S3 objects from applicable buckets. For now, each DAAC has different AWS credentials endpoints. LP DAAC and PO.DAAC are listed here:\n\ns3_cred_endpoint = {\n    'podaac':'https://archive.podaac.earthdata.nasa.gov/s3credentials',\n    'lpdaac':'https://data.lpdaac.earthdatacloud.nasa.gov/s3credentials'\n}\n\nIn this example we’re interested in the ECCO data collection from PO.DAAC in Earthdata Cloud in AWS S3, so we specify the podaac endpoint in the next code block.\nSet up an s3fs session for authneticated access to ECCO netCDF files in s3:\n\ndef begin_s3_direct_access(url: str=s3_cred_endpoint['podaac']):\n    response = requests.get(url).json()\n    return s3fs.S3FileSystem(key=response['accessKeyId'],\n                             secret=response['secretAccessKey'],\n                             token=response['sessionToken'],\n                             client_kwargs={'region_name':'us-west-2'})\n\nfs = begin_s3_direct_access()\n\nGet a list of netCDF files located at the S3 path corresponding to the ECCO V4r4 monthly sea surface height dataset on the 0.5-degree latitude/longitude grid, for year 2015.\n\nssh_Files = fs.glob(join(\"podaac-ops-cumulus-protected/\", ShortName, \"*2015*.nc\"))\n\nlen(ssh_Files)\n\n12\n\n\n\nAccess in-region S3 cloud data without moving files\nNow that we have authenticated in AWS, this next code block accesses data directly from the NASA Earthdata archive in an S3 bucket in us-west-2 region, without downloading or moving any files into your user cloud workspace (instnace).\nOpen with the netCDF files using the s3fs package, then load them all at once into a concatenated xarray dataset.\n\nssh_Dataset = xr.open_mfdataset(\n    paths=[fs.open(f) for f in ssh_Files],\n    combine='by_coords',\n    mask_and_scale=True,\n    decode_cf=True,\n    chunks={'latitude': 60,   # These were chosen arbitrarily. You must specify \n            'longitude': 120, # chunking that is suitable to the data and target\n            'time': 100}      # analysis.\n)\n\nssh = ssh_Dataset.SSH\n\nprint(ssh)\n\n<xarray.DataArray 'SSH' (time: 12, latitude: 360, longitude: 720)>\ndask.array<concatenate, shape=(12, 360, 720), dtype=float32, chunksize=(1, 60, 120), chunktype=numpy.ndarray>\nCoordinates:\n  * time       (time) datetime64[ns] 2015-01-16T12:00:00 ... 2015-12-16T12:00:00\n  * latitude   (latitude) float32 -89.75 -89.25 -88.75 ... 88.75 89.25 89.75\n  * longitude  (longitude) float32 -179.8 -179.2 -178.8 ... 178.8 179.2 179.8\nAttributes:\n    coverage_content_type:  modelResult\n    long_name:              Dynamic sea surface height anomaly\n    standard_name:          sea_surface_height_above_geoid\n    units:                  m\n    comment:                Dynamic sea surface height anomaly above the geoi...\n    valid_min:              [-1.88057721]\n    valid_max:              [1.42077196]\n\n\n\n\n\nPlot the gridded sea surface height time series\nBut only the timesteps beginning in 2015:\n\nssh_after_201x = ssh[ssh['time.year']>=2015,:,:]\n\nprint(ssh_after_201x)\n\n<xarray.DataArray 'SSH' (time: 12, latitude: 360, longitude: 720)>\ndask.array<concatenate, shape=(12, 360, 720), dtype=float32, chunksize=(1, 60, 120), chunktype=numpy.ndarray>\nCoordinates:\n  * time       (time) datetime64[ns] 2015-01-16T12:00:00 ... 2015-12-16T12:00:00\n  * latitude   (latitude) float32 -89.75 -89.25 -88.75 ... 88.75 89.25 89.75\n  * longitude  (longitude) float32 -179.8 -179.2 -178.8 ... 178.8 179.2 179.8\nAttributes:\n    coverage_content_type:  modelResult\n    long_name:              Dynamic sea surface height anomaly\n    standard_name:          sea_surface_height_above_geoid\n    units:                  m\n    comment:                Dynamic sea surface height anomaly above the geoi...\n    valid_min:              [-1.88057721]\n    valid_max:              [1.42077196]\n\n\nPlot the grid for the first time step using a Robinson projection. Define a helper function for consistency throughout the notebook:\n\ndef make_figure(proj):\n    fig = plt.figure(figsize=(16,6))\n    ax = fig.add_subplot(1, 1, 1, projection=proj)\n    ax.add_feature(cfeat.LAND)\n    ax.add_feature(cfeat.OCEAN)\n    ax.add_feature(cfeat.COASTLINE)\n    ax.add_feature(cfeat.BORDERS, linestyle='dotted')\n    return fig, ax\n\nfig, ax = make_figure(proj=ccrs.Robinson())\n\nssh_after_201x.isel(time=0).plot(ax=ax, transform=ccrs.PlateCarree(), cmap='Spectral_r')\n\n<cartopy.mpl.geocollection.GeoQuadMesh at 0x7fd040602b20>\n\n\n\n\n\n\n\nAdditional Resources\n\nFull example with additional plots and use cases here: https://github.com/podaac/ECCO/blob/main/Data_Access/cloud_direct_access_s3.ipynb"
  },
  {
    "objectID": "Direct_Access_netCDF_simple.html",
    "href": "Direct_Access_netCDF_simple.html",
    "title": "2021 Cloud Workshop at AGU",
    "section": "",
    "text": "Direct Access - ECCO netCDF example\n\nGetting Started\nIn this notebook, we will access monthly sea surface height from ECCO V4r4 (10.5067/ECG5D-SSH44). The data are provided as a time series of monthly netCDFs on a 0.5-degree latitude/longitude grid.\nWe will access the data from inside the AWS cloud (us-west-2 region, specifically) and load a time series made of multiple netCDF datasets into a single xarray dataset. This approach leverages S3 native protocols for efficient access to the data.\n\n\nRequirements\n\nAWS\nThis notebook should be running in an EC2 instance in AWS region us-west-2, as previously mentioned. We recommend using an EC2 with at least 8GB of memory available.\nThe notebook was developed and tested using a t2.small instance (_ CPUs; 8GB memory). Python 3\nMost of these imports are from the Python standard library. However, you will need to install these packages into your Python 3 environment if you have not already done so:\n\ns3fs\nrequests\npandas\nxarray\nmatplotlib\ncartopy\n\n\n\n\nLearning Objectives\n\nimport needed libraries\ndefine dataset of interest\nauthenticate for NASA Earthdata archive (Earthdata Login)\nobtain AWS credentials for Earthdata DAAC archive in AWS S3\naccess DAAC data directly from the in-region S3 bucket without moving or downloading any files to your local (cloud) workspace\nplot the first time step in the data\n\n\nimport os\nimport subprocess\nfrom os.path import dirname, join\n\n# Access EDS\nimport requests\n\n# Access AWS S3\nimport boto3\nimport s3fs\n\n# Read and work with datasets\nimport pandas as pd\nimport numpy as np\nimport xarray as xr\n\n# Plotting\nimport matplotlib.pyplot as plt\nimport cartopy\nimport cartopy.crs as ccrs\nimport cartopy.feature as cfeat\n\n\nDefine dataset of interest\nIn this case it’s the following string that unique identifies the collection of monthly, 0.5-degree sea surface height data.\n\nShortName = \"ECCO_L4_SSH_05DEG_MONTHLY_V4R4\"\n\n\n\n\nEarthdata login\nYou should have a .netrc file set up like:\nmachine urs.earthdata.nasa.gov login <username> password <password>\nSee the following (Authentication for NASA Earthdata tutorial)[https://nasa-openscapes.github.io/2021-Cloud-Hackathon/tutorials/04_NASA_Earthdata_Authentication.html]\n\n\nAWS credentials to Access Data from S3\nPass credentials and configuration to AWS so we can interact with S3 objects from applicable buckets. For now, each DAAC has different AWS credentials endpoints. LP DAAC and PO.DAAC are listed here:\n\ns3_cred_endpoint = {\n    'podaac':'https://archive.podaac.earthdata.nasa.gov/s3credentials',\n    'lpdaac':'https://data.lpdaac.earthdatacloud.nasa.gov/s3credentials'\n}\n\nIn this example we’re interested in the ECCO data collection from PO.DAAC in Earthdata Cloud in AWS S3, so we specify the podaac endpoint in the next code block.\nSet up an s3fs session for authneticated access to ECCO netCDF files in s3:\n\ndef begin_s3_direct_access(url: str=s3_cred_endpoint['podaac']):\n    response = requests.get(url).json()\n    return s3fs.S3FileSystem(key=response['accessKeyId'],\n                             secret=response['secretAccessKey'],\n                             token=response['sessionToken'],\n                             client_kwargs={'region_name':'us-west-2'})\n\nfs = begin_s3_direct_access()\n\nGet a list of netCDF files located at the S3 path corresponding to the ECCO V4r4 monthly sea surface height dataset on the 0.5-degree latitude/longitude grid, for year 2015.\n\nssh_Files = fs.glob(join(\"podaac-ops-cumulus-protected/\", ShortName, \"*2015*.nc\"))\n\nlen(ssh_Files)\n\n12\n\n\n\nAccess in-region S3 cloud data without moving files\nNow that we have authenticated in AWS, this next code block accesses data directly from the NASA Earthdata archive in an S3 bucket in us-west-2 region, without downloading or moving any files into your user cloud workspace (instnace).\nOpen with the netCDF files using the s3fs package, then load them all at once into a concatenated xarray dataset.\n\nssh_Dataset = xr.open_mfdataset(\n    paths=[fs.open(f) for f in ssh_Files],\n    combine='by_coords',\n    mask_and_scale=True,\n    decode_cf=True,\n    chunks={'latitude': 60,   # These were chosen arbitrarily. You must specify \n            'longitude': 120, # chunking that is suitable to the data and target\n            'time': 100}      # analysis.\n)\n\nssh = ssh_Dataset.SSH\n\nprint(ssh)\n\n<xarray.DataArray 'SSH' (time: 12, latitude: 360, longitude: 720)>\ndask.array<concatenate, shape=(12, 360, 720), dtype=float32, chunksize=(1, 60, 120), chunktype=numpy.ndarray>\nCoordinates:\n  * time       (time) datetime64[ns] 2015-01-16T12:00:00 ... 2015-12-16T12:00:00\n  * latitude   (latitude) float32 -89.75 -89.25 -88.75 ... 88.75 89.25 89.75\n  * longitude  (longitude) float32 -179.8 -179.2 -178.8 ... 178.8 179.2 179.8\nAttributes:\n    coverage_content_type:  modelResult\n    long_name:              Dynamic sea surface height anomaly\n    standard_name:          sea_surface_height_above_geoid\n    units:                  m\n    comment:                Dynamic sea surface height anomaly above the geoi...\n    valid_min:              [-1.88057721]\n    valid_max:              [1.42077196]\n\n\n\n\n\nPlot the gridded sea surface height time series\nBut only the timesteps beginning in 2015:\n\nssh_after_201x = ssh[ssh['time.year']>=2015,:,:]\n\nprint(ssh_after_201x)\n\n<xarray.DataArray 'SSH' (time: 12, latitude: 360, longitude: 720)>\ndask.array<concatenate, shape=(12, 360, 720), dtype=float32, chunksize=(1, 60, 120), chunktype=numpy.ndarray>\nCoordinates:\n  * time       (time) datetime64[ns] 2015-01-16T12:00:00 ... 2015-12-16T12:00:00\n  * latitude   (latitude) float32 -89.75 -89.25 -88.75 ... 88.75 89.25 89.75\n  * longitude  (longitude) float32 -179.8 -179.2 -178.8 ... 178.8 179.2 179.8\nAttributes:\n    coverage_content_type:  modelResult\n    long_name:              Dynamic sea surface height anomaly\n    standard_name:          sea_surface_height_above_geoid\n    units:                  m\n    comment:                Dynamic sea surface height anomaly above the geoi...\n    valid_min:              [-1.88057721]\n    valid_max:              [1.42077196]\n\n\nPlot the grid for the first time step using a Robinson projection. Define a helper function for consistency throughout the notebook:\n\ndef make_figure(proj):\n    fig = plt.figure(figsize=(16,6))\n    ax = fig.add_subplot(1, 1, 1, projection=proj)\n    ax.add_feature(cfeat.LAND)\n    ax.add_feature(cfeat.OCEAN)\n    ax.add_feature(cfeat.COASTLINE)\n    ax.add_feature(cfeat.BORDERS, linestyle='dotted')\n    return fig, ax\n\nfig, ax = make_figure(proj=ccrs.Robinson())\n\nssh_after_201x.isel(time=0).plot(ax=ax, transform=ccrs.PlateCarree(), cmap='Spectral_r')\n\n<cartopy.mpl.geocollection.GeoQuadMesh at 0x7fd040602b20>\n\n\n\n\n\n\n\nAdditional Resources\n\nFull example with additional plots and use cases here: https://github.com/podaac/ECCO/blob/main/Data_Access/cloud_direct_access_s3.ipynb"
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Single_File__HTTPS_Access_COG_Example.html#summary",
    "href": "how-tos/Earthdata_Cloud__Single_File__HTTPS_Access_COG_Example.html#summary",
    "title": "2021 Cloud Workshop at AGU",
    "section": "Summary",
    "text": "Summary\nIn this notebook, we will access data for the Harmonized Landsat Sentinel-2 (HLS) Operational Land Imager Surface Reflectance and TOA Brightness Daily Global 30m v2.0 (L30) (10.5067/HLS/HLSL30.002) data product. These data are archived and distributed as Cloud Optimized GeoTIFF (COG) files, one file for each spectral band.\nWe will access a single COG file, L30 red band (0.64 – 0.67 μm), from inside the AWS cloud (us-west-2 region, specifically) and load it into Python as an xarray dataarray. This approach leverages S3 native protocols for efficient access to the data."
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Single_File__HTTPS_Access_COG_Example.html#requirements",
    "href": "how-tos/Earthdata_Cloud__Single_File__HTTPS_Access_COG_Example.html#requirements",
    "title": "2021 Cloud Workshop at AGU",
    "section": "Requirements",
    "text": "Requirements\n\n1. AWS instance running in us-west-2\nNASA Earthdata Cloud data in S3 can be directly accessed via temporary credentials; this access is limited to requests made within the US West (Oregon) (code: us-west-2) AWS region.\n\n\n2. Earthdata Login\nAn Earthdata Login account is required to access data, as well as discover restricted data, from the NASA Earthdata system. Thus, to access NASA data, you need Earthdata Login. Please visit https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account. This account is free to create and only takes a moment to set up.\n\n\n3. netrc File\nYou will need a netrc file containing your NASA Earthdata Login credentials in order to execute the notebooks. A netrc file can be created manually within text editor and saved to your home directory. For additional information see: Authentication for NASA Earthdata."
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Single_File__HTTPS_Access_COG_Example.html#learning-objectives",
    "href": "how-tos/Earthdata_Cloud__Single_File__HTTPS_Access_COG_Example.html#learning-objectives",
    "title": "2021 Cloud Workshop at AGU",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nhow to configure you Python work environment to access Cloud Optimized geoTIFF (COG) files\nhow to access HLS COG files\nhow to plot the data"
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Single_File__HTTPS_Access_COG_Example.html#cloud-optimized-geotiff-cog",
    "href": "how-tos/Earthdata_Cloud__Single_File__HTTPS_Access_COG_Example.html#cloud-optimized-geotiff-cog",
    "title": "2021 Cloud Workshop at AGU",
    "section": "Cloud Optimized GeoTIFF (COG)",
    "text": "Cloud Optimized GeoTIFF (COG)\nUsing Harmonized Landsat Sentinel-2 (HLS) version 2.0\n\nImport Packages\n\nimport os\nfrom osgeo import gdal\nimport rasterio as rio\nimport rioxarray\nimport hvplot.xarray\nimport holoviews as hv"
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Single_File__HTTPS_Access_COG_Example.html#workspace-environment-setup",
    "href": "how-tos/Earthdata_Cloud__Single_File__HTTPS_Access_COG_Example.html#workspace-environment-setup",
    "title": "2021 Cloud Workshop at AGU",
    "section": "Workspace Environment Setup",
    "text": "Workspace Environment Setup\nFor this exercise, we are going to open up a context manager for the notebook using the rasterio.env module to store the required GDAL configurations we need to access the data from Earthdata Cloud. While the context manager is open (rio_env.__enter__()) we will be able to run the open or get data commands that would typically be executed within a with statement, thus allowing us to more freely interact with the data. We’ll close the context (rio_env.__exit__()) at the end of the notebook.\nGDAL environment variables must be configured to access COGs from Earthdata Cloud. Geospatial data access Python packages like rasterio and rioxarray depend on GDAL, leveraging GDAL’s “Virtual File Systems” to read remote files. GDAL has a lot of environment variables that control it’s behavior. Changing these settings can mean the difference being able to access a file or not. They can also have an impact on the performance.\n\nrio_env = rio.Env(GDAL_DISABLE_READDIR_ON_OPEN='TRUE',\n                  GDAL_HTTP_COOKIEFILE=os.path.expanduser('~/cookies.txt'),\n                  GDAL_HTTP_COOKIEJAR=os.path.expanduser('~/cookies.txt'))\nrio_env.__enter__()\n\nIn this example we’re interested in the HLS L30 data collection from NASA’s LP DAAC in Earthdata Cloud. Below we specify the s3 URL to the data asset in Earthdata Cloud. This URL can be found via Earthdata Search or programmatically through the CMR and CMR-STAC APIs.\n\nhttps_url = 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T11SQA.2021333T181532.v2.0/HLS.L30.T11SQA.2021333T181532.v2.0.B04.tif'"
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Single_File__HTTPS_Access_COG_Example.html#https-data-access",
    "href": "how-tos/Earthdata_Cloud__Single_File__HTTPS_Access_COG_Example.html#https-data-access",
    "title": "2021 Cloud Workshop at AGU",
    "section": "HTTPS Data Access",
    "text": "HTTPS Data Access\nRead in the HLS s3 URL for the L30 red band (0.64 – 0.67 μm) into our workspace using rioxarray, an extension of xarray used to read geospatial data.\n\nda = rioxarray.open_rasterio(https_url)\nda\n\nThe file is read into Python as an xarray dataarray with a band, x, and y dimension. In this example the band dimension is meaningless, so we’ll use the squeeze() function to remove band as a dimension.\n\nda_red = da.squeeze('band', drop=True)\nda_red\n\nPlot the dataarray, representing the L30 red band, using hvplot.\n\nda_red.hvplot.image(x='x', y='y', cmap='gray', aspect='equal')\n\nExit the context manager.\n\nrio_env.__exit__()"
  },
  {
    "objectID": "how-tos/Multi-File_Direct_S3_Access_COG_Example.html",
    "href": "how-tos/Multi-File_Direct_S3_Access_COG_Example.html",
    "title": "2021 Cloud Workshop at AGU",
    "section": "",
    "text": "from pystac_client import Client\nimport stackstac\n\n\nSTAC_URL = 'https://cmr.earthdata.nasa.gov/stac'\n\n\ncatalog = Client.open(f\"{STAC_URL}/LPCLOUD\")\n\n\nsearch = catalog.search(\n    collections = ['HLSL30.v2.0', 'HLSS30.v2.0'],\n    intersects = {'type': 'Polygon',\n                  'coordinates': [[[-101.67271614074707, 41.04754380304359],\n                                   [-101.65344715118408, 41.04754380304359],\n                                   [-101.65344715118408, 41.06213891056728],\n                                   [-101.67271614074707, 41.06213891056728],\n                                   [-101.67271614074707, 41.04754380304359]]]},\n    datetime = '2021-05/2021-08'\n)               \n\n\nsearch.matched()\n\n\nic = search.get_all_items()\n\n\nil = list(search.get_items())\n\n\ntic = [x for x in ic if 'T13TGF' in x.id]\n\n\nimport pystac\n\n\nitem_collection = pystac.ItemCollection(items=tic)\n\n\nitem_collection\n\n\nil\n\n\ndata = stackstac.stack(item_collection, assets=['B04', 'B02'], epsg=32613, resolution=30)\n\n\ndata.sel(band='B04').isel(time=[0])\n\n\nimport stackstac\nimport pystac_client\n\nURL = \"https://earth-search.aws.element84.com/v0\"\ncatalog = pystac_client.Client.open(URL)\n\n\ncatalog\n\n\nstac_items = catalog.search(\n    intersects=dict(type=\"Point\", coordinates=[-105.78, 35.79]),\n    collections=[\"sentinel-s2-l2a-cogs\"],\n    datetime=\"2020-04-01/2020-05-01\"\n).get_all_items()\n\n\nstac_items\n\n\nstack = stackstac.stack(stac_items)\n\n\nstack"
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Single_File__Direct_S3_Access_COG_Example.html#summary",
    "href": "how-tos/Earthdata_Cloud__Single_File__Direct_S3_Access_COG_Example.html#summary",
    "title": "2021 Cloud Workshop at AGU",
    "section": "Summary",
    "text": "Summary\nIn this notebook, we will access data for the Harmonized Landsat Sentinel-2 (HLS) Operational Land Imager Surface Reflectance and TOA Brightness Daily Global 30m v2.0 (L30) (10.5067/HLS/HLSL30.002) data product. These data are archived and distributed as Cloud Optimized GeoTIFF (COG) files, one file for each spectral band.\nWe will access a single COG file, L30 red band (0.64 – 0.67 μm), from inside the AWS cloud (us-west-2 region, specifically) and load it into Python as an xarray dataarray. This approach leverages S3 native protocols for efficient access to the data."
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Single_File__Direct_S3_Access_COG_Example.html#requirements",
    "href": "how-tos/Earthdata_Cloud__Single_File__Direct_S3_Access_COG_Example.html#requirements",
    "title": "2021 Cloud Workshop at AGU",
    "section": "Requirements",
    "text": "Requirements\n\n1. AWS instance running in us-west-2\nNASA Earthdata Cloud data in S3 can be directly accessed via temporary credentials; this access is limited to requests made within the US West (Oregon) (code: us-west-2) AWS region.\n\n\n2. Earthdata Login\nAn Earthdata Login account is required to access data, as well as discover restricted data, from the NASA Earthdata system. Thus, to access NASA data, you need Earthdata Login. Please visit https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account. This account is free to create and only takes a moment to set up.\n\n\n3. netrc File\nYou will need a netrc file containing your NASA Earthdata Login credentials in order to execute the notebooks. A netrc file can be created manually within text editor and saved to your home directory. For additional information see: Authentication for NASA Earthdata."
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Single_File__Direct_S3_Access_COG_Example.html#learning-objectives",
    "href": "how-tos/Earthdata_Cloud__Single_File__Direct_S3_Access_COG_Example.html#learning-objectives",
    "title": "2021 Cloud Workshop at AGU",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nhow to retrieve temporary S3 credentials for in-region direct S3 bucket access\nhow to perform in-region direct access of HLS Cloud Optimized geoTIFF (COG) files in S3\nhow to plot the data\n\n\n\nImport Packages\n\nimport os\nimport requests \nimport boto3\nfrom osgeo import gdal\nimport rasterio as rio\nfrom rasterio.session import AWSSession\nimport rioxarray\nimport hvplot.xarray\nimport holoviews as hv"
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Single_File__Direct_S3_Access_COG_Example.html#get-temporary-aws-credentials",
    "href": "how-tos/Earthdata_Cloud__Single_File__Direct_S3_Access_COG_Example.html#get-temporary-aws-credentials",
    "title": "2021 Cloud Workshop at AGU",
    "section": "Get Temporary AWS Credentials",
    "text": "Get Temporary AWS Credentials\nDirect S3 access is achieved by passing NASA supplied temporary credentials to AWS so we can interact with S3 objects from applicable Earthdata Cloud buckets. For now, each NASA DAAC has different AWS credentials endpoints. Below are some of the credential endpoints to various DAACs:\n\ns3_cred_endpoint = {\n    'podaac':'https://archive.podaac.earthdata.nasa.gov/s3credentials',\n    'gesdisc': 'https://data.gesdisc.earthdata.nasa.gov/s3credentials',\n    'lpdaac':'https://data.lpdaac.earthdatacloud.nasa.gov/s3credentials',\n    'ornldaac': 'https://data.ornldaac.earthdata.nasa.gov/s3credentials',\n    'ghrcdaac': 'https://data.ghrc.earthdata.nasa.gov/s3credentials'\n}\n\nCreate a function to make a request to an endpoint for temporary credentials. Remember, each DAAC has their own endpoint and credentials are not usable for cloud data from other DAACs.\n\ndef get_temp_creds(provider):\n    return requests.get(s3_cred_endpoint[provider]).json()\n\n\ntemp_creds_req = get_temp_creds('lpdaac')\n#temp_creds_req"
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Single_File__Direct_S3_Access_COG_Example.html#workspace-environment-setup",
    "href": "how-tos/Earthdata_Cloud__Single_File__Direct_S3_Access_COG_Example.html#workspace-environment-setup",
    "title": "2021 Cloud Workshop at AGU",
    "section": "Workspace Environment Setup",
    "text": "Workspace Environment Setup\nFor this exercise, we are going to open up a context manager for the notebook using the rasterio.env module to store the required GDAL and AWS configurations we need to access the data in Earthdata Cloud. While the context manager is open (rio_env.__enter__()) we will be able to run the open or get data commands that would typically be executed within a with statement, thus allowing us to more freely interact with the data. We’ll close the context (rio_env.__exit__()) at the end of the notebook.\nCreate a boto3 Session object using your temporary credentials. This Session is used to pass credentials and configuration to AWS so we can interact wit S3 objects from applicable buckets.\n\nsession = boto3.Session(aws_access_key_id=temp_creds_req['accessKeyId'], \n                        aws_secret_access_key=temp_creds_req['secretAccessKey'],\n                        aws_session_token=temp_creds_req['sessionToken'],\n                        region_name='us-west-2')\n\nGDAL environment variables must be configured to access COGs in Earthdata Cloud. Geospatial data access Python packages like rasterio and rioxarray depend on GDAL, leveraging GDAL’s “Virtual File Systems” to read remote files. GDAL has a lot of environment variables that control it’s behavior. Changing these settings can mean the difference being able to access a file or not. They can also have an impact on the performance.\n\nrio_env = rio.Env(AWSSession(session),\n                  GDAL_DISABLE_READDIR_ON_OPEN='TRUE',\n                  GDAL_HTTP_COOKIEFILE=os.path.expanduser('~/cookies.txt'),\n                  GDAL_HTTP_COOKIEJAR=os.path.expanduser('~/cookies.txt'))\nrio_env.__enter__()\n\nIn this example we’re interested in the HLS L30 data collection from NASA’s LP DAAC in Earthdata Cloud. Below we specify the s3 URL to the data asset in Earthdata Cloud. This URL can be found via Earthdata Search or programmatically through the CMR and CMR-STAC APIs.\n\ns3_url = 's3://lp-prod-protected/HLSL30.020/HLS.L30.T11SQA.2021333T181532.v2.0/HLS.L30.T11SQA.2021333T181532.v2.0.B04.tif'"
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Single_File__Direct_S3_Access_COG_Example.html#direct-in-region-access",
    "href": "how-tos/Earthdata_Cloud__Single_File__Direct_S3_Access_COG_Example.html#direct-in-region-access",
    "title": "2021 Cloud Workshop at AGU",
    "section": "Direct In-region Access",
    "text": "Direct In-region Access\nRead in the HLS s3 URL for the L30 red band (0.64 – 0.67 μm) into our workspace using rioxarray, an extension of xarray used to read geospatial data.\n\nda = rioxarray.open_rasterio(s3_url)\nda\n\nThe file is read into Python as an xarray dataarray with a band, x, and y dimension. In this example the band dimension is meaningless, so we’ll use the squeeze() function to remove band as a dimension.\n\nda_red = da.squeeze('band', drop=True)\nda_red\n\nPlot the dataarray, representing the L30 red band, using hvplot.\n\nda_red.hvplot.image(x='x', y='y', cmap='gray', aspect='equal')\n\nExit the context manager.\n\nrio_env.__exit__()"
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Single_File__Direct_S3_Access_COG_Example.html#resources",
    "href": "how-tos/Earthdata_Cloud__Single_File__Direct_S3_Access_COG_Example.html#resources",
    "title": "2021 Cloud Workshop at AGU",
    "section": "Resources",
    "text": "Resources\nDirect S3 Data Access with rioxarray\nDirect_S3_Access__gdalvrt\nDirect_S3_Access__rioxarray_clipping\nGetting Started with Cloud-Native Harmonized Landsat Sentinel-2 (HLS) Data in R"
  },
  {
    "objectID": "how-tos/Multi-File_Direct_S3_Access_NetCDF_Example.html#summary",
    "href": "how-tos/Multi-File_Direct_S3_Access_NetCDF_Example.html#summary",
    "title": "2021 Cloud Workshop at AGU",
    "section": "Summary",
    "text": "Summary\nIn this notebook, we will access monthly sea surface height from ECCO V4r4 (10.5067/ECG5D-SSH44). The data are provided as a time series of monthly netCDFs on a 0.5-degree latitude/longitude grid.\nWe will access the data from inside the AWS cloud (us-west-2 region, specifically) and load a time series made of multiple netCDF datasets into an xarray dataset. This approach leverages S3 native protocols for efficient access to the data."
  },
  {
    "objectID": "how-tos/Multi-File_Direct_S3_Access_NetCDF_Example.html#requirements",
    "href": "how-tos/Multi-File_Direct_S3_Access_NetCDF_Example.html#requirements",
    "title": "2021 Cloud Workshop at AGU",
    "section": "Requirements",
    "text": "Requirements\n\n1. AWS instance running in us-west-2\nNASA Earthdata Cloud data in S3 can be directly accessed via temporary credentials; this access is limited to requests made within the US West (Oregon) (code: us-west-2) AWS region.\n\n\n2. Earthdata Login\nAn Earthdata Login account is required to access data, as well as discover restricted data, from the NASA Earthdata system. Thus, to access NASA data, you need Earthdata Login. Please visit https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account. This account is free to create and only takes a moment to set up.\n\n\n3. netrc File\nYou will need a netrc file containing your NASA Earthdata Login credentials in order to execute the notebooks. A netrc file can be created manually within text editor and saved to your home directory. For additional information see: Authentication for NASA Earthdata."
  },
  {
    "objectID": "how-tos/Multi-File_Direct_S3_Access_NetCDF_Example.html#learning-objectives",
    "href": "how-tos/Multi-File_Direct_S3_Access_NetCDF_Example.html#learning-objectives",
    "title": "2021 Cloud Workshop at AGU",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nhow to retrieve temporary S3 credentials for in-region direct S3 bucket access\nhow to define a dataset of interest and find netCDF files in S3 bucket\nhow to perform in-region direct access of ECCO_L4_SSH_05DEG_MONTHLY_V4R4 data in S3\nhow to plot the data"
  },
  {
    "objectID": "how-tos/Multi-File_Direct_S3_Access_NetCDF_Example.html#import-packages",
    "href": "how-tos/Multi-File_Direct_S3_Access_NetCDF_Example.html#import-packages",
    "title": "2021 Cloud Workshop at AGU",
    "section": "Import Packages",
    "text": "Import Packages\n\nimport os\nimport requests\nimport s3fs\nimport xarray as xr\nimport hvplot.xarray"
  },
  {
    "objectID": "how-tos/Multi-File_Direct_S3_Access_NetCDF_Example.html#get-temporary-aws-credentials",
    "href": "how-tos/Multi-File_Direct_S3_Access_NetCDF_Example.html#get-temporary-aws-credentials",
    "title": "2021 Cloud Workshop at AGU",
    "section": "Get Temporary AWS Credentials",
    "text": "Get Temporary AWS Credentials\nDirect S3 access is achieved by passing NASA supplied temporary credentials to AWS so we can interact with S3 objects from applicable Earthdata Cloud buckets. For now, each NASA DAAC has different AWS credentials endpoints. Below are some of the credential endpoints to various DAACs:\n\ns3_cred_endpoint = {\n    'podaac':'https://archive.podaac.earthdata.nasa.gov/s3credentials',\n    'gesdisc': 'https://data.gesdisc.earthdata.nasa.gov/s3credentials',\n    'lpdaac':'https://data.lpdaac.earthdatacloud.nasa.gov/s3credentials',\n    'ornldaac': 'https://data.ornldaac.earthdata.nasa.gov/s3credentials',\n    'ghrcdaac': 'https://data.ghrc.earthdata.nasa.gov/s3credentials'\n}\n\nCreate a function to make a request to an endpoint for temporary credentials. Remember, each DAAC has their own endpoint and credentials are not usable for cloud data from other DAACs.\n\ndef get_temp_creds(provider):\n    return requests.get(s3_cred_endpoint[provider]).json()\n\n\ntemp_creds_req = get_temp_creds('podaac')\n#temp_creds_req"
  },
  {
    "objectID": "how-tos/Multi-File_Direct_S3_Access_NetCDF_Example.html#set-up-an-s3fs-session-for-direct-access",
    "href": "how-tos/Multi-File_Direct_S3_Access_NetCDF_Example.html#set-up-an-s3fs-session-for-direct-access",
    "title": "2021 Cloud Workshop at AGU",
    "section": "Set up an s3fs session for Direct Access",
    "text": "Set up an s3fs session for Direct Access\ns3fs sessions are used for authenticated access to s3 bucket and allows for typical file-system style operations. Below we create session by passing in the temporary credentials we recieved from our temporary credentials endpoint.\n\nfs_s3 = s3fs.S3FileSystem(anon=False, \n                          key=temp_creds_req['accessKeyId'], \n                          secret=temp_creds_req['secretAccessKey'], \n                          token=temp_creds_req['sessionToken'],\n                          client_kwargs={'region_name':'us-west-2'})\n\nIn this example we’re interested in the ECCO data collection from NASA’s PO.DAAC in Earthdata Cloud. In this case it’s the following string that unique identifies the collection of monthly, 0.5-degree sea surface height data (ECCO_L4_SSH_05DEG_MONTHLY_V4R4).\n\nshort_name = 'ECCO_L4_SSH_05DEG_MONTHLY_V4R4'\n\n\nbucket = os.path.join('podaac-ops-cumulus-protected/', short_name, '*2015*.nc')\nbucket\n\nGet a list of netCDF files located at the S3 path corresponding to the ECCO V4r4 monthly sea surface height dataset on the 0.5-degree latitude/longitude grid, for year 2015.\n\nssh_files = fs_s3.glob(bucket)\nssh_files"
  },
  {
    "objectID": "how-tos/Multi-File_Direct_S3_Access_NetCDF_Example.html#direct-in-region-access",
    "href": "how-tos/Multi-File_Direct_S3_Access_NetCDF_Example.html#direct-in-region-access",
    "title": "2021 Cloud Workshop at AGU",
    "section": "Direct In-region Access",
    "text": "Direct In-region Access\nOpen with the netCDF files using the s3fs package, then load them all at once into a concatenated xarray dataset.\n\nfileset = [fs_s3.open(file) for file in ssh_files]\n\nCreate an xarray dataset using the open_mfdataset() function to “read in” all of the netCDF4 files in one call.\n\nssh_ds = xr.open_mfdataset(fileset,\n                           combine='by_coords',\n                           mask_and_scale=True,\n                           decode_cf=True,\n                           chunks='auto')\nssh_ds\n\nGet the SSH variable as an xarray dataarray\n\nssh_da = ssh_ds.SSH\nssh_da\n\nPlot the SSH time series using hvplot\n\nssh_da.hvplot.image(y='latitude', x='longitude', cmap='Viridis',).opts(clim=(ssh_da.attrs['valid_min'][0],ssh_da.attrs['valid_max'][0]))"
  },
  {
    "objectID": "how-tos/Multi-File_Direct_S3_Access_NetCDF_Example.html#resources",
    "href": "how-tos/Multi-File_Direct_S3_Access_NetCDF_Example.html#resources",
    "title": "2021 Cloud Workshop at AGU",
    "section": "Resources",
    "text": "Resources\nDirect access to ECCO data in S3 (from us-west-2)\nData_Access__Direct_S3_Access__PODAAC_ECCO_SSH using CMR-STAC API to retrieve S3 links"
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Single_File__Direct_S3_Access_NetCDF4_Example.html#summary",
    "href": "how-tos/Earthdata_Cloud__Single_File__Direct_S3_Access_NetCDF4_Example.html#summary",
    "title": "2021 Cloud Workshop at AGU",
    "section": "Summary",
    "text": "Summary\nIn this notebook, we will access monthly sea surface height from ECCO V4r4 (10.5067/ECG5D-SSH44). The data are provided as a time series of monthly netCDFs on a 0.5-degree latitude/longitude grid.\nWe will access a single netCDF file from inside the AWS cloud (us-west-2 region, specifically) and load it into Python as an xarray dataset. This approach leverages S3 native protocols for efficient access to the data."
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Single_File__Direct_S3_Access_NetCDF4_Example.html#requirements",
    "href": "how-tos/Earthdata_Cloud__Single_File__Direct_S3_Access_NetCDF4_Example.html#requirements",
    "title": "2021 Cloud Workshop at AGU",
    "section": "Requirements",
    "text": "Requirements\n\n1. AWS instance running in us-west-2\nNASA Earthdata Cloud data in S3 can be directly accessed via temporary credentials; this access is limited to requests made within the US West (Oregon) (code: us-west-2) AWS region.\n\n\n2. Earthdata Login\nAn Earthdata Login account is required to access data, as well as discover restricted data, from the NASA Earthdata system. Thus, to access NASA data, you need Earthdata Login. Please visit https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account. This account is free to create and only takes a moment to set up.\n\n\n3. netrc File\nYou will need a netrc file containing your NASA Earthdata Login credentials in order to execute the notebooks. A netrc file can be created manually within text editor and saved to your home directory. For additional information see: Authentication for NASA Earthdata."
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Single_File__Direct_S3_Access_NetCDF4_Example.html#learning-objectives",
    "href": "how-tos/Earthdata_Cloud__Single_File__Direct_S3_Access_NetCDF4_Example.html#learning-objectives",
    "title": "2021 Cloud Workshop at AGU",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nhow to retrieve temporary S3 credentials for in-region direct S3 bucket access\nhow to perform in-region direct access of ECCO_L4_SSH_05DEG_MONTHLY_V4R4 data in S3\nhow to plot the data"
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Single_File__Direct_S3_Access_NetCDF4_Example.html#import-packages",
    "href": "how-tos/Earthdata_Cloud__Single_File__Direct_S3_Access_NetCDF4_Example.html#import-packages",
    "title": "2021 Cloud Workshop at AGU",
    "section": "Import Packages",
    "text": "Import Packages\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport os\nimport requests\nimport s3fs\nfrom osgeo import gdal\nimport xarray as xr\nimport hvplot.xarray\nimport holoviews as hv"
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Single_File__Direct_S3_Access_NetCDF4_Example.html#get-temporary-aws-credentials",
    "href": "how-tos/Earthdata_Cloud__Single_File__Direct_S3_Access_NetCDF4_Example.html#get-temporary-aws-credentials",
    "title": "2021 Cloud Workshop at AGU",
    "section": "Get Temporary AWS Credentials",
    "text": "Get Temporary AWS Credentials\nDirect S3 access is achieved by passing NASA supplied temporary credentials to AWS so we can interact with S3 objects from applicable Earthdata Cloud buckets. For now, each NASA DAAC has different AWS credentials endpoints. Below are some of the credential endpoints to various DAACs:\n\ns3_cred_endpoint = {\n    'podaac':'https://archive.podaac.earthdata.nasa.gov/s3credentials',\n    'gesdisc': 'https://data.gesdisc.earthdata.nasa.gov/s3credentials',\n    'lpdaac':'https://data.lpdaac.earthdatacloud.nasa.gov/s3credentials',\n    'ornldaac': 'https://data.ornldaac.earthdata.nasa.gov/s3credentials',\n    'ghrcdaac': 'https://data.ghrc.earthdata.nasa.gov/s3credentials'\n}\n\nCreate a function to make a request to an endpoint for temporary credentials. Remember, each DAAC has their own endpoint and credentials are not usable for cloud data from other DAACs.\n\ndef get_temp_creds(provider):\n    return requests.get(s3_cred_endpoint[provider]).json()\n\n\ntemp_creds_req = get_temp_creds('podaac')\n#temp_creds_req"
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Single_File__Direct_S3_Access_NetCDF4_Example.html#set-up-an-s3fs-session-for-direct-access",
    "href": "how-tos/Earthdata_Cloud__Single_File__Direct_S3_Access_NetCDF4_Example.html#set-up-an-s3fs-session-for-direct-access",
    "title": "2021 Cloud Workshop at AGU",
    "section": "Set up an s3fs session for Direct Access",
    "text": "Set up an s3fs session for Direct Access\ns3fs sessions are used for authenticated access to s3 bucket and allows for typical file-system style operations. Below we create session by passing in the temporary credentials we recieved from our temporary credentials endpoint.\n\nfs_s3 = s3fs.S3FileSystem(anon=False, \n                          key=temp_creds_req['accessKeyId'], \n                          secret=temp_creds_req['secretAccessKey'], \n                          token=temp_creds_req['sessionToken'])\n\nIn this example we’re interested in the ECCO data collection from NASA’s PO.DAAC in Earthdata Cloud. Below we specify the s3 URL to the data asset in Earthdata Cloud. This URL can be found via Earthdata Search or programmatically through the CMR and CMR-STAC APIs.\n\ns3_url = 's3://podaac-ops-cumulus-protected/ECCO_L4_SSH_05DEG_MONTHLY_V4R4/SEA_SURFACE_HEIGHT_mon_mean_2015-01_ECCO_V4r4_latlon_0p50deg.nc'"
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Single_File__Direct_S3_Access_NetCDF4_Example.html#direct-in-region-access",
    "href": "how-tos/Earthdata_Cloud__Single_File__Direct_S3_Access_NetCDF4_Example.html#direct-in-region-access",
    "title": "2021 Cloud Workshop at AGU",
    "section": "Direct In-region Access",
    "text": "Direct In-region Access\nOpen with the netCDF file using the s3fs package, then load the cloud asset into an xarray dataset.\n\ns3_file_obj = fs_s3.open(s3_url, mode='rb')\n\n\nssh_ds = xr.open_dataset(s3_file_obj, engine='h5netcdf')\nssh_ds\n\nGet the SSH variable as an xarray dataarray\n\nssh_da = ssh_ds.SSH\nssh_da\n\nPlot the SSH dataarray for time 2015-01-16T12:00:00 using hvplot.\n\nssh_da.hvplot.image(x='longitude', y='latitude', cmap='Spectral_r', aspect='equal').opts(clim=(ssh_da.attrs['valid_min'][0],ssh_da.attrs['valid_max'][0]))"
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Single_File__Direct_S3_Access_NetCDF4_Example.html#resources",
    "href": "how-tos/Earthdata_Cloud__Single_File__Direct_S3_Access_NetCDF4_Example.html#resources",
    "title": "2021 Cloud Workshop at AGU",
    "section": "Resources",
    "text": "Resources\nDirect access to ECCO data in S3 (from us-west-2)\nData_Access__Direct_S3_Access__PODAAC_ECCO_SSH using CMR-STAC API to retrieve S3 links"
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#outline",
    "href": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#outline",
    "title": "2021 Cloud Workshop at AGU",
    "section": "Outline:",
    "text": "Outline:\n\nIntroduction to NASA Earthdata’s move to the cloud\n\nBackground and motivation\nEnabling Open Science via “Analysis-in-Place”\nResources for cloud adopters: NASA Earthdata Openscapes\n\nNASA Earthdata discovery and access in the cloud\n\nPart 1: Explore Earthdata cloud data availablity\nPart 2: Working with Cloud-Optimized GeoTIFFs using NASA’s Common Metadata Repository Spatio-Temporal Assett Catalog (CMR-STAC)\nPart 3: Working with Zarr-formatted data using NASA’s Harmony cloud transformation service\n\n\n\nTutorial materials are adapted from repos on the NASA Openscapes public Github:\n\nThis notebook source code: update https://github.com/NASA-Openscapes/2021-Cloud-Workshop-AGU/tree/main/how-tos\nAlso available via online Quarto book: update https://nasa-openscapes.github.io/2021-Cloud-Workshop-AGU/"
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#the-nasa-earthdata-archive-continues-to-grow",
    "href": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#the-nasa-earthdata-archive-continues-to-grow",
    "title": "2021 Cloud Workshop at AGU",
    "section": "The NASA Earthdata archive continues to grow",
    "text": "The NASA Earthdata archive continues to grow\n\n\n\nEOSDIS Data Archive"
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#the-nasa-earthdata-cloud-evolution",
    "href": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#the-nasa-earthdata-cloud-evolution",
    "title": "2021 Cloud Workshop at AGU",
    "section": "The NASA Earthdata Cloud Evolution",
    "text": "The NASA Earthdata Cloud Evolution\n \n\nNASA Distributed Active Archive Centers (DAACs) are continuing to migrate data to the Earthdata Cloud\n\nSupporting increased data volume as new, high-resolution remote sensing missions launch in the coming years\nData hosted via Amazon Web Services, or AWS\nDAACs continuing to support tools, services, and tutorial resources for our user communities"
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#nasa-earthdata-cloud-as-an-enabler-of-open-science",
    "href": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#nasa-earthdata-cloud-as-an-enabler-of-open-science",
    "title": "2021 Cloud Workshop at AGU",
    "section": "NASA Earthdata Cloud as an enabler of Open Science",
    "text": "NASA Earthdata Cloud as an enabler of Open Science\n\nReducing barriers to large-scale scientific research in the era of “big data”\nIncreasing community contributions with hands-on engagement\nPromoting reproducible and shareable workflows without relying on local storage systems\n\n\n\n\nOpen Data"
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#data-and-analysis-co-located-in-place",
    "href": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#data-and-analysis-co-located-in-place",
    "title": "2021 Cloud Workshop at AGU",
    "section": "Data and Analysis co-located “in place”",
    "text": "Data and Analysis co-located “in place”\n\n\n\nEarthdata Cloud Paradigm"
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#building-nasa-earthdata-cloud-resources",
    "href": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#building-nasa-earthdata-cloud-resources",
    "title": "2021 Cloud Workshop at AGU",
    "section": "Building NASA Earthdata Cloud Resources",
    "text": "Building NASA Earthdata Cloud Resources\nShow slide with 3 panels of user resources\nEmphasize that the following tutorials are short examples that were taken from the tutorial resources we have been building for our users"
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#nasa-earthdata-cloud-discovery-and-access-using-open-source-technologies",
    "href": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#nasa-earthdata-cloud-discovery-and-access-using-open-source-technologies",
    "title": "2021 Cloud Workshop at AGU",
    "section": "NASA Earthdata Cloud: Discovery and access using open source technologies",
    "text": "NASA Earthdata Cloud: Discovery and access using open source technologies\nThe following tutorial demonstrates several basic end-to-end workflows to interact with data “in-place” from the NASA Earthdata Cloud, accessing Amazon Web Services (AWS) Single Storage Solution (S3) data locations without the need to download data. While the data can be downloaded locally, the cloud offers the ability to scale compute resources to perform analyses over large areas and time spans, which is critical as data volumes continue to grow.\nAlthough the examples we’re working with in this notebook only focuses on a small time and area for demonstration purposes, this workflow can be modified and scaled up to suit a larger time range and region of interest.\n\nDatasets of interest:\n\nHarmonized Landsat Sentinel-2 (HLS) Operational Land Imager Surface Reflectance and TOA Brightness Daily Global 30m v2.0 (L30) (10.5067/HLS/HLSL30.002)\n\nSurface reflectance (SR) and top of atmosphere (TOA) brightness data\nGlobal observations of the land every 2–3 days at 30-meter (m)\nCloud Optimized GeoTIFF (COG) format\n\nECCO Sea Surface Height - Daily Mean 0.5 Degree (Version 4 Release 4)(10.5067/ECG5D-SSH44).\n\nDaily-averaged dynamic sea surface height\nTime series of monthly NetCDFs on a 0.5-degree latitude/longitude grid."
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#part-1-explore-data-hosted-in-the-earthdata-cloud",
    "href": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#part-1-explore-data-hosted-in-the-earthdata-cloud",
    "title": "2021 Cloud Workshop at AGU",
    "section": "Part 1: Explore Data hosted in the Earthdata Cloud",
    "text": "Part 1: Explore Data hosted in the Earthdata Cloud\n\nEarthdata Search Demonstration\nFrom Earthdata Search https://search.earthdata.nasa.gov, use your Earthdata login credentials to log in. You can create an Earthdata Login account at https://urs.earthdata.nasa.gov.\nIn this example we are interested in the ECCO dataset, hosted by the PO.DAAC. This dataset is available from the NASA Earthdata Cloud archive hosted in AWS cloud.\nClick on the “Available from AWS Cloud” filter option on the left. Here, 39 matching collections were found with the ECCO monthly SSH search, and for the time period for year 2015. The latter can be done using the calendar icon on the left under the search box. Scroll down the list of returned matches until we see the dataset of interest, in this case ECCO Sea Surface Height - Monthly Mean 0.5 Degree (Version 4 Release 4).\n\n\nView and Select Data Access Options\nClicking on the ECCO Sea Surface Height - Monthly Mean 0.5 Degree (Version 4 Release 4) dataset, we now see a list of files (granules) that are part of the dataset (collection). We can click on the green + symbol to add a few files to our project. Here we added the first 3 listed for 2015. Then click on the green button towards the bottom that says “Download”. This will take us to another page with options to customize our download or access link(s).\n\n\n\nFigure caption: Select granules and click download"
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#access-options",
    "href": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#access-options",
    "title": "2021 Cloud Workshop at AGU",
    "section": "Access Options",
    "text": "Access Options\nSelect the “Direct Download” option to view Access options via Direct Download and from the AWS Cloud. Additional options to customize the data are also available for this dataset.\n\n\n\nFigure caption: Customize your download or access"
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#earthdata-cloud-access-information",
    "href": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#earthdata-cloud-access-information",
    "title": "2021 Cloud Workshop at AGU",
    "section": "Earthdata Cloud access information",
    "text": "Earthdata Cloud access information\nClicking the green Download Data button again, will take us to the final page for instructions to download and links for data access in the cloud. The AWS S3 Access tab provides the S3:// links, which is what we would use to access the data directly in-region (us-west-2) within the AWS cloud.\nE.g.: s3://podaac-ops-cumulus-protected/ECCO_L4_SSH_05DEG_MONTHLY_V4R4/SEA_SURFACE_HEIGHT_mon_mean_2015-09_ECCO_V4r4_latlon_0p50deg.nc where s3 indicates data is stored in AWS S3 storage, podaac-ops-cumulus-protected is the bucket, and ECCO_L4_SSH_05DEG_MONTHLY_V4R4 is the object prefix (the latter two are also listed in the dataset collection information under Cloud Access (step 3 above)).\n\nIntegrate file links into programmatic workflow, locally or in the AWS cloud.\nIn the next two examples we will work programmatically in the cloud to access datasets of interest, to get us set up for further scientific analysis of choice. There are several ways to do this. One way to connect the search part of the workflow we just did in Earthdata Search to our next steps working in the cloud is to simply copy/paste the s3:// links provides in Step 4 above into a JupyterHub notebook or script in our cloud workspace, and continue the data analysis from there.\nOne could also copy/paste the s3:// links and save them in a text file, then open and read the text file in the notebook or script in the JupyterHub in the cloud.\n\n\n\nFigure caption: Direct S3 access"
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#part-2-working-with-cloud-optimized-geotiffs-using-nasas-common-metadata-repository-spatio-temporal-assett-catalog-cmr-stac",
    "href": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#part-2-working-with-cloud-optimized-geotiffs-using-nasas-common-metadata-repository-spatio-temporal-assett-catalog-cmr-stac",
    "title": "2021 Cloud Workshop at AGU",
    "section": "Part 2: Working with Cloud-Optimized GeoTIFFs using NASA’s Common Metadata Repository Spatio-Temporal Assett Catalog (CMR-STAC)",
    "text": "Part 2: Working with Cloud-Optimized GeoTIFFs using NASA’s Common Metadata Repository Spatio-Temporal Assett Catalog (CMR-STAC)\nIn this example we will access the NASA’s Harmonized Landsat Sentinel-2 (HLS) version 2 assets, which are archived in cloud optimized geoTIFF (COG) format archived by the Land Processes (LP) DAAC. The COGs can be used like any other GeoTIFF file, but have some added features that make them more efficient within the cloud data access paradigm. These features include: overviews and internal tiling.\n\nBut first, what is STAC?\nSpatioTemporal Asset Catalog (STAC) is a specification that provides a common language for interpreting geospatial information in order to standardize indexing and discovering data.\nThe STAC specification is made up of a collection of related, yet independent specifications that when used together provide search and discovery capabilities for remote assets.\n\nFour STAC Specifications\nSTAC Catalog (aka DAAC Archive)\nSTAC Collection (aka Data Product)\nSTAC Item (aka Granule)\nSTAC API\n\n\n\nCMR-STAC API\nThe CMR-STAC API is NASA’s implementation of the STAC API specification for all NASA data holdings within EOSDIS. The current implementation does not allow for querries accross the entire NASA catalog. Users must execute searches within provider catalogs (e.g., LPCLOUD) to find the STAC Items they are searching for. All the providers can be found at the CMR-STAC endpoint here: https://cmr.earthdata.nasa.gov/stac/.\nIn this example, we will query the LPCLOUD provider to identify STAC Items from the Harmonized Landsat Sentinel-2 (HLS) collection that fall within our region of interest (ROI) and within our specified time range."
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#import-packages",
    "href": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#import-packages",
    "title": "2021 Cloud Workshop at AGU",
    "section": "Import packages",
    "text": "Import packages\n\nimport os\nimport requests \nimport boto3\nfrom osgeo import gdal\nimport rasterio as rio\nfrom rasterio.session import AWSSession\nimport rioxarray\nimport hvplot.xarray\nimport holoviews as hv\n\nfrom pystac_client import Client  \nfrom collections import defaultdict    \nimport json\nimport geopandas\nimport geoviews as gv\nfrom cartopy import crs\ngv.extension('bokeh', 'matplotlib')"
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#connect-to-the-cmr-stac-api",
    "href": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#connect-to-the-cmr-stac-api",
    "title": "2021 Cloud Workshop at AGU",
    "section": "Connect to the CMR-STAC API",
    "text": "Connect to the CMR-STAC API\n\nSTAC_URL = 'https://cmr.earthdata.nasa.gov/stac'\n\n\nprovider_cat = Client.open(STAC_URL)\n\n\nConnect to the LPCLOUD Provider/STAC Catalog\nFor this next step we need the provider title (e.g., LPCLOUD). We will add the provider to the end of the CMR-STAC API URL (i.e., https://cmr.earthdata.nasa.gov/stac/) to connect to the LPCLOUD STAC Catalog.\n\ncatalog = Client.open(f'{STAC_URL}/LPCLOUD/')\n\nSince we are using a dedicated client (i.e., pystac-client.Client) to connect to our STAC Provider Catalog, we will have access to some useful internal methods and functions (e.g., get_children() or get_all_items()) we can use to get information from these objects."
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#search-for-stac-items",
    "href": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#search-for-stac-items",
    "title": "2021 Cloud Workshop at AGU",
    "section": "Search for STAC Items",
    "text": "Search for STAC Items\nWe will define our ROI using a geojson file containing a small polygon feature in western Nebraska, USA. We’ll also specify the data collections and a time range for our example.\n\nRead in a geojson file and plot\nReading in a geojson file with geopandas and extract coodinates for our ROI.\n\nfield = geopandas.read_file('../data/ne_w_agfields.geojson')\nfieldShape = field['geometry'][0]\nroi = json.loads(field.to_json())['features'][0]['geometry']\n\nWe can plot the polygon using the geoviews package that we imported as gv with ‘bokeh’ and ‘matplotlib’ extensions. The following has reasonable width, height, color, and line widths to view our polygon when it is overlayed on a base tile map.\n\nbase = gv.tile_sources.EsriImagery.opts(width=650, height=500)\nfarmField = gv.Polygons(fieldShape).opts(line_color='yellow', line_width=10, color=None)\nbase * farmField\n\nUnable to display output for mime type(s): \n\n\n\n\n\n\n\n\n  \n\n\n\n\nWe will now start to specify the search criteria we are interested in, i.e, the date range, the ROI, and the data collections, that we will pass to the STAC API."
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#search-the-cmr-stac-api-with-our-search-criteria",
    "href": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#search-the-cmr-stac-api-with-our-search-criteria",
    "title": "2021 Cloud Workshop at AGU",
    "section": "Search the CMR-STAC API with our search criteria",
    "text": "Search the CMR-STAC API with our search criteria\nNow we can put all our search criteria together using catalog.search from the pystac_client package. STAC Collection is synonomous with what we usually consider a NASA data product. Desired STAC Collections are submitted to the search API as a list containing the collection id. Let’s focus on S30 and L30 collections.\n\ncollections = ['HLSL30.v2.0', 'HLSS30.v2.0']\n\ndate_range = \"2021-05/2021-08\"\n\nsearch = catalog.search(\n    collections=collections,\n    intersects=roi,\n    datetime=date_range,\n    limit=100\n)\n\n\nView STAC Items that matched our search query\n\nprint('Matching STAC Items:', search.matched())\nitem_collection = search.get_all_items()\nitem_collection[0].to_dict()\n\nMatching STAC Items: 113\n\n\n{'type': 'Feature',\n 'stac_version': '1.0.0',\n 'id': 'HLS.L30.T13TGF.2021124T173013.v2.0',\n 'properties': {'datetime': '2021-05-04T17:30:13.428000Z',\n  'start_datetime': '2021-05-04T17:30:13.428Z',\n  'end_datetime': '2021-05-04T17:30:37.319Z',\n  'eo:cloud_cover': 36},\n 'geometry': {'type': 'Polygon',\n  'coordinates': [[[-101.5423534, 40.5109845],\n    [-101.3056118, 41.2066375],\n    [-101.2894253, 41.4919436],\n    [-102.6032964, 41.5268623],\n    [-102.638891, 40.5386175],\n    [-101.5423534, 40.5109845]]]},\n 'links': [{'rel': 'self',\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSL30.v2.0/items/HLS.L30.T13TGF.2021124T173013.v2.0'},\n  {'rel': 'parent',\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSL30.v2.0'},\n  {'rel': 'collection',\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSL30.v2.0'},\n  {'rel': <RelType.ROOT: 'root'>,\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/',\n   'type': <MediaType.JSON: 'application/json'>,\n   'title': 'LPCLOUD'},\n  {'rel': 'provider', 'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD'},\n  {'rel': 'via',\n   'href': 'https://cmr.earthdata.nasa.gov/search/concepts/G2144020713-LPCLOUD.json'},\n  {'rel': 'via',\n   'href': 'https://cmr.earthdata.nasa.gov/search/concepts/G2144020713-LPCLOUD.umm_json'}],\n 'assets': {'B11': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.B11.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.B11.tif'},\n  'B07': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.B07.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.B07.tif'},\n  'SAA': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.SAA.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.SAA.tif'},\n  'B06': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.B06.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.B06.tif'},\n  'B09': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.B09.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.B09.tif'},\n  'B10': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.B10.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.B10.tif'},\n  'VZA': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.VZA.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.VZA.tif'},\n  'SZA': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.SZA.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.SZA.tif'},\n  'B01': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.B01.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.B01.tif'},\n  'VAA': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.VAA.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.VAA.tif'},\n  'B05': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.B05.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.B05.tif'},\n  'B02': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.B02.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.B02.tif'},\n  'Fmask': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.Fmask.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.Fmask.tif'},\n  'B03': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.B03.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.B03.tif'},\n  'B04': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.B04.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.B04.tif'},\n  'browse': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-public/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.jpg',\n   'type': 'image/jpeg',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.jpg'},\n  'metadata': {'href': 'https://cmr.earthdata.nasa.gov/search/concepts/G2144020713-LPCLOUD.xml',\n   'type': 'application/xml'}},\n 'bbox': [-102.638891, 40.510984, -101.289425, 41.526862],\n 'stac_extensions': ['https://stac-extensions.github.io/eo/v1.0.0/schema.json'],\n 'collection': 'HLSL30.v2.0'}"
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#filtering-stac-items",
    "href": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#filtering-stac-items",
    "title": "2021 Cloud Workshop at AGU",
    "section": "Filtering STAC Items",
    "text": "Filtering STAC Items\nBelow we will loop through and filter the item_collection by a specified cloud cover as well as extract the band we’d need to do an Enhanced Vegetation Index (EVI) calculation for a future analysis. We will also specify the STAC Assets (i.e., bands/layers) of interest for both the S30 and L30 collections (also in our collections variable above) and print out the first ten links, converted to s3 locations:\n\ncloudcover = 25\n\ns30_bands = ['B8A', 'B04', 'B02', 'Fmask']    # S30 bands for EVI calculation and quality filtering -> NIR, RED, BLUE, Quality \nl30_bands = ['B05', 'B04', 'B02', 'Fmask']    # L30 bands for EVI calculation and quality filtering -> NIR, RED, BLUE, Quality \n\nevi_band_links = []\n\nfor i in item_collection:\n    if i.properties['eo:cloud_cover'] <= cloudcover:\n        if i.collection_id == 'HLSS30.v2.0':\n            #print(i.properties['eo:cloud_cover'])\n            evi_bands = s30_bands\n        elif i.collection_id == 'HLSL30.v2.0':\n            #print(i.properties['eo:cloud_cover'])\n            evi_bands = l30_bands\n\n        for a in i.assets:\n            if any(b==a for b in evi_bands):\n                evi_band_links.append(i.assets[a].href)\n                \ns3_links = [l.replace('https://data.lpdaac.earthdatacloud.nasa.gov/', 's3://') for l in evi_band_links]\ns3_links[:10]\n\n['s3://lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021133T172406.v2.0/HLS.L30.T13TGF.2021133T172406.v2.0.B04.tif',\n 's3://lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021133T172406.v2.0/HLS.L30.T13TGF.2021133T172406.v2.0.B05.tif',\n 's3://lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021133T172406.v2.0/HLS.L30.T13TGF.2021133T172406.v2.0.Fmask.tif',\n 's3://lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021133T172406.v2.0/HLS.L30.T13TGF.2021133T172406.v2.0.B02.tif',\n 's3://lp-prod-protected/HLSL30.020/HLS.L30.T14TKL.2021133T172406.v2.0/HLS.L30.T14TKL.2021133T172406.v2.0.B02.tif',\n 's3://lp-prod-protected/HLSL30.020/HLS.L30.T14TKL.2021133T172406.v2.0/HLS.L30.T14TKL.2021133T172406.v2.0.B04.tif',\n 's3://lp-prod-protected/HLSL30.020/HLS.L30.T14TKL.2021133T172406.v2.0/HLS.L30.T14TKL.2021133T172406.v2.0.B05.tif',\n 's3://lp-prod-protected/HLSL30.020/HLS.L30.T14TKL.2021133T172406.v2.0/HLS.L30.T14TKL.2021133T172406.v2.0.Fmask.tif',\n 's3://lp-prod-protected/HLSS30.020/HLS.S30.T14TKL.2021133T173859.v2.0/HLS.S30.T14TKL.2021133T173859.v2.0.B04.tif',\n 's3://lp-prod-protected/HLSS30.020/HLS.S30.T14TKL.2021133T173859.v2.0/HLS.S30.T14TKL.2021133T173859.v2.0.B8A.tif']"
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#access-s3-storage-location",
    "href": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#access-s3-storage-location",
    "title": "2021 Cloud Workshop at AGU",
    "section": "Access s3 storage location",
    "text": "Access s3 storage location\nAccess s3 credentials from LP.DAAC and create a boto3 Session object using your temporary credentials. This Session is used to pass credentials and configuration to AWS so we can interact wit S3 objects from applicable buckets.\n\ns3_cred_endpoint = 'https://data.lpdaac.earthdatacloud.nasa.gov/s3credentials'\ntemp_creds_req = requests.get(s3_cred_endpoint).json()\n\nsession = boto3.Session(aws_access_key_id=temp_creds_req['accessKeyId'], \n                        aws_secret_access_key=temp_creds_req['secretAccessKey'],\n                        aws_session_token=temp_creds_req['sessionToken'],\n                        region_name='us-west-2')\n\nGDAL Configurations\nGDAL is a foundational piece of geospatial software that is leveraged by several popular open-source, and closed, geospatial software. The rasterio package is no exception. Rasterio leverages GDAL to, among other things, read and write raster data files, e.g., GeoTIFFs/Cloud Optimized GeoTIFFs. To read remote files, i.e., files/objects stored in the cloud, GDAL uses its Virtual File System API. In a perfect world, one would be able to point a Virtual File System (there are several) at a remote data asset and have the asset retrieved, but that is not always the case. GDAL has a host of configurations/environmental variables that adjust its behavior to, for example, make a request more performant or to pass AWS credentials to the distribution system. Below, we’ll identify the evironmental variables that will help us get our data from cloud\n\nrio_env = rio.Env(AWSSession(session),\n                  GDAL_DISABLE_READDIR_ON_OPEN='TRUE',\n                  GDAL_HTTP_COOKIEFILE=os.path.expanduser('~/cookies.txt'),\n                  GDAL_HTTP_COOKIEJAR=os.path.expanduser('~/cookies.txt'))\nrio_env.__enter__()\n\n<rasterio.env.Env at 0x7f64510812e0>\n\n\n\ns3_url = 's3://lp-prod-protected/HLSL30.020/HLS.L30.T11SQA.2021333T181532.v2.0/HLS.L30.T11SQA.2021333T181532.v2.0.B04.tif'\n# s3_url = 's3://lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021133T172406.v2.0/HLS.L30.T13TGF.2021133T172406.v2.0.B04.tif'"
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#read-cloud-optimized-geotiff-into-rioxarray",
    "href": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#read-cloud-optimized-geotiff-into-rioxarray",
    "title": "2021 Cloud Workshop at AGU",
    "section": "Read Cloud-Optimized GeoTIFF into rioxarray",
    "text": "Read Cloud-Optimized GeoTIFF into rioxarray\n\nda = rioxarray.open_rasterio(s3_url)\nda\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray (band: 1, y: 3660, x: 3660)>\n[13395600 values with dtype=int16]\nCoordinates:\n  * band         (band) int64 1\n  * x            (x) float64 7e+05 7e+05 7e+05 ... 8.097e+05 8.097e+05 8.097e+05\n  * y            (y) float64 4.1e+06 4.1e+06 4.1e+06 ... 3.99e+06 3.99e+06\n    spatial_ref  int64 0\nAttributes:\n    _FillValue:    -9999.0\n    scale_factor:  0.0001\n    add_offset:    0.0\n    long_name:     Redxarray.DataArrayband: 1y: 3660x: 3660...[13395600 values with dtype=int16]Coordinates: (4)band(band)int641array([1])x(x)float647e+05 7e+05 ... 8.097e+05 8.097e+05array([699975., 700005., 700035., ..., 809685., 809715., 809745.])y(y)float644.1e+06 4.1e+06 ... 3.99e+06array([4100025., 4099995., 4099965., ..., 3990315., 3990285., 3990255.])spatial_ref()int640crs_wkt :PROJCS[\"UTM Zone 11, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :Unknown datum based upon the WGS 84 ellipsoidhorizontal_datum_name :Not_specified_based_on_WGS_84_spheroidprojected_crs_name :UTM Zone 11, Northern Hemispheregrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-117.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"UTM Zone 11, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]GeoTransform :699960.0 30.0 0.0 4100040.0 0.0 -30.0array(0)Attributes: (4)_FillValue :-9999.0scale_factor :0.0001add_offset :0.0long_name :Red\n\n\nWhen GeoTIFFS/Cloud Optimized GeoTIFFS are read in, a band coordinate variable is automatically created (see the print out above). In this exercise we will not use that coordinate variable, so we will remove it using the squeeze() function to avoid confusion.\n\nda_red = da.squeeze('band', drop=True)\nda_red\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray (y: 3660, x: 3660)>\n[13395600 values with dtype=int16]\nCoordinates:\n  * x            (x) float64 7e+05 7e+05 7e+05 ... 8.097e+05 8.097e+05 8.097e+05\n  * y            (y) float64 4.1e+06 4.1e+06 4.1e+06 ... 3.99e+06 3.99e+06\n    spatial_ref  int64 0\nAttributes:\n    _FillValue:    -9999.0\n    scale_factor:  0.0001\n    add_offset:    0.0\n    long_name:     Redxarray.DataArrayy: 3660x: 3660...[13395600 values with dtype=int16]Coordinates: (3)x(x)float647e+05 7e+05 ... 8.097e+05 8.097e+05array([699975., 700005., 700035., ..., 809685., 809715., 809745.])y(y)float644.1e+06 4.1e+06 ... 3.99e+06array([4100025., 4099995., 4099965., ..., 3990315., 3990285., 3990255.])spatial_ref()int640crs_wkt :PROJCS[\"UTM Zone 11, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :Unknown datum based upon the WGS 84 ellipsoidhorizontal_datum_name :Not_specified_based_on_WGS_84_spheroidprojected_crs_name :UTM Zone 11, Northern Hemispheregrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-117.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"UTM Zone 11, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]GeoTransform :699960.0 30.0 0.0 4100040.0 0.0 -30.0array(0)Attributes: (4)_FillValue :-9999.0scale_factor :0.0001add_offset :0.0long_name :Red"
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#plot-using-hvplot",
    "href": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#plot-using-hvplot",
    "title": "2021 Cloud Workshop at AGU",
    "section": "Plot using hvplot",
    "text": "Plot using hvplot\n\nda_red.hvplot.image(x='x', y='y', cmap='gray', aspect='equal')\n\nUnable to display output for mime type(s): \n\n\n\n\n\n\n\n\n  \n\n\n\n\n\nrio_env.__exit__()"
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#part-3-working-with-zarr-formatted-data-using-nasas-harmony-cloud-transformation-service",
    "href": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#part-3-working-with-zarr-formatted-data-using-nasas-harmony-cloud-transformation-service",
    "title": "2021 Cloud Workshop at AGU",
    "section": "Part 3: Working with Zarr-formatted data using NASA’s Harmony cloud transformation service",
    "text": "Part 3: Working with Zarr-formatted data using NASA’s Harmony cloud transformation service\nWe have already explored direct access to the NASA EOSDIS archive in the cloud via the Amazon Simple Storage Service (S3). In addition to directly accessing the files archived and distributed by each of the NASA DAACs, many datasets also support services that allow us to customize the data via subsetting, reformatting, reprojection, and other transformations.\nThis example demonstrates “analysis in place” using customized ECCO Level 4 monthly sea surface height data, in this case reformatted to Zarr, from a new ecosystem of services operating within the NASA Earthdata Cloud: NASA Harmony:\n\nConsistent access patterns to EOSDIS holdings make cross-data center data access easier\nData reduction services allow us to request only the data we want, in the format and projection we want\nAnalysis Ready Data and cloud access will help reduce time-to-science\nCommunity Development helps reduce the barriers for re-use of code and sharing of domain knowledge"
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#import-packages-1",
    "href": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#import-packages-1",
    "title": "2021 Cloud Workshop at AGU",
    "section": "Import packages",
    "text": "Import packages\n\nfrom harmony import BBox, Client, Collection, Request, LinkType\nfrom harmony.config import Environment\nfrom pprint import pprint\nimport datetime as dt\nimport s3fs\nfrom pqdm.threads import pqdm\nimport xarray as xr"
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#using-harmony-py-to-customize-data",
    "href": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#using-harmony-py-to-customize-data",
    "title": "2021 Cloud Workshop at AGU",
    "section": "Using Harmony-Py to customize data",
    "text": "Using Harmony-Py to customize data\nHarmony-Py provides a pip installable Python alternative to directly using Harmony’s RESTful API to make it easier to request data and service options, especially when interacting within a Python Jupyter Notebook environment.\n\nCreate Harmony Client object\nFirst, we need to create a Harmony Client, which is what we will interact with to submit and inspect a data request to Harmony, as well as to retrieve results.\n\nharmony_client = Client()"
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#create-harmony-request",
    "href": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#create-harmony-request",
    "title": "2021 Cloud Workshop at AGU",
    "section": "Create Harmony Request",
    "text": "Create Harmony Request\nSpecify a temporal range over 2015, and Zarr as an output format.\nZarr is an open source library for storing N-dimensional array data. It supports multidimensional arrays with attributes and dimensions similar to NetCDF4, and it can be read by XArray. Zarr is often used for data held in cloud object storage (like Amazon S3), because it is better optimized for these situations than NetCDF4.\n\nshort_name = 'ECCO_L4_SSH_05DEG_MONTHLY_V4R4'\n\nrequest = Request(\n    collection=Collection(id=short_name),\n    temporal={\n        'start': dt.datetime(2015, 1, 2),\n        'stop': dt.datetime(2015, 12, 31),\n    },\n    format='application/x-zarr'\n)\n\njob_id = harmony_client.submit(request)"
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#check-request-status-and-view-output-urls",
    "href": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#check-request-status-and-view-output-urls",
    "title": "2021 Cloud Workshop at AGU",
    "section": "Check request status and view output URLs",
    "text": "Check request status and view output URLs\nHarmony data outputs can be accessed within the cloud using the s3 URLs and AWS credentials provided in the Harmony job response:\n\nharmony_client.wait_for_processing(job_id, show_progress=True)\n\nresults = harmony_client.result_urls(job_id, link_type=LinkType.s3)\ns3_urls = list(results)\ns3_urls\n\n [ Processing:  83% ] |##########################################         | [/]\n\n\n\nAWS credential retrieval\nUsing aws_credentials you can retrieve the credentials needed to access the Harmony s3 staging bucket and its contents.\n\ncreds = harmony_client.aws_credentials()"
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#open-staged-files-with-s3fs-and-xarray",
    "href": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#open-staged-files-with-s3fs-and-xarray",
    "title": "2021 Cloud Workshop at AGU",
    "section": "Open staged files with s3fs and xarray",
    "text": "Open staged files with s3fs and xarray\nAccess AWS credentials for the Harmony bucket, and use the AWS s3fs package to create a file system that can then be read by xarray. Below we create session by passing in the temporary credentials we recieved from our temporary credentials endpoint.\n\ncreds = harmony_client.aws_credentials()\n\ns3_fs = s3fs.S3FileSystem(\n    key=creds['aws_access_key_id'],\n    secret=creds['aws_secret_access_key'],\n    token=creds['aws_session_token'],\n    client_kwargs={'region_name':'us-west-2'},\n)\n\nOpen the Zarr stores using the s3fs package, then load them all at once into a concatenated xarray dataset:\n\nstores = [s3fs.S3Map(root=url, s3=s3_fs, check=False) for url in s3_urls]\ndef open_zarr_xarray(store):\n    return xr.open_zarr(store=store, consolidated=True)\n\ndatasets = pqdm(stores, open_zarr_xarray, n_jobs=12)\n\nds = xr.concat(datasets, 'time', coords='minimal', )\nds = xr.decode_cf(ds, mask_and_scale=True, decode_coords=True)\nds\n\n\nssh_da = ds.SSH\n\nssh_da.to_masked_array(copy=False)\n\nssh_da"
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#plot-the-sea-surface-height-time-series-using-hvplot",
    "href": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#plot-the-sea-surface-height-time-series-using-hvplot",
    "title": "2021 Cloud Workshop at AGU",
    "section": "Plot the Sea Surface Height time series using hvplot",
    "text": "Plot the Sea Surface Height time series using hvplot\nNow we can start looking at aggregations across the time dimension. In this case, plot the standard deviation of the temperature at each point to get a visual sense of how much temperatures fluctuate over the course of the month.\n\nssh_da = ds.SSH\n\nstdev_ssh = ssh_da.std('time')\nstdev_ssh.name = 'stdev of analysed_sst [Kelvin]'\nstdev_ssh.plot();\n\nssh_da.hvplot.image(x='longitude', y='latitude', cmap='Spectral_r', aspect='equal').opts(clim=(ssh_da.attrs['valid_min'],ssh_da.attrs['valid_max']))"
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#further-resources",
    "href": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html#further-resources",
    "title": "2021 Cloud Workshop at AGU",
    "section": "Further Resources",
    "text": "Further Resources\n\nReference Hackathon/workshop tutorials that go into more detail!\nEarthdata Cloud Cookbook\nEarthdata Cloud Primer\n\nGetting started with Amazon Web Services outside of the Workshop to access and work with data with a cloud environment."
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Data_Access_OPeNDAP_Example.html#import-packages",
    "href": "how-tos/Earthdata_Cloud__Data_Access_OPeNDAP_Example.html#import-packages",
    "title": "2021 Cloud Workshop at AGU",
    "section": "Import Packages",
    "text": "Import Packages\n\nimport xarray as xr\nimport dask\nimport hvplot.xarray"
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Data_Access_OPeNDAP_Example.html#access-on-prem-opendap-hyrax-server---open",
    "href": "how-tos/Earthdata_Cloud__Data_Access_OPeNDAP_Example.html#access-on-prem-opendap-hyrax-server---open",
    "title": "2021 Cloud Workshop at AGU",
    "section": "Access On-prem OPeNDAP (Hyrax Server) - Open",
    "text": "Access On-prem OPeNDAP (Hyrax Server) - Open\n\nopd_sst_url = 'https://podaac-opendap.jpl.nasa.gov/opendap/allData/ghrsst/data/GDS2/L4/GLOB/NCEI/AVHRR_OI/v2/1981/244/19810901120000-NCEI-L4_GHRSST-SSTblend-AVHRR_OI-GLOB-v02.0-fv02.0.nc'\n\n\nopd_sst_ds = xr.open_dataset(opd_sst_url)\nopd_sst_ds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:           (lat: 720, lon: 1440, time: 1, nv: 2)\nCoordinates:\n  * lat               (lat) float32 -89.88 -89.62 -89.38 ... 89.38 89.62 89.88\n  * lon               (lon) float32 -179.9 -179.6 -179.4 ... 179.4 179.6 179.9\n  * time              (time) datetime64[ns] 1981-09-01\nDimensions without coordinates: nv\nData variables:\n    lat_bnds          (lat, nv) float32 -90.0 -89.75 -89.75 ... 89.75 89.75 90.0\n    lon_bnds          (lon, nv) float32 -180.0 -179.8 -179.8 ... 179.8 180.0\n    time_bnds         (time, nv) datetime64[ns] 1981-09-01 1981-09-02\n    analysed_sst      (time, lat, lon) float32 ...\n    analysis_error    (time, lat, lon) float32 ...\n    mask              (time, lat, lon) float32 ...\n    sea_ice_fraction  (time, lat, lon) float32 ...\nAttributes: (12/48)\n    product_version:                 Version 2.0\n    spatial_resolution:              0.25 degree\n    Conventions:                     CF-1.6,ACDD-1.3\n    title:                           NCEI global 0.25 deg daily sea surface t...\n    references:                      Reynolds, et al.(2009) What is New in Ve...\n    institution:                     NCEI\n    ...                              ...\n    source:                          AVHRR_Pathfinder-NODC-L3C-v5.1,ICOADS_SH...\n    summary:                         NOAA's 1/4-degree Daily Optimum Interpol...\n    time_coverage_start:             19810901T000000Z\n    time_coverage_end:               19810902T000000Z\n    uuid:                            39832cc3-d409-438a-820e-2bb1b38ebca8\n    DODS_EXTRA.Unlimited_Dimension:  timexarray.DatasetDimensions:lat: 720lon: 1440time: 1nv: 2Coordinates: (3)lat(lat)float32-89.88 -89.62 ... 89.62 89.88long_name :latitudestandard_name :latitudeaxis :Yunits :degrees_northcomment :Uniform grid with centers from -89.875 to 89.875 by 0.25 degrees.bounds :lat_bndsvalid_max :90.0valid_min :-90.0array([-89.875, -89.625, -89.375, ...,  89.375,  89.625,  89.875],\n      dtype=float32)lon(lon)float32-179.9 -179.6 ... 179.6 179.9long_name :longitudestandard_name :longitudeaxis :Xunits :degrees_eastcomment :Uniform grid with centers from -179.875 to 179.875 by 0.25 degrees.bounds :lon_bndsvalid_max :180.0valid_min :-180.0array([-179.875, -179.625, -179.375, ...,  179.375,  179.625,  179.875],\n      dtype=float32)time(time)datetime64[ns]1981-09-01long_name :reference time of sst fieldstandard_name :timeaxis :Tbounds :time_bndscomment :Nominal time because observations are from different sources and are made at different times of the day.array(['1981-09-01T00:00:00.000000000'], dtype='datetime64[ns]')Data variables: (7)lat_bnds(lat, nv)float32...comment :This variable defines the latitude values at the north and south bounds of every 0.25-degree pixel.array([[-90.  , -89.75],\n       [-89.75, -89.5 ],\n       [-89.5 , -89.25],\n       ...,\n       [ 89.25,  89.5 ],\n       [ 89.5 ,  89.75],\n       [ 89.75,  90.  ]], dtype=float32)lon_bnds(lon, nv)float32...comment :This variable defines the longitude values at the west and east bounds of every 0.25-degree pixel.array([[-180.  , -179.75],\n       [-179.75, -179.5 ],\n       [-179.5 , -179.25],\n       ...,\n       [ 179.25,  179.5 ],\n       [ 179.5 ,  179.75],\n       [ 179.75,  180.  ]], dtype=float32)time_bnds(time, nv)datetime64[ns]...comment :This variable defines the start and end of the time span for the data.array([['1981-09-01T00:00:00.000000000', '1981-09-02T00:00:00.000000000']],\n      dtype='datetime64[ns]')analysed_sst(time, lat, lon)float32...long_name :analysed sea surface temperaturestandard_name :sea_surface_temperatureunits :kelvinvalid_min :-300valid_max :4500comment :Single-sensor Pathfinder 5.0/5.1 AVHRR SSTs used until 2005; two AVHRRs at a time are used 2007 onward. Sea ice and in-situ data used also are 'near real time' quality for recent period.  SST (bulk) is at ambiguous depth because multiple types of observations are used.source :AVHRR_Pathfinder-NODC-L3C-v5.1,ICOADS_SHIP-NCAR-IN_SITU-v2.4,ICOADS_BUOY-NCAR-IN_SITU-v2.4,GSFC_25KM-NSIDC-ICE[1036800 values with dtype=float32]analysis_error(time, lat, lon)float32...long_name :estimated error standard deviation of analysed_sstunits :kelvinvalid_min :0valid_max :32767comment :Sum of bias, sampling and random errors.[1036800 values with dtype=float32]mask(time, lat, lon)float32...long_name :sea/land field composite maskflag_meanings :water landcomment :Binary mask distinguishing water and land only.flag_masks :[1 2]source :RWReynolds_landmask_V1.0valid_max :2valid_min :1[1036800 values with dtype=float32]sea_ice_fraction(time, lat, lon)float32...long_name :sea ice area fractionvalid_min :0valid_max :100standard_name :sea_ice_area_fractionunits :1comment :7-day median filtered .  Switch from 25 km NASA team ice (http://nsidc.org/data/nsidc-0051.html)  to 50 km NCEP ice (http://polar.ncep.noaa.gov/seaice) after 2004 results in artificial increase in ice coverage.source :GSFC_25KM-NSIDC-ICE[1036800 values with dtype=float32]Attributes: (48)product_version :Version 2.0spatial_resolution :0.25 degreeConventions :CF-1.6,ACDD-1.3title :NCEI global 0.25 deg daily sea surface temperature analysis based mainly on Advanced Very High Resolution Radiometer, finalreferences :Reynolds, et al.(2009) What is New in Version 2. Available at http://www.ncdc.noaa.gov/sites/default/files/attachments/Reynolds2009_oisst_daily_v02r00_version2-features.pdf; Daily 1/4 Degree Optimum Interpolation Sea Surface Temperature (OISST)- Climate Algorithm Theoretical Theoretical Basis Document, NOAA Climate Data Record Program CDRP-ATBD-0303 Rev. 2 (2013). Available at http://www1.ncdc.noaa.gov/pub/data/sds/cdr/CDRs/Sea_Surface_Temperature_Optimum_Interpolation/AlgorithmDescription.pdf.institution :NCEInetcdf_version_id :4.3.2history :2015-11-02T19:52:40Z: Modified format and attributes with NCO to match the GDS 2.0 rev 5 specification.start_time :19810901T000000Zstop_time :19810902T000000Zwesternmost_longitude :-180.0easternmost_longitude :180.0southernmost_latitude :-90.0northernmost_latitude :90.0comment :The daily OISST version 2.0 data contained in this file are the same as those in the equivalent GDS 1.0 file.Metadata_Conventions :ACDD-1.3acknowledgment :This project was supported in part by a grant from the NOAA Climate Data Record (CDR) Program. Cite this dataset when used as a source. The recommended citation and DOI depends on the data center from which the files were acquired. For data accessed from NOAA in near real-time or from the GHRSST LTSRF, cite as: Richard W. Reynolds, Viva F. Banzon, and NOAA CDR Program (2008): NOAA Optimum Interpolation 1/4 Degree Daily Sea Surface Temperature (OISST) Analysis, Version 2. [indicate subset used]. NOAA National Centers for Environmental Information. http://doi.org/doi:10.7289/V5SQ8XB5 [access date]. For data accessed from the NASA PO.DAAC, cite as: Richard W. Reynolds, Viva F. Banzon, and NOAA CDR Program (2008): NOAA Optimum Interpolation 1/4 Degree Daily Sea Surface Temperature (OISST) Analysis, Version 2. [indicate subset used]. PO.DAAC, CA, USA. http://doi.org/10.5067/GHAAO-4BC01 [access date].cdm_data_type :Gridcreator_name :Viva Banzoncreator_email :viva.banzon@noaa.govcreator_url :http://www.ncdc.noaa.govdate_created :20091203T000000Zfile_quality_level :3gds_version_id :2.0r5geospatial_lat_resolution :0.25geospatial_lat_units :degrees_northgeospatial_lon_resolution :0.25geospatial_lon_units :degrees_eastid :NCEI-L4LRblend-GLOB-AVHRR_OIkeywords :Oceans>Ocean Temperature>Sea Surface Temperaturekeywords_vocabulary :NASA Global Change Master Directory (GCMD) Science Keywords, Version 8.1license :No constraints on data access or use.metadata_link :http://doi.org/10.7289/V5SQ8XB5naming_authority :org.ghrsstplatform :NOAA-7processing_level :L4project :Group for High Resolution Sea Surface Temperaturepublisher_email :oisst_contacts@noaa.govpublisher_name :OISST Operations Teampublisher_url :http://www.ncdc.noaa.gov/sstsensor :AVHRR_GACstandard_name_vocabulary :CF Standard Name Table v29source :AVHRR_Pathfinder-NODC-L3C-v5.1,ICOADS_SHIP-NCAR-IN_SITU-v2.4,ICOADS_BUOY-NCAR-IN_SITU-v2.4,GSFC_25KM-NSIDC-ICEsummary :NOAA's 1/4-degree Daily Optimum Interpolation Sea Surface Temperature (OISST) (sometimes referred to as Reynold's SST, which however also refers to earlier products at different resolution), currently available as version 2,  is created by interpolating and extrapolating SST observations from different sources, resulting in a smoothed complete field. The sources of data are satellite (AVHRR) and in situ platforms (i.e., ships and buoys), and the specific datasets employed may change over. At the marginal ice zone, sea ice concentrations are used to generate proxy SSTs.  A preliminary version of this file is produced in near-real time (1-day latency), and then replaced with a final version after 2 weeks. Note that this is the AVHRR-ONLY DOISST, available from Oct 1981, but there is a companion DOISST product that includes microwave satellite data, available from June 2002.time_coverage_start :19810901T000000Ztime_coverage_end :19810902T000000Zuuid :39832cc3-d409-438a-820e-2bb1b38ebca8DODS_EXTRA.Unlimited_Dimension :time\n\n\n\nopd_sst_ds.analysed_sst.isel(time=0).hvplot.image(cmap='Inferno')\n\nUnable to display output for mime type(s):"
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Data_Access_OPeNDAP_Example.html#access-on-prem-opendap-hyrax-server---authentication",
    "href": "how-tos/Earthdata_Cloud__Data_Access_OPeNDAP_Example.html#access-on-prem-opendap-hyrax-server---authentication",
    "title": "2021 Cloud Workshop at AGU",
    "section": "Access On-prem OPeNDAP (Hyrax Server) - Authentication",
    "text": "Access On-prem OPeNDAP (Hyrax Server) - Authentication\n\nimport opendap_auth\n\n\nopendap_auth.create_dodsrc()\n\n'.dodsrc file created: /home/jovyan/.dodsrc'\n\n\nIntegrated Multi-satellitE Retrievals for GPM (IMERG) Level 3 IMERG Final Daily 10 x 10 km (GPM_3IMERGDF)\n\nopd_prec_url = 'https://gpm1.gesdisc.eosdis.nasa.gov/opendap/GPM_L3/GPM_3IMERGDF.06/2021/07/3B-DAY.MS.MRG.3IMERG.20210704-S000000-E235959.V06.nc4' \n\n\nopd_prec_ds = xr.open_dataset(opd_prec_url)\nopd_prec_ds\n\nsyntax error, unexpected WORD_WORD, expecting SCAN_ATTR or SCAN_DATASET or SCAN_ERROR\ncontext: HTTP^ Basic: Access denied.\nsyntax error, unexpected WORD_WORD, expecting SCAN_ATTR or SCAN_DATASET or SCAN_ERROR\ncontext: HTTP^ Basic: Access denied.\nsyntax error, unexpected WORD_WORD, expecting SCAN_ATTR or SCAN_DATASET or SCAN_ERROR\ncontext: HTTP^ Basic: Access denied.\nsyntax error, unexpected WORD_WORD, expecting SCAN_ATTR or SCAN_DATASET or SCAN_ERROR\ncontext: HTTP^ Basic: Access denied.\n\n\nKeyboardInterrupt: \n\n\n\nopd_prec_ds.precipitationCal.isel(time=0).hvplot.image(cmap='rainbow')"
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Data_Access_OPeNDAP_Example.html#access-earthdata-cloud-opendap-hyrax-server---authentication",
    "href": "how-tos/Earthdata_Cloud__Data_Access_OPeNDAP_Example.html#access-earthdata-cloud-opendap-hyrax-server---authentication",
    "title": "2021 Cloud Workshop at AGU",
    "section": "Access Earthdata cloud OPeNDAP (Hyrax Server) - Authentication",
    "text": "Access Earthdata cloud OPeNDAP (Hyrax Server) - Authentication\n\nedc_odp_ssh_url = 'https://opendap.earthdata.nasa.gov/providers/POCLOUD/collections/ECCO%20Sea%20Surface%20Height%20-%20Daily%20Mean%200.5%20Degree%20(Version%204%20Release%204)/granules/SEA_SURFACE_HEIGHT_day_mean_1992-01-01_ECCO_V4r4_latlon_0p50deg.dap.nc'\n\n\nedc_odp_ssh_ds = xr.open_dataset(edc_odp_ssh_url)\nedc_odp_ssh_ds\n\n\nurl = 'https://opendap.earthdata.nasa.gov/providers/POCLOUD/collections/GHRSST%20Level%204%20MUR%20Global%20Foundation%20Sea%20Surface%20Temperature%20Analysis%20(v4.1)/granules/20190201090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.dap.nc4'\n\n\nxr.open_dataset(url)\n\n\nurl = 'https://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_001_20210713T162644_20210713T182234_F02.nc4'\n\n\nxr.open_dataset(url)\n\n\nurl = 'https://opendap.earthdata.nasa.gov/providers/POCLOUD/collections/ECCO%20Sea%20Surface%20Height%20-%20Daily%20Mean%200.5%20Degree%20(Version%204%20Release%204)/granules/SEA_SURFACE_HEIGHT_day_mean_1992-01-01_ECCO_V4r4_latlon_0p50deg.dap.nc4'\n\n\nxr.open_dataset(url)"
  },
  {
    "objectID": "external/cof-zarr-reformat.html#getting-started",
    "href": "external/cof-zarr-reformat.html#getting-started",
    "title": "2021 Cloud Workshop at AGU",
    "section": "Getting Started",
    "text": "Getting Started\nWe will access monthly ocean bottom pressure (OBP) data from ECCO V4r4 (10.5067/ECG5M-OBP44), which are provided as a monthly time series on a 0.5-degree latitude/longitude grid.\nThe data are archived in netCDF format. However, this notebook demonstration will request conversion to Zarr format for files covering the period between 2010 and 2018. Upon receiving our request, Harmony’s backend will convert the files and stage them in S3 for native access in AWS (us-west-2 region, specifically). We will access the new Zarr datasets as an aggregated dataset using xarray, and leverage the S3 native protocols for direct access to the data in an efficient manner.\n\n\nRequirements\n\nAWS\nThis notebook should be running in an EC2 instance in AWS region us-west-2, as previously mentioned. We recommend using an EC2 with at least 8GB of memory available.\nThe notebook was developed and tested using a t2.large instance (2 cpus; 8GB memory).\n\n\nPython 3\nMost of these imports are from the Python standard library. However, you will need to install these packages into your Python 3 environment if you have not already done so:\n\ns3fs\nrequests\npandas\nxarray\nmatplotlib\n\n\n\n\nRequirements\n\nimport matplotlib.pyplot as plt\nimport xarray as xr\nimport pandas as pd\nimport numpy as np\nimport requests\nimport json\nimport time\nimport s3fs\n\nShortName = \"ECCO_L4_OBP_05DEG_MONTHLY_V4R4\"\n\n\n\nStudy period\nSet some “master” inputs to define the time and place contexts for our case studies in the ipynb. This example will be requesting time subsets and receiving global data back from the Harmony API.\n\nstart_date = \"2010-01-01\"\nend_date   = \"2018-12-31\"\n\n\n\nData Access\nSome features in the Harmony API require us to identify the target dataset/collection by its concept-id (which uniquely idenfifies it among the other datasets in the Common Metadata Repository). Support for selection by the dataset ShortName will be added in a future release.\n\nCommon Metadata Repository (CMR)\nFor now, we will need to get the concept-id that corresponds to our dataset by accessing its metadata from the CMR. Read more about the CMR at: https://cmr.earthdata.nasa.gov/\nRequest the UMM Collection metadata (i.e. metadata about the dataset) from the CMR and select the concept-id as a new variable ccid.\n\nresponse = requests.get(\n    url='https://cmr.earthdata.nasa.gov/search/collections.umm_json', \n    params={'provider': \"POCLOUD\",\n            'ShortName': ShortName,\n            'page_size': 1}\n)\n\nummc = response.json()['items'][0]\n\nccid = ummc['meta']['concept-id']\n\nccid\n\n'C1990404791-POCLOUD'\n\n\n\n\nHarmony API\nAnd get the Harmony API endpoint and zarr parameter like we did for SMAP before:\n\nbase = f\"https://harmony.earthdata.nasa.gov/{ccid}\"\nhreq = f\"{base}/ogc-api-coverages/1.0.0/collections/all/coverage/rangeset\"\nrurl = f\"{hreq}?format=application/x-zarr\"\n\nprint(rurl)\n\nhttps://harmony.earthdata.nasa.gov/C1990404791-POCLOUD/ogc-api-coverages/1.0.0/collections/all/coverage/rangeset?format=application/x-zarr\n\n\nECCO monthly collections have 312 granules in V4r4 (you can confirm with the granule listing from CMR Search API) so we can get the entire time series for 2010 to 2018 with one request to the Harmony API.\nFormat a string of query parameters to limit the processing to the desired time period. Then, append the string of time subset parameters to the variable rurl.\n\nsubs = '&'.join([f'subset=time(\"{start_date}T00:00:00.000Z\":\"{end_date}T23:59:59.999Z\")'])\n\nrurl = f\"{rurl}&{subs}\"\n\nprint(rurl)\n\nhttps://harmony.earthdata.nasa.gov/C1990404791-POCLOUD/ogc-api-coverages/1.0.0/collections/all/coverage/rangeset?format=application/x-zarr&subset=time(\"2010-01-01T00:00:00.000Z\":\"2018-12-31T23:59:59.999Z\")\n\n\nSubmit the request and monitor the processing status in a while loop, breaking it on completion of the request job:\n\nresponse = requests.get(url=rurl).json()\n\n# Monitor status in a while loop. Wait 10 seconds for each check.\nwait = 10\nwhile True:\n    response = requests.get(url=response['links'][0]['href']).json()\n    if response['status']!='running':\n        break\n    print(f\"Job in progress ({response['progress']}%)\")\n    time.sleep(wait)\n\nprint(\"DONE!\")\n\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nDONE!\n\n\nAccess the staged cloud datasets over native AWS interfaces\nCheck the message field in the response for clues about how to proceed:\n\nprint(response['message'])\n\nThe job has completed successfully. Contains results in AWS S3. Access from AWS us-west-2 with keys from https://harmony.earthdata.nasa.gov/cloud-access.sh\n\n\nThe third item in the list of links contains the shell script from the job status message printed above. Let’s download the same information in JSON format. It should be the fourth item; check to be sure:\n\nlen(response['links'])\n\n102\n\n\nSelect the url and download the json, then load to Python dictionary and print the keys:\n\nwith requests.get(response['links'][3]['href']) as r:\n    creds = r.json()\n\nprint(creds.keys())\n\ndict_keys(['AccessKeyId', 'SecretAccessKey', 'SessionToken', 'Expiration'])\n\n\nCheck the expiration timestamp for the temporary credentials:\n\ncreds['Expiration']\n\n'2021-06-11T02:36:29.000Z'\n\n\nOpen zarr datasets with s3fs and xarray\nGet the s3 output directory and list of zarr datasets from the list of links. The s3 directory should be the fifth item; the urls are from item six onward:\n\ns3_dir = response['links'][4]['href']\n\nprint(s3_dir)\n\ns3://harmony-prod-staging/public/harmony/netcdf-to-zarr/2295236b-8086-4543-9482-f524a9f2d0c3/\n\n\nNow select the URLs for the staged files and print the first one:\n\ns3_urls = [u['href'] for u in response['links'][5:]]\n\nprint(s3_urls[0])\n\ns3://harmony-prod-staging/public/harmony/netcdf-to-zarr/2295236b-8086-4543-9482-f524a9f2d0c3/OCEAN_BOTTOM_PRESSURE_mon_mean_2009-12_ECCO_V4r4_latlon_0p50deg.zarr\n\n\nUse the AWS s3fs package and your temporary aws_creds to open the zarr directory storage:\n\ns3 = s3fs.S3FileSystem(\n    key=creds['AccessKeyId'],\n    secret=creds['SecretAccessKey'],\n    token=creds['SessionToken'],\n    client_kwargs={'region_name':'us-west-2'},\n)\n\nlen(s3.ls(s3_dir))\n\n97\n\n\nPlot the first Ocean Bottom Pressure dataset\nCheck out the documentation for xarray’s open_zarr method at this link. Open the first dataset and plot the OBP variable:\n\nds0 = xr.open_zarr(s3.get_mapper(s3_urls[0]), decode_cf=True, mask_and_scale=True)\n\n# Mask the dataset where OBP is not within the bounds of the variable's valid min/max:\nds0_masked = ds0.where((ds0.OBP>=ds0.OBP.valid_min) & (ds0.OBP<=ds0.OBP.valid_max))\n\n# Plot the masked dataset\nds0_masked.OBP.isel(time=0).plot.imshow(size=10)\n\n<matplotlib.image.AxesImage at 0x7f28ed2ba4c0>\n\n\n\n\n\nLoad the zarr datasets into one large xarray dataset\nLoad all the datasets in a loop and concatenate them:\n\nzds = xr.concat([xr.open_zarr(s3.get_mapper(u)) for u in s3_urls], dim=\"time\")\n\nprint(zds)\n\n<xarray.Dataset>\nDimensions:         (latitude: 360, longitude: 720, nv: 2, time: 97)\nCoordinates:\n  * latitude        (latitude) float64 -89.75 -89.25 -88.75 ... 89.25 89.75\n    latitude_bnds   (latitude, nv) float64 -90.0 -89.5 -89.5 ... 89.5 89.5 90.0\n  * longitude       (longitude) float64 -179.8 -179.2 -178.8 ... 179.2 179.8\n    longitude_bnds  (longitude, nv) float64 -180.0 -179.5 -179.5 ... 179.5 180.0\n  * time            (time) datetime64[ns] 2009-12-16T12:00:00 ... 2017-12-16T...\n    time_bnds       (time, nv) datetime64[ns] dask.array<chunksize=(1, 2), meta=np.ndarray>\nDimensions without coordinates: nv\nData variables:\n    OBP             (time, latitude, longitude) float64 dask.array<chunksize=(1, 360, 720), meta=np.ndarray>\n    OBPGMAP         (time, latitude, longitude) float64 dask.array<chunksize=(1, 360, 720), meta=np.ndarray>\nAttributes: (12/57)\n    Conventions:                  CF-1.8, ACDD-1.3\n    acknowledgement:              This research was carried out by the Jet Pr...\n    author:                       Ian Fenty and Ou Wang\n    cdm_data_type:                Grid\n    comment:                      Fields provided on a regular lat-lon grid. ...\n    coordinates_comment:          Note: the global 'coordinates' attribute de...\n    ...                           ...\n    time_coverage_duration:       P1M\n    time_coverage_end:            2010-01-01T00:00:00\n    time_coverage_resolution:     P1M\n    time_coverage_start:          2009-12-01T00:00:00\n    title:                        ECCO Ocean Bottom Pressure - Monthly Mean 0...\n    uuid:                         297c8df0-4158-11eb-b208-0cc47a3f687b\n\n\nReference OBP and mask the dataset according to the valid minimum and maximum:\n\nobp = zds.OBP\n\nprint(obp)\n\n<xarray.DataArray 'OBP' (time: 97, latitude: 360, longitude: 720)>\ndask.array<concatenate, shape=(97, 360, 720), dtype=float64, chunksize=(1, 360, 720), chunktype=numpy.ndarray>\nCoordinates:\n  * latitude   (latitude) float64 -89.75 -89.25 -88.75 ... 88.75 89.25 89.75\n  * longitude  (longitude) float64 -179.8 -179.2 -178.8 ... 178.8 179.2 179.8\n  * time       (time) datetime64[ns] 2009-12-16T12:00:00 ... 2017-12-16T06:00:00\nAttributes:\n    comment:                OBP excludes the contribution from global mean at...\n    coverage_content_type:  modelResult\n    long_name:              Ocean bottom pressure given as equivalent water t...\n    units:                  m\n    valid_max:              72.07011413574219\n    valid_min:              -1.7899188995361328\n\n\nGet the valid min and max from the corresponding CF attributes:\n\nobp_vmin, obp_vmax = obp.valid_min, obp.valid_max\n\nobp_vmin, obp_vmax\n\n(-1.7899188995361328, 72.07011413574219)\n\n\nMask the dataset according to the OBP min and max and plot a series:\n\n# Mask dataset where not inside OBP variable valid min/max:\nzds_masked = zds.where((obp>=obp_vmin)&(obp<=obp_vmax))\n\n# Plot SSH again for the first 12 time slices:\nobpp = zds_masked.OBP.isel(time=slice(0, 6)).plot(\n    x=\"longitude\", \n    y=\"latitude\", \n    col=\"time\",\n    levels=8,\n    col_wrap=3, \n    add_colorbar=False,\n    figsize=(14, 8)\n)\n\n# Plot a colorbar on a secondary axis\nmappable = obpp.axes[0][0].collections[0]\ncax = plt.axes([0.05, -0.04, 0.95, 0.04])\ncbar1 = plt.colorbar(mappable, cax=cax, orientation='horizontal')"
  },
  {
    "objectID": "external/zarr-eosdis-store.html",
    "href": "external/zarr-eosdis-store.html",
    "title": "2021 Cloud Workshop at AGU",
    "section": "",
    "text": "Zarr Example\nimported on: 2022-03-10\n\nThis notebook is from NASA’s Zarr EOSDIS store notebook\n\n\nThe original source for this document is https://github.com/nasa/zarr-eosdis-store\n\n\n\nzarr-eosdis-store example\nInstall dependencies\n\nimport sys\n\n# zarr and zarr-eosdis-store, the main libraries being demoed\n!{sys.executable} -m pip install zarr zarr-eosdis-store\n\n# Notebook-specific libraries\n!{sys.executable} -m pip install matplotlib\n\nImportant: To run this, you must first create an Earthdata Login account (https://urs.earthdata.nasa.gov) and place your credentials in ~/.netrc e.g.:\n   machine urs.earthdata.nasa.gov login YOUR_USER password YOUR_PASSWORD\nNever share or commit your password / .netrc file!\nBasic usage. After these lines, we work with ds as though it were a normal Zarr dataset\n\nimport zarr\nfrom eosdis_store import EosdisStore\n\nurl = 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210715090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc'\n\nds = zarr.open(EosdisStore(url))\n\nView the file’s variable structure\n\nprint(ds.tree())\n\n/\n ├── analysed_sst (1, 17999, 36000) int16\n ├── analysis_error (1, 17999, 36000) int16\n ├── dt_1km_data (1, 17999, 36000) int16\n ├── lat (17999,) float32\n ├── lon (36000,) float32\n ├── mask (1, 17999, 36000) int16\n ├── sea_ice_fraction (1, 17999, 36000) int16\n ├── sst_anomaly (1, 17999, 36000) int16\n └── time (1,) int32\n\n\nFetch the latitude and longitude arrays and determine start and end indices for our area of interest. In this case, we’re looking at the Great Lakes, which have a nice, recognizeable shape. Latitudes 41 to 49, longitudes -93 to 76.\n\nlats = ds['lat'][:]\nlons = ds['lon'][:]\nlat_range = slice(lats.searchsorted(41), lats.searchsorted(49))\nlon_range = slice(lons.searchsorted(-93), lons.searchsorted(-76))\n\nGet the analysed sea surface temperature variable over our area of interest and apply scale factor and offset from the file metadata. In a future release, scale factor and add offset will be automatically applied.\n\nvar = ds['analysed_sst']\nanalysed_sst = var[0, lat_range, lon_range] * var.attrs['scale_factor'] + var.attrs['add_offset']\n\nDraw a pretty picture\n\nfrom matplotlib import pyplot as plt\n\nplt.rcParams[\"figure.figsize\"] = [16, 8]\nplt.imshow(analysed_sst[::-1, :])\nNone\n\n\n\n\nIn a dozen lines of code and a few seconds, we have managed to fetch and visualize the 3.2 megabyte we needed from a 732 megabyte file using the original archive URL and no processing services"
  },
  {
    "objectID": "further-resources.html#a-growing-list-of-resources",
    "href": "further-resources.html#a-growing-list-of-resources",
    "title": "Additional resources",
    "section": "A growing list of resources!",
    "text": "A growing list of resources!\n\n2021 NASA Cloud Hackathon - November 2021, co-hosted by PODAAC, NSIDC DAAC, and LPDAAC. Additional support is provided by ASDC, GESDISC, IMPACT, and Openscapes.\nNASA Earthdata: How to Cloud\nUSGS Eyes on Earth Podcast: Satellites and Cloud Computing - with Aaron Friesz (LP DAAC!)\nPO.DAAC Cloud Data Page\nPO.DAAC Earthdata Webinar (Aug 2021): Surfing Ocean Data in the Cloud - The Beginner’s Guide to PO.DAAC in the NASA Earthdata Cloud\nPO.DAAC Github Repository\nNASA Earthdata Cloud Primer -AWS cloud primer: helpful tutorials for how to set up your own EC2 cloud instance in AWS, attach storeage, move files back and forth, and more.\nSetting up Jupyter Notebooks in a user EC2 instance in AWS - helpful blog post for setting up jupyter notebooks in an EC2 instance in AWS. (Builds on the Cloud Primer tutorials, which are missing that next step)"
  },
  {
    "objectID": "further-resources.html#additional-tutorials",
    "href": "further-resources.html#additional-tutorials",
    "title": "Additional resources",
    "section": "Additional tutorials",
    "text": "Additional tutorials\n\nData_Access__Direct_S3_Access__PODAAC_ECCO_SSH using CMR-STAC API to retrieve S3 links\nDirect access to ECCO data in S3 (from us-west-2) - Direct S3 access example with netCDF data\nDirect_S3_Access__gdalvrt\nDirect_S3_Access__rioxarray_clipping\nGetting Started with Cloud-Native Harmonized Landsat Sentinel-2 (HLS) Data in R\nCalculate black-sky, white-sky, and actual albedo (MCD43A) from MCD43A1 BRDF Parameters using R\nXarray Zonal Statistics"
  }
]